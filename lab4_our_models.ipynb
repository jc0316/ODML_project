{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "!pip install torch torchvision\n",
    "!pip install tqdm\n",
    "!pip install pandas\n",
    "!pip install -U coremltools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Torch version 2.5.1 has not been tested with coremltools. You may run into unexpected errors. Torch 2.4.0 is the most recent version that has been tested.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.utils.prune as prune\n",
    "import torch.nn.functional as F\n",
    "from copy import deepcopy\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import coremltools as ct\n",
    "from coremltools.optimize.torch.pruning import ModuleMagnitudePrunerConfig, MagnitudePruner, MagnitudePrunerConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [3 points] Exercise 3: Your Model, Device, and Data\n",
    "\n",
    "\n",
    "In this section, you will repeat the simple experiments from Exercise 2 on your own model, device, and data. Additionally, you will choose two of three options for practical benefits to your pruned model's accuracy and latency. You may use a different sparsity level, higher or lower than 33%, if it makes sense for your settings. Make sure to report any changes you made and why you made them. Additionally, report any challenges encountered measuring latency or storage on your device.\n",
    "\n",
    "### [1 point] 1. Repeat Exercise 2.4 (repeated unstructured pruning) for your model, on your device and with your data.\n",
    "\n",
    "Keep performing the same unstructured magnitude pruning of your choice of sparsity level of the remaining weights on the same model (*without re-training or resetting the model*). You will apply the same function as above with the same 0.33 proportion parameter.\n",
    "\n",
    "Collect values for this table, keeping in mind that you will need to plot the results later. You might want to keep the values in Pandas DataFrames. Sparsity reported should be the percentage of *prunable* parameters pruned. \n",
    "\n",
    "| Iteration | Sparsity (%) | Accuracy | Latency (s) | Disk Size (MB) |\n",
    "| --------- | ------------ | -------- | ----------- | -------------- |\n",
    "|     0     |   0.0%       |          |             |                |\n",
    "|     1     |      ?       |          |             |                |\n",
    "|     2     |              |          |             |                |\n",
    "|     3     |              |          |             |                |\n",
    "|     4     |              |          |             |                |\n",
    "|     5     |              |          |             |                |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sparsity(model, print_results=False):\n",
    "    \"\"\"\n",
    "    Calculate the sparsity level (using the percent of elements that are 0) for:\n",
    "    - each parameter,\n",
    "    - all pruned parameters overall, and\n",
    "    - the model overall.\n",
    "    \n",
    "    Report each of these values: \n",
    "    - the sparsity level of each parameter, \n",
    "    - across all pruned parameters, and \n",
    "    - for the model overall. \n",
    "    \"\"\"\n",
    "    sparsity_per_parameter = {}\n",
    "    total_zero_count_pruned = 0\n",
    "    total_element_count_pruned = 0\n",
    "    total_zero_count_model = 0\n",
    "    total_element_count_model = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "    # Iterate over all buffers in the model\n",
    "    for name, buffer in model.named_buffers():\n",
    "        # Calculate the number of zero elements and total elements in the buffer\n",
    "        zero_count = (buffer == 0).sum().item()\n",
    "        total_elements = buffer.numel()\n",
    "        \n",
    "        # Calculate the sparsity level for this parameter\n",
    "        sparsity_per_parameter[name] = zero_count / total_elements * 100\n",
    "\n",
    "        # Check if this is a pruned parameter by looking for \"weight_mask\" or \"bias_mask\" in the name\n",
    "        if \"weight_mask\" in name or \"bias_mask\" in name:\n",
    "            total_zero_count_pruned += zero_count\n",
    "            total_element_count_pruned += total_elements\n",
    "\n",
    "        # Accumulate for overall model sparsity\n",
    "        total_zero_count_model += zero_count\n",
    "\n",
    "    # Calculate overall sparsity for pruned parameters and the entire model\n",
    "    sparsity_pruned_parameters = (total_zero_count_pruned / total_element_count_pruned * 100\n",
    "                                  if total_element_count_pruned > 0 else 0)\n",
    "    sparsity_model = total_zero_count_model / total_element_count_model * 100\n",
    "\n",
    "    # Print or return the results\n",
    "    if print_results:\n",
    "        print(\"Sparsity per parameter:\")\n",
    "        for name, sparsity in sparsity_per_parameter.items():\n",
    "            print(f\"  {name}: {sparsity:.2f}%\")\n",
    "        \n",
    "        print(f\"Sparsity across all pruned parameters: {sparsity_pruned_parameters:.2f}%\")\n",
    "        print(f\"Sparsity for the model overall: {sparsity_model:.2f}%\")\n",
    "\n",
    "    # Optionally, return the values for further use\n",
    "    return {\n",
    "        \"sparsity_per_parameter\": sparsity_per_parameter,\n",
    "        \"sparsity_pruned_parameters\": sparsity_pruned_parameters,\n",
    "        \"sparsity_model\": sparsity_model\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_size_of_model(model, label=\"\"):\n",
    "    sd = model.state_dict()\n",
    "    for item in sd:\n",
    "        sd[item] = model.state_dict()[item].to_sparse()\n",
    "    \n",
    "    torch.save(sd, \"temp.pt\")\n",
    "    size=os.path.getsize(\"temp.pt\")\n",
    "    #print(\"model: \",label,' \\t','Size (MB):', size/1e6)\n",
    "\n",
    "    os.remove('temp.pt')\n",
    "    return size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model):\n",
    "    model.eval()\n",
    "\n",
    "    size = print_size_of_model(model, \"sparse\")\n",
    "    \n",
    "    return size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_evaluate(model, device):\n",
    "    model.to(device)\n",
    "\n",
    "    labels = [\n",
    "        'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', \n",
    "        'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', \n",
    "        'del', 'nothing', 'space'\n",
    "    ]\n",
    "\n",
    "    # Create model\n",
    "    num_classes = len(labels)  # 29 classes\n",
    "    model_copy = models.resnet50(weights=False)\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model_copy.fc = nn.Linear(num_ftrs, num_classes)\n",
    "\n",
    "    prune_params = [(m[1], \"weight\") for m in model_copy.named_modules() if len(list(m[1].children()))==0 and not isinstance(m[1], (nn.ReLU, nn.MaxPool2d, nn.AdaptiveAvgPool2d))]\n",
    "    for p in prune_params:\n",
    "        prune.identity(p[0], \"weight\")\n",
    "    # Copy the parameters\n",
    "    model_copy.load_state_dict(model.state_dict())\n",
    "    \n",
    "    copy_params = [(m[1], \"weight\") for m in model_copy.named_modules() if len(list(m[1].children()))==0 and not isinstance(m[1], (nn.ReLU, nn.MaxPool2d, nn.AdaptiveAvgPool2d))]\n",
    "    # (we assume the same model architecture)\n",
    "    for p in copy_params:\n",
    "        prune.remove(*p)\n",
    "    \n",
    "    return evaluate_model(model_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_asl_model(model_path):\n",
    "    labels = [\n",
    "        'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', \n",
    "        'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', \n",
    "        'del', 'nothing', 'space'\n",
    "    ]\n",
    "    # Create model\n",
    "    num_classes = len(labels)  # 29 classes\n",
    "    model = models.resnet50(weights=False)\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "\n",
    "    # Load the saved weights\n",
    "    model.load_state_dict(torch.load(model_path, map_location=torch.device(\"cpu\")))\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    return model, device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_copy_of_model(model, model_path):\n",
    "    for p in prune_params:\n",
    "    prune.remove(*p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_repeated_pruning(model_path, sparsity):\n",
    "\n",
    "    # Create empty DataFrame to store results\n",
    "    results_df = pd.DataFrame(columns=[\n",
    "        'iteration',\n",
    "        'sparsity_model',\n",
    "        'size_mb'\n",
    "    ])\n",
    "\n",
    "    # Define the labels for the classes (A-Z, del, nothing, space)\n",
    "\n",
    "    model, device = load_asl_model(model_path)\n",
    "    print(f\"Model weights loaded from {model_path}\")\n",
    "\n",
    "    prune_params = [(m[1], \"weight\") for m in model.named_modules() if len(list(m[1].children()))==0 and not isinstance(m[1], (nn.ReLU, nn.MaxPool2d, nn.AdaptiveAvgPool2d))]\n",
    "\n",
    "    # Store initial results (iteration 0)\n",
    "    sparsity_results = calculate_sparsity(model)\n",
    "    size = evaluate_model(model)\n",
    "    \n",
    "    results_df.loc[0] = {\n",
    "        'iteration': 0,\n",
    "        'sparsity_model': sparsity_results['sparsity_model'],\n",
    "        'size_mb': size/1e6\n",
    "    }\n",
    "\n",
    "    model_path_list = [model_path]\n",
    "\n",
    "    print(f\"Iteration 0 - Model Sparsity: {sparsity_results['sparsity_model']:.2f}%\")\n",
    "\n",
    "    for i in range(1, 6):\n",
    "        print(f\"Pruning iteration {i}\")\n",
    "        prune.global_unstructured(prune_params, pruning_method=prune.L1Unstructured, amount=sparsity)\n",
    "\n",
    "\n",
    "\n",
    "        #print(prune_params[0][0].weight)\n",
    "        #print(type(prune_params[0][0]))\n",
    "\n",
    "        saved_model_path = f\"models/pruned_model_iteration_{i}.pth\"\n",
    "        model_path_list.append(saved_model_path)\n",
    "        torch.save(model.state_dict(), saved_model_path)\n",
    "\n",
    "        size = sparse_evaluate(model, device)\n",
    "        sparsity_results = calculate_sparsity(model)\n",
    "\n",
    "          \n",
    "        # Store results in DataFrame\n",
    "        results_df.loc[i] = {\n",
    "            'iteration': i,\n",
    "            'sparsity_model': sparsity_results['sparsity_model'],\n",
    "            'size_mb': size/1e6\n",
    "        }\n",
    "\n",
    "        print(f\"Sparsity for the model overall at Iteration {i}: {sparsity_results['sparsity_model']:.2f}%, Size MB: {size/1e6}\")\n",
    "    \n",
    "    \n",
    "    # Display the DataFrame\n",
    "    print(\"\\nFinal Results DataFrame:\")\n",
    "    print(results_df)\n",
    "    \n",
    "    return results_df, model_path_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/briancurtin/Documents/GitHub/ODML_project/.venv/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "/var/folders/mv/q059yr4j7l18ms7xb987dwtm0000gn/T/ipykernel_40481/2142981156.py:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=torch.device(\"cpu\")))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weights loaded from ./models/model_weights_ResNet50_224_resize.pth\n",
      "Iteration 0 - Model Sparsity: 0.00%\n",
      "Pruning iteration 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/briancurtin/Documents/GitHub/ODML_project/.venv/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for ResNet:\n\tMissing key(s) in state_dict: \"conv1.weight_orig\", \"conv1.weight_mask\", \"bn1.weight_orig\", \"bn1.weight_mask\", \"layer1.0.conv1.weight_orig\", \"layer1.0.conv1.weight_mask\", \"layer1.0.bn1.weight_orig\", \"layer1.0.bn1.weight_mask\", \"layer1.0.conv2.weight_orig\", \"layer1.0.conv2.weight_mask\", \"layer1.0.bn2.weight_orig\", \"layer1.0.bn2.weight_mask\", \"layer1.0.conv3.weight_orig\", \"layer1.0.conv3.weight_mask\", \"layer1.0.bn3.weight_orig\", \"layer1.0.bn3.weight_mask\", \"layer1.0.downsample.0.weight_orig\", \"layer1.0.downsample.0.weight_mask\", \"layer1.0.downsample.1.weight_orig\", \"layer1.0.downsample.1.weight_mask\", \"layer1.1.conv1.weight_orig\", \"layer1.1.conv1.weight_mask\", \"layer1.1.bn1.weight_orig\", \"layer1.1.bn1.weight_mask\", \"layer1.1.conv2.weight_orig\", \"layer1.1.conv2.weight_mask\", \"layer1.1.bn2.weight_orig\", \"layer1.1.bn2.weight_mask\", \"layer1.1.conv3.weight_orig\", \"layer1.1.conv3.weight_mask\", \"layer1.1.bn3.weight_orig\", \"layer1.1.bn3.weight_mask\", \"layer1.2.conv1.weight_orig\", \"layer1.2.conv1.weight_mask\", \"layer1.2.bn1.weight_orig\", \"layer1.2.bn1.weight_mask\", \"layer1.2.conv2.weight_orig\", \"layer1.2.conv2.weight_mask\", \"layer1.2.bn2.weight_orig\", \"layer1.2.bn2.weight_mask\", \"layer1.2.conv3.weight_orig\", \"layer1.2.conv3.weight_mask\", \"layer1.2.bn3.weight_orig\", \"layer1.2.bn3.weight_mask\", \"layer2.0.conv1.weight_orig\", \"layer2.0.conv1.weight_mask\", \"layer2.0.bn1.weight_orig\", \"layer2.0.bn1.weight_mask\", \"layer2.0.conv2.weight_orig\", \"layer2.0.conv2.weight_mask\", \"layer2.0.bn2.weight_orig\", \"layer2.0.bn2.weight_mask\", \"layer2.0.conv3.weight_orig\", \"layer2.0.conv3.weight_mask\", \"layer2.0.bn3.weight_orig\", \"layer2.0.bn3.weight_mask\", \"layer2.0.downsample.0.weight_orig\", \"layer2.0.downsample.0.weight_mask\", \"layer2.0.downsample.1.weight_orig\", \"layer2.0.downsample.1.weight_mask\", \"layer2.1.conv1.weight_orig\", \"layer2.1.conv1.weight_mask\", \"layer2.1.bn1.weight_orig\", \"layer2.1.bn1.weight_mask\", \"layer2.1.conv2.weight_orig\", \"layer2.1.conv2.weight_mask\", \"layer2.1.bn2.weight_orig\", \"layer2.1.bn2.weight_mask\", \"layer2.1.conv3.weight_orig\", \"layer2.1.conv3.weight_mask\", \"layer2.1.bn3.weight_orig\", \"layer2.1.bn3.weight_mask\", \"layer2.2.conv1.weight_orig\", \"layer2.2.conv1.weight_mask\", \"layer2.2.bn1.weight_orig\", \"layer2.2.bn1.weight_mask\", \"layer2.2.conv2.weight_orig\", \"layer2.2.conv2.weight_mask\", \"layer2.2.bn2.weight_orig\", \"layer2.2.bn2.weight_mask\", \"layer2.2.conv3.weight_orig\", \"layer2.2.conv3.weight_mask\", \"layer2.2.bn3.weight_orig\", \"layer2.2.bn3.weight_mask\", \"layer2.3.conv1.weight_orig\", \"layer2.3.conv1.weight_mask\", \"layer2.3.bn1.weight_orig\", \"layer2.3.bn1.weight_mask\", \"layer2.3.conv2.weight_orig\", \"layer2.3.conv2.weight_mask\", \"layer2.3.bn2.weight_orig\", \"layer2.3.bn2.weight_mask\", \"layer2.3.conv3.weight_orig\", \"layer2.3.conv3.weight_mask\", \"layer2.3.bn3.weight_orig\", \"layer2.3.bn3.weight_mask\", \"layer3.0.conv1.weight_orig\", \"layer3.0.conv1.weight_mask\", \"layer3.0.bn1.weight_orig\", \"layer3.0.bn1.weight_mask\", \"layer3.0.conv2.weight_orig\", \"layer3.0.conv2.weight_mask\", \"layer3.0.bn2.weight_orig\", \"layer3.0.bn2.weight_mask\", \"layer3.0.conv3.weight_orig\", \"layer3.0.conv3.weight_mask\", \"layer3.0.bn3.weight_orig\", \"layer3.0.bn3.weight_mask\", \"layer3.0.downsample.0.weight_orig\", \"layer3.0.downsample.0.weight_mask\", \"layer3.0.downsample.1.weight_orig\", \"layer3.0.downsample.1.weight_mask\", \"layer3.1.conv1.weight_orig\", \"layer3.1.conv1.weight_mask\", \"layer3.1.bn1.weight_orig\", \"layer3.1.bn1.weight_mask\", \"layer3.1.conv2.weight_orig\", \"layer3.1.conv2.weight_mask\", \"layer3.1.bn2.weight_orig\", \"layer3.1.bn2.weight_mask\", \"layer3.1.conv3.weight_orig\", \"layer3.1.conv3.weight_mask\", \"layer3.1.bn3.weight_orig\", \"layer3.1.bn3.weight_mask\", \"layer3.2.conv1.weight_orig\", \"layer3.2.conv1.weight_mask\", \"layer3.2.bn1.weight_orig\", \"layer3.2.bn1.weight_mask\", \"layer3.2.conv2.weight_orig\", \"layer3.2.conv2.weight_mask\", \"layer3.2.bn2.weight_orig\", \"layer3.2.bn2.weight_mask\", \"layer3.2.conv3.weight_orig\", \"layer3.2.conv3.weight_mask\", \"layer3.2.bn3.weight_orig\", \"layer3.2.bn3.weight_mask\", \"layer3.3.conv1.weight_orig\", \"layer3.3.conv1.weight_mask\", \"layer3.3.bn1.weight_orig\", \"layer3.3.bn1.weight_mask\", \"layer3.3.conv2.weight_orig\", \"layer3.3.conv2.weight_mask\", \"layer3.3.bn2.weight_orig\", \"layer3.3.bn2.weight_mask\", \"layer3.3.conv3.weight_orig\", \"layer3.3.conv3.weight_mask\", \"layer3.3.bn3.weight_orig\", \"layer3.3.bn3.weight_mask\", \"layer3.4.conv1.weight_orig\", \"layer3.4.conv1.weight_mask\", \"layer3.4.bn1.weight_orig\", \"layer3.4.bn1.weight_mask\", \"layer3.4.conv2.weight_orig\", \"layer3.4.conv2.weight_mask\", \"layer3.4.bn2.weight_orig\", \"layer3.4.bn2.weight_mask\", \"layer3.4.conv3.weight_orig\", \"layer3.4.conv3.weight_mask\", \"layer3.4.bn3.weight_orig\", \"layer3.4.bn3.weight_mask\", \"layer3.5.conv1.weight_orig\", \"layer3.5.conv1.weight_mask\", \"layer3.5.bn1.weight_orig\", \"layer3.5.bn1.weight_mask\", \"layer3.5.conv2.weight_orig\", \"layer3.5.conv2.weight_mask\", \"layer3.5.bn2.weight_orig\", \"layer3.5.bn2.weight_mask\", \"layer3.5.conv3.weight_orig\", \"layer3.5.conv3.weight_mask\", \"layer3.5.bn3.weight_orig\", \"layer3.5.bn3.weight_mask\", \"layer4.0.conv1.weight_orig\", \"layer4.0.conv1.weight_mask\", \"layer4.0.bn1.weight_orig\", \"layer4.0.bn1.weight_mask\", \"layer4.0.conv2.weight_orig\", \"layer4.0.conv2.weight_mask\", \"layer4.0.bn2.weight_orig\", \"layer4.0.bn2.weight_mask\", \"layer4.0.conv3.weight_orig\", \"layer4.0.conv3.weight_mask\", \"layer4.0.bn3.weight_orig\", \"layer4.0.bn3.weight_mask\", \"layer4.0.downsample.0.weight_orig\", \"layer4.0.downsample.0.weight_mask\", \"layer4.0.downsample.1.weight_orig\", \"layer4.0.downsample.1.weight_mask\", \"layer4.1.conv1.weight_orig\", \"layer4.1.conv1.weight_mask\", \"layer4.1.bn1.weight_orig\", \"layer4.1.bn1.weight_mask\", \"layer4.1.conv2.weight_orig\", \"layer4.1.conv2.weight_mask\", \"layer4.1.bn2.weight_orig\", \"layer4.1.bn2.weight_mask\", \"layer4.1.conv3.weight_orig\", \"layer4.1.conv3.weight_mask\", \"layer4.1.bn3.weight_orig\", \"layer4.1.bn3.weight_mask\", \"layer4.2.conv1.weight_orig\", \"layer4.2.conv1.weight_mask\", \"layer4.2.bn1.weight_orig\", \"layer4.2.bn1.weight_mask\", \"layer4.2.conv2.weight_orig\", \"layer4.2.conv2.weight_mask\", \"layer4.2.bn2.weight_orig\", \"layer4.2.bn2.weight_mask\", \"layer4.2.conv3.weight_orig\", \"layer4.2.conv3.weight_mask\", \"layer4.2.bn3.weight_orig\", \"layer4.2.bn3.weight_mask\", \"fc.weight_orig\", \"fc.weight_mask\". \n\tUnexpected key(s) in state_dict: \"conv1.weight\", \"bn1.weight\", \"layer1.0.conv1.weight\", \"layer1.0.bn1.weight\", \"layer1.0.conv2.weight\", \"layer1.0.bn2.weight\", \"layer1.0.conv3.weight\", \"layer1.0.bn3.weight\", \"layer1.0.downsample.0.weight\", \"layer1.0.downsample.1.weight\", \"layer1.1.conv1.weight\", \"layer1.1.bn1.weight\", \"layer1.1.conv2.weight\", \"layer1.1.bn2.weight\", \"layer1.1.conv3.weight\", \"layer1.1.bn3.weight\", \"layer1.2.conv1.weight\", \"layer1.2.bn1.weight\", \"layer1.2.conv2.weight\", \"layer1.2.bn2.weight\", \"layer1.2.conv3.weight\", \"layer1.2.bn3.weight\", \"layer2.0.conv1.weight\", \"layer2.0.bn1.weight\", \"layer2.0.conv2.weight\", \"layer2.0.bn2.weight\", \"layer2.0.conv3.weight\", \"layer2.0.bn3.weight\", \"layer2.0.downsample.0.weight\", \"layer2.0.downsample.1.weight\", \"layer2.1.conv1.weight\", \"layer2.1.bn1.weight\", \"layer2.1.conv2.weight\", \"layer2.1.bn2.weight\", \"layer2.1.conv3.weight\", \"layer2.1.bn3.weight\", \"layer2.2.conv1.weight\", \"layer2.2.bn1.weight\", \"layer2.2.conv2.weight\", \"layer2.2.bn2.weight\", \"layer2.2.conv3.weight\", \"layer2.2.bn3.weight\", \"layer2.3.conv1.weight\", \"layer2.3.bn1.weight\", \"layer2.3.conv2.weight\", \"layer2.3.bn2.weight\", \"layer2.3.conv3.weight\", \"layer2.3.bn3.weight\", \"layer3.0.conv1.weight\", \"layer3.0.bn1.weight\", \"layer3.0.conv2.weight\", \"layer3.0.bn2.weight\", \"layer3.0.conv3.weight\", \"layer3.0.bn3.weight\", \"layer3.0.downsample.0.weight\", \"layer3.0.downsample.1.weight\", \"layer3.1.conv1.weight\", \"layer3.1.bn1.weight\", \"layer3.1.conv2.weight\", \"layer3.1.bn2.weight\", \"layer3.1.conv3.weight\", \"layer3.1.bn3.weight\", \"layer3.2.conv1.weight\", \"layer3.2.bn1.weight\", \"layer3.2.conv2.weight\", \"layer3.2.bn2.weight\", \"layer3.2.conv3.weight\", \"layer3.2.bn3.weight\", \"layer3.3.conv1.weight\", \"layer3.3.bn1.weight\", \"layer3.3.conv2.weight\", \"layer3.3.bn2.weight\", \"layer3.3.conv3.weight\", \"layer3.3.bn3.weight\", \"layer3.4.conv1.weight\", \"layer3.4.bn1.weight\", \"layer3.4.conv2.weight\", \"layer3.4.bn2.weight\", \"layer3.4.conv3.weight\", \"layer3.4.bn3.weight\", \"layer3.5.conv1.weight\", \"layer3.5.bn1.weight\", \"layer3.5.conv2.weight\", \"layer3.5.bn2.weight\", \"layer3.5.conv3.weight\", \"layer3.5.bn3.weight\", \"layer4.0.conv1.weight\", \"layer4.0.bn1.weight\", \"layer4.0.conv2.weight\", \"layer4.0.bn2.weight\", \"layer4.0.conv3.weight\", \"layer4.0.bn3.weight\", \"layer4.0.downsample.0.weight\", \"layer4.0.downsample.1.weight\", \"layer4.1.conv1.weight\", \"layer4.1.bn1.weight\", \"layer4.1.conv2.weight\", \"layer4.1.bn2.weight\", \"layer4.1.conv3.weight\", \"layer4.1.bn3.weight\", \"layer4.2.conv1.weight\", \"layer4.2.bn1.weight\", \"layer4.2.conv2.weight\", \"layer4.2.bn2.weight\", \"layer4.2.conv3.weight\", \"layer4.2.bn3.weight\", \"fc.weight\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# [1 point] 1. Repeat Exercise 2.4 (repeated unstructured pruning) for your model, on your device and with your data.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# https://apple.github.io/coremltools/source/coremltools.optimize.torch.pruning.html\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m pruning_results, model_paths \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_repeated_pruning\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./models/model_weights_ResNet50_224_resize.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.33\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[17], line 45\u001b[0m, in \u001b[0;36mmodel_repeated_pruning\u001b[0;34m(model_path, sparsity)\u001b[0m\n\u001b[1;32m     42\u001b[0m model_path_list\u001b[38;5;241m.\u001b[39mappend(saved_model_path)\n\u001b[1;32m     43\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), saved_model_path)\n\u001b[0;32m---> 45\u001b[0m size \u001b[38;5;241m=\u001b[39m \u001b[43msparse_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m sparsity_results \u001b[38;5;241m=\u001b[39m calculate_sparsity(model)\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Store results in DataFrame\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[7], line 20\u001b[0m, in \u001b[0;36msparse_evaluate\u001b[0;34m(model, device)\u001b[0m\n\u001b[1;32m     18\u001b[0m     prune\u001b[38;5;241m.\u001b[39midentity(p[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Copy the parameters\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m \u001b[43mmodel_copy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m copy_params \u001b[38;5;241m=\u001b[39m [(m[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m model_copy\u001b[38;5;241m.\u001b[39mnamed_modules() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mlist\u001b[39m(m[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mchildren()))\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(m[\u001b[38;5;241m1\u001b[39m], (nn\u001b[38;5;241m.\u001b[39mReLU, nn\u001b[38;5;241m.\u001b[39mMaxPool2d, nn\u001b[38;5;241m.\u001b[39mAdaptiveAvgPool2d))]\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# (we assume the same model architecture)\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/GitHub/ODML_project/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2584\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2576\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2577\u001b[0m             \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   2578\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2579\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[1;32m   2580\u001b[0m             ),\n\u001b[1;32m   2581\u001b[0m         )\n\u001b[1;32m   2583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2584\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   2585\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2586\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)\n\u001b[1;32m   2587\u001b[0m         )\n\u001b[1;32m   2588\u001b[0m     )\n\u001b[1;32m   2589\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for ResNet:\n\tMissing key(s) in state_dict: \"conv1.weight_orig\", \"conv1.weight_mask\", \"bn1.weight_orig\", \"bn1.weight_mask\", \"layer1.0.conv1.weight_orig\", \"layer1.0.conv1.weight_mask\", \"layer1.0.bn1.weight_orig\", \"layer1.0.bn1.weight_mask\", \"layer1.0.conv2.weight_orig\", \"layer1.0.conv2.weight_mask\", \"layer1.0.bn2.weight_orig\", \"layer1.0.bn2.weight_mask\", \"layer1.0.conv3.weight_orig\", \"layer1.0.conv3.weight_mask\", \"layer1.0.bn3.weight_orig\", \"layer1.0.bn3.weight_mask\", \"layer1.0.downsample.0.weight_orig\", \"layer1.0.downsample.0.weight_mask\", \"layer1.0.downsample.1.weight_orig\", \"layer1.0.downsample.1.weight_mask\", \"layer1.1.conv1.weight_orig\", \"layer1.1.conv1.weight_mask\", \"layer1.1.bn1.weight_orig\", \"layer1.1.bn1.weight_mask\", \"layer1.1.conv2.weight_orig\", \"layer1.1.conv2.weight_mask\", \"layer1.1.bn2.weight_orig\", \"layer1.1.bn2.weight_mask\", \"layer1.1.conv3.weight_orig\", \"layer1.1.conv3.weight_mask\", \"layer1.1.bn3.weight_orig\", \"layer1.1.bn3.weight_mask\", \"layer1.2.conv1.weight_orig\", \"layer1.2.conv1.weight_mask\", \"layer1.2.bn1.weight_orig\", \"layer1.2.bn1.weight_mask\", \"layer1.2.conv2.weight_orig\", \"layer1.2.conv2.weight_mask\", \"layer1.2.bn2.weight_orig\", \"layer1.2.bn2.weight_mask\", \"layer1.2.conv3.weight_orig\", \"layer1.2.conv3.weight_mask\", \"layer1.2.bn3.weight_orig\", \"layer1.2.bn3.weight_mask\", \"layer2.0.conv1.weight_orig\", \"layer2.0.conv1.weight_mask\", \"layer2.0.bn1.weight_orig\", \"layer2.0.bn1.weight_mask\", \"layer2.0.conv2.weight_orig\", \"layer2.0.conv2.weight_mask\", \"layer2.0.bn2.weight_orig\", \"layer2.0.bn2.weight_mask\", \"layer2.0.conv3.weight_orig\", \"layer2.0.conv3.weight_mask\", \"layer2.0.bn3.weight_orig\", \"layer2.0.bn3.weight_mask\", \"layer2.0.downsample.0.weight_orig\", \"layer2.0.downsample.0.weight_mask\", \"layer2.0.downsample.1.weight_orig\", \"layer2.0.downsample.1.weight_mask\", \"layer2.1.conv1.weight_orig\", \"layer2.1.conv1.weight_mask\", \"layer2.1.bn1.weight_orig\", \"layer2.1.bn1.weight_mask\", \"layer2.1.conv2.weight_orig\", \"layer2.1.conv2.weight_mask\", \"layer2.1.bn2.weight_orig\", \"layer2.1.bn2.weight_mask\", \"layer2.1.conv3.weight_orig\", \"layer2.1.conv3.weight_mask\", \"layer2.1.bn3.weight_orig\", \"layer2.1.bn3.weight_mask\", \"layer2.2.conv1.weight_orig\", \"layer2.2.conv1.weight_mask\", \"layer2.2.bn1.weight_orig\", \"layer2.2.bn1.weight_mask\", \"layer2.2.conv2.weight_orig\", \"layer2.2.conv2.weight_mask\", \"layer2.2.bn2.weight_orig\", \"layer2.2.bn2.weight_mask\", \"layer2.2.conv3.weight_orig\", \"layer2.2.conv3.weight_mask\", \"layer2.2.bn3.weight_orig\", \"layer2.2.bn3.weight_mask\", \"layer2.3.conv1.weight_orig\", \"layer2.3.conv1.weight_mask\", \"layer2.3.bn1.weight_orig\", \"layer2.3.bn1.weight_mask\", \"layer2.3.conv2.weight_orig\", \"layer2.3.conv2.weight_mask\", \"layer2.3.bn2.weight_orig\", \"layer2.3.bn2.weight_mask\", \"layer2.3.conv3.weight_orig\", \"layer2.3.conv3.weight_mask\", \"layer2.3.bn3.weight_orig\", \"layer2.3.bn3.weight_mask\", \"layer3.0.conv1.weight_orig\", \"layer3.0.conv1.weight_mask\", \"layer3.0.bn1.weight_orig\", \"layer3.0.bn1.weight_mask\", \"layer3.0.conv2.weight_orig\", \"layer3.0.conv2.weight_mask\", \"layer3.0.bn2.weight_orig\", \"layer3.0.bn2.weight_mask\", \"layer3.0.conv3.weight_orig\", \"layer3.0.conv3.weight_mask\", \"layer3.0.bn3.weight_orig\", \"layer3.0.bn3.weight_mask\", \"layer3.0.downsample.0.weight_orig\", \"layer3.0.downsample.0.weight_mask\", \"layer3.0.downsample.1.weight_orig\", \"layer3.0.downsample.1.weight_mask\", \"layer3.1.conv1.weight_orig\", \"layer3.1.conv1.weight_mask\", \"layer3.1.bn1.weight_orig\", \"layer3.1.bn1.weight_mask\", \"layer3.1.conv2.weight_orig\", \"layer3.1.conv2.weight_mask\", \"layer3.1.bn2.weight_orig\", \"layer3.1.bn2.weight_mask\", \"layer3.1.conv3.weight_orig\", \"layer3.1.conv3.weight_mask\", \"layer3.1.bn3.weight_orig\", \"layer3.1.bn3.weight_mask\", \"layer3.2.conv1.weight_orig\", \"layer3.2.conv1.weight_mask\", \"layer3.2.bn1.weight_orig\", \"layer3.2.bn1.weight_mask\", \"layer3.2.conv2.weight_orig\", \"layer3.2.conv2.weight_mask\", \"layer3.2.bn2.weight_orig\", \"layer3.2.bn2.weight_mask\", \"layer3.2.conv3.weight_orig\", \"layer3.2.conv3.weight_mask\", \"layer3.2.bn3.weight_orig\", \"layer3.2.bn3.weight_mask\", \"layer3.3.conv1.weight_orig\", \"layer3.3.conv1.weight_mask\", \"layer3.3.bn1.weight_orig\", \"layer3.3.bn1.weight_mask\", \"layer3.3.conv2.weight_orig\", \"layer3.3.conv2.weight_mask\", \"layer3.3.bn2.weight_orig\", \"layer3.3.bn2.weight_mask\", \"layer3.3.conv3.weight_orig\", \"layer3.3.conv3.weight_mask\", \"layer3.3.bn3.weight_orig\", \"layer3.3.bn3.weight_mask\", \"layer3.4.conv1.weight_orig\", \"layer3.4.conv1.weight_mask\", \"layer3.4.bn1.weight_orig\", \"layer3.4.bn1.weight_mask\", \"layer3.4.conv2.weight_orig\", \"layer3.4.conv2.weight_mask\", \"layer3.4.bn2.weight_orig\", \"layer3.4.bn2.weight_mask\", \"layer3.4.conv3.weight_orig\", \"layer3.4.conv3.weight_mask\", \"layer3.4.bn3.weight_orig\", \"layer3.4.bn3.weight_mask\", \"layer3.5.conv1.weight_orig\", \"layer3.5.conv1.weight_mask\", \"layer3.5.bn1.weight_orig\", \"layer3.5.bn1.weight_mask\", \"layer3.5.conv2.weight_orig\", \"layer3.5.conv2.weight_mask\", \"layer3.5.bn2.weight_orig\", \"layer3.5.bn2.weight_mask\", \"layer3.5.conv3.weight_orig\", \"layer3.5.conv3.weight_mask\", \"layer3.5.bn3.weight_orig\", \"layer3.5.bn3.weight_mask\", \"layer4.0.conv1.weight_orig\", \"layer4.0.conv1.weight_mask\", \"layer4.0.bn1.weight_orig\", \"layer4.0.bn1.weight_mask\", \"layer4.0.conv2.weight_orig\", \"layer4.0.conv2.weight_mask\", \"layer4.0.bn2.weight_orig\", \"layer4.0.bn2.weight_mask\", \"layer4.0.conv3.weight_orig\", \"layer4.0.conv3.weight_mask\", \"layer4.0.bn3.weight_orig\", \"layer4.0.bn3.weight_mask\", \"layer4.0.downsample.0.weight_orig\", \"layer4.0.downsample.0.weight_mask\", \"layer4.0.downsample.1.weight_orig\", \"layer4.0.downsample.1.weight_mask\", \"layer4.1.conv1.weight_orig\", \"layer4.1.conv1.weight_mask\", \"layer4.1.bn1.weight_orig\", \"layer4.1.bn1.weight_mask\", \"layer4.1.conv2.weight_orig\", \"layer4.1.conv2.weight_mask\", \"layer4.1.bn2.weight_orig\", \"layer4.1.bn2.weight_mask\", \"layer4.1.conv3.weight_orig\", \"layer4.1.conv3.weight_mask\", \"layer4.1.bn3.weight_orig\", \"layer4.1.bn3.weight_mask\", \"layer4.2.conv1.weight_orig\", \"layer4.2.conv1.weight_mask\", \"layer4.2.bn1.weight_orig\", \"layer4.2.bn1.weight_mask\", \"layer4.2.conv2.weight_orig\", \"layer4.2.conv2.weight_mask\", \"layer4.2.bn2.weight_orig\", \"layer4.2.bn2.weight_mask\", \"layer4.2.conv3.weight_orig\", \"layer4.2.conv3.weight_mask\", \"layer4.2.bn3.weight_orig\", \"layer4.2.bn3.weight_mask\", \"fc.weight_orig\", \"fc.weight_mask\". \n\tUnexpected key(s) in state_dict: \"conv1.weight\", \"bn1.weight\", \"layer1.0.conv1.weight\", \"layer1.0.bn1.weight\", \"layer1.0.conv2.weight\", \"layer1.0.bn2.weight\", \"layer1.0.conv3.weight\", \"layer1.0.bn3.weight\", \"layer1.0.downsample.0.weight\", \"layer1.0.downsample.1.weight\", \"layer1.1.conv1.weight\", \"layer1.1.bn1.weight\", \"layer1.1.conv2.weight\", \"layer1.1.bn2.weight\", \"layer1.1.conv3.weight\", \"layer1.1.bn3.weight\", \"layer1.2.conv1.weight\", \"layer1.2.bn1.weight\", \"layer1.2.conv2.weight\", \"layer1.2.bn2.weight\", \"layer1.2.conv3.weight\", \"layer1.2.bn3.weight\", \"layer2.0.conv1.weight\", \"layer2.0.bn1.weight\", \"layer2.0.conv2.weight\", \"layer2.0.bn2.weight\", \"layer2.0.conv3.weight\", \"layer2.0.bn3.weight\", \"layer2.0.downsample.0.weight\", \"layer2.0.downsample.1.weight\", \"layer2.1.conv1.weight\", \"layer2.1.bn1.weight\", \"layer2.1.conv2.weight\", \"layer2.1.bn2.weight\", \"layer2.1.conv3.weight\", \"layer2.1.bn3.weight\", \"layer2.2.conv1.weight\", \"layer2.2.bn1.weight\", \"layer2.2.conv2.weight\", \"layer2.2.bn2.weight\", \"layer2.2.conv3.weight\", \"layer2.2.bn3.weight\", \"layer2.3.conv1.weight\", \"layer2.3.bn1.weight\", \"layer2.3.conv2.weight\", \"layer2.3.bn2.weight\", \"layer2.3.conv3.weight\", \"layer2.3.bn3.weight\", \"layer3.0.conv1.weight\", \"layer3.0.bn1.weight\", \"layer3.0.conv2.weight\", \"layer3.0.bn2.weight\", \"layer3.0.conv3.weight\", \"layer3.0.bn3.weight\", \"layer3.0.downsample.0.weight\", \"layer3.0.downsample.1.weight\", \"layer3.1.conv1.weight\", \"layer3.1.bn1.weight\", \"layer3.1.conv2.weight\", \"layer3.1.bn2.weight\", \"layer3.1.conv3.weight\", \"layer3.1.bn3.weight\", \"layer3.2.conv1.weight\", \"layer3.2.bn1.weight\", \"layer3.2.conv2.weight\", \"layer3.2.bn2.weight\", \"layer3.2.conv3.weight\", \"layer3.2.bn3.weight\", \"layer3.3.conv1.weight\", \"layer3.3.bn1.weight\", \"layer3.3.conv2.weight\", \"layer3.3.bn2.weight\", \"layer3.3.conv3.weight\", \"layer3.3.bn3.weight\", \"layer3.4.conv1.weight\", \"layer3.4.bn1.weight\", \"layer3.4.conv2.weight\", \"layer3.4.bn2.weight\", \"layer3.4.conv3.weight\", \"layer3.4.bn3.weight\", \"layer3.5.conv1.weight\", \"layer3.5.bn1.weight\", \"layer3.5.conv2.weight\", \"layer3.5.bn2.weight\", \"layer3.5.conv3.weight\", \"layer3.5.bn3.weight\", \"layer4.0.conv1.weight\", \"layer4.0.bn1.weight\", \"layer4.0.conv2.weight\", \"layer4.0.bn2.weight\", \"layer4.0.conv3.weight\", \"layer4.0.bn3.weight\", \"layer4.0.downsample.0.weight\", \"layer4.0.downsample.1.weight\", \"layer4.1.conv1.weight\", \"layer4.1.bn1.weight\", \"layer4.1.conv2.weight\", \"layer4.1.bn2.weight\", \"layer4.1.conv3.weight\", \"layer4.1.bn3.weight\", \"layer4.2.conv1.weight\", \"layer4.2.bn1.weight\", \"layer4.2.conv2.weight\", \"layer4.2.bn2.weight\", \"layer4.2.conv3.weight\", \"layer4.2.bn3.weight\", \"fc.weight\". "
     ]
    }
   ],
   "source": [
    "# [1 point] 1. Repeat Exercise 2.4 (repeated unstructured pruning) for your model, on your device and with your data.\n",
    "# https://apple.github.io/coremltools/source/coremltools.optimize.torch.pruning.html\n",
    "pruning_results, model_paths = model_repeated_pruning(\"./models/model_weights_ResNet50_224_resize.pth\", 0.33)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_weights(model_path)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/briancurtin/Documents/GitHub/ODML_project/.venv/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "/var/folders/mv/q059yr4j7l18ms7xb987dwtm0000gn/T/ipykernel_40481/2142981156.py:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=torch.device(\"cpu\")))\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for ResNet:\n\tMissing key(s) in state_dict: \"conv1.weight\", \"bn1.weight\", \"layer1.0.conv1.weight\", \"layer1.0.bn1.weight\", \"layer1.0.conv2.weight\", \"layer1.0.bn2.weight\", \"layer1.0.conv3.weight\", \"layer1.0.bn3.weight\", \"layer1.0.downsample.0.weight\", \"layer1.0.downsample.1.weight\", \"layer1.1.conv1.weight\", \"layer1.1.bn1.weight\", \"layer1.1.conv2.weight\", \"layer1.1.bn2.weight\", \"layer1.1.conv3.weight\", \"layer1.1.bn3.weight\", \"layer1.2.conv1.weight\", \"layer1.2.bn1.weight\", \"layer1.2.conv2.weight\", \"layer1.2.bn2.weight\", \"layer1.2.conv3.weight\", \"layer1.2.bn3.weight\", \"layer2.0.conv1.weight\", \"layer2.0.bn1.weight\", \"layer2.0.conv2.weight\", \"layer2.0.bn2.weight\", \"layer2.0.conv3.weight\", \"layer2.0.bn3.weight\", \"layer2.0.downsample.0.weight\", \"layer2.0.downsample.1.weight\", \"layer2.1.conv1.weight\", \"layer2.1.bn1.weight\", \"layer2.1.conv2.weight\", \"layer2.1.bn2.weight\", \"layer2.1.conv3.weight\", \"layer2.1.bn3.weight\", \"layer2.2.conv1.weight\", \"layer2.2.bn1.weight\", \"layer2.2.conv2.weight\", \"layer2.2.bn2.weight\", \"layer2.2.conv3.weight\", \"layer2.2.bn3.weight\", \"layer2.3.conv1.weight\", \"layer2.3.bn1.weight\", \"layer2.3.conv2.weight\", \"layer2.3.bn2.weight\", \"layer2.3.conv3.weight\", \"layer2.3.bn3.weight\", \"layer3.0.conv1.weight\", \"layer3.0.bn1.weight\", \"layer3.0.conv2.weight\", \"layer3.0.bn2.weight\", \"layer3.0.conv3.weight\", \"layer3.0.bn3.weight\", \"layer3.0.downsample.0.weight\", \"layer3.0.downsample.1.weight\", \"layer3.1.conv1.weight\", \"layer3.1.bn1.weight\", \"layer3.1.conv2.weight\", \"layer3.1.bn2.weight\", \"layer3.1.conv3.weight\", \"layer3.1.bn3.weight\", \"layer3.2.conv1.weight\", \"layer3.2.bn1.weight\", \"layer3.2.conv2.weight\", \"layer3.2.bn2.weight\", \"layer3.2.conv3.weight\", \"layer3.2.bn3.weight\", \"layer3.3.conv1.weight\", \"layer3.3.bn1.weight\", \"layer3.3.conv2.weight\", \"layer3.3.bn2.weight\", \"layer3.3.conv3.weight\", \"layer3.3.bn3.weight\", \"layer3.4.conv1.weight\", \"layer3.4.bn1.weight\", \"layer3.4.conv2.weight\", \"layer3.4.bn2.weight\", \"layer3.4.conv3.weight\", \"layer3.4.bn3.weight\", \"layer3.5.conv1.weight\", \"layer3.5.bn1.weight\", \"layer3.5.conv2.weight\", \"layer3.5.bn2.weight\", \"layer3.5.conv3.weight\", \"layer3.5.bn3.weight\", \"layer4.0.conv1.weight\", \"layer4.0.bn1.weight\", \"layer4.0.conv2.weight\", \"layer4.0.bn2.weight\", \"layer4.0.conv3.weight\", \"layer4.0.bn3.weight\", \"layer4.0.downsample.0.weight\", \"layer4.0.downsample.1.weight\", \"layer4.1.conv1.weight\", \"layer4.1.bn1.weight\", \"layer4.1.conv2.weight\", \"layer4.1.bn2.weight\", \"layer4.1.conv3.weight\", \"layer4.1.bn3.weight\", \"layer4.2.conv1.weight\", \"layer4.2.bn1.weight\", \"layer4.2.conv2.weight\", \"layer4.2.bn2.weight\", \"layer4.2.conv3.weight\", \"layer4.2.bn3.weight\", \"fc.weight\". \n\tUnexpected key(s) in state_dict: \"conv1.weight_orig\", \"conv1.weight_mask\", \"bn1.weight_orig\", \"bn1.weight_mask\", \"layer1.0.conv1.weight_orig\", \"layer1.0.conv1.weight_mask\", \"layer1.0.bn1.weight_orig\", \"layer1.0.bn1.weight_mask\", \"layer1.0.conv2.weight_orig\", \"layer1.0.conv2.weight_mask\", \"layer1.0.bn2.weight_orig\", \"layer1.0.bn2.weight_mask\", \"layer1.0.conv3.weight_orig\", \"layer1.0.conv3.weight_mask\", \"layer1.0.bn3.weight_orig\", \"layer1.0.bn3.weight_mask\", \"layer1.0.downsample.0.weight_orig\", \"layer1.0.downsample.0.weight_mask\", \"layer1.0.downsample.1.weight_orig\", \"layer1.0.downsample.1.weight_mask\", \"layer1.1.conv1.weight_orig\", \"layer1.1.conv1.weight_mask\", \"layer1.1.bn1.weight_orig\", \"layer1.1.bn1.weight_mask\", \"layer1.1.conv2.weight_orig\", \"layer1.1.conv2.weight_mask\", \"layer1.1.bn2.weight_orig\", \"layer1.1.bn2.weight_mask\", \"layer1.1.conv3.weight_orig\", \"layer1.1.conv3.weight_mask\", \"layer1.1.bn3.weight_orig\", \"layer1.1.bn3.weight_mask\", \"layer1.2.conv1.weight_orig\", \"layer1.2.conv1.weight_mask\", \"layer1.2.bn1.weight_orig\", \"layer1.2.bn1.weight_mask\", \"layer1.2.conv2.weight_orig\", \"layer1.2.conv2.weight_mask\", \"layer1.2.bn2.weight_orig\", \"layer1.2.bn2.weight_mask\", \"layer1.2.conv3.weight_orig\", \"layer1.2.conv3.weight_mask\", \"layer1.2.bn3.weight_orig\", \"layer1.2.bn3.weight_mask\", \"layer2.0.conv1.weight_orig\", \"layer2.0.conv1.weight_mask\", \"layer2.0.bn1.weight_orig\", \"layer2.0.bn1.weight_mask\", \"layer2.0.conv2.weight_orig\", \"layer2.0.conv2.weight_mask\", \"layer2.0.bn2.weight_orig\", \"layer2.0.bn2.weight_mask\", \"layer2.0.conv3.weight_orig\", \"layer2.0.conv3.weight_mask\", \"layer2.0.bn3.weight_orig\", \"layer2.0.bn3.weight_mask\", \"layer2.0.downsample.0.weight_orig\", \"layer2.0.downsample.0.weight_mask\", \"layer2.0.downsample.1.weight_orig\", \"layer2.0.downsample.1.weight_mask\", \"layer2.1.conv1.weight_orig\", \"layer2.1.conv1.weight_mask\", \"layer2.1.bn1.weight_orig\", \"layer2.1.bn1.weight_mask\", \"layer2.1.conv2.weight_orig\", \"layer2.1.conv2.weight_mask\", \"layer2.1.bn2.weight_orig\", \"layer2.1.bn2.weight_mask\", \"layer2.1.conv3.weight_orig\", \"layer2.1.conv3.weight_mask\", \"layer2.1.bn3.weight_orig\", \"layer2.1.bn3.weight_mask\", \"layer2.2.conv1.weight_orig\", \"layer2.2.conv1.weight_mask\", \"layer2.2.bn1.weight_orig\", \"layer2.2.bn1.weight_mask\", \"layer2.2.conv2.weight_orig\", \"layer2.2.conv2.weight_mask\", \"layer2.2.bn2.weight_orig\", \"layer2.2.bn2.weight_mask\", \"layer2.2.conv3.weight_orig\", \"layer2.2.conv3.weight_mask\", \"layer2.2.bn3.weight_orig\", \"layer2.2.bn3.weight_mask\", \"layer2.3.conv1.weight_orig\", \"layer2.3.conv1.weight_mask\", \"layer2.3.bn1.weight_orig\", \"layer2.3.bn1.weight_mask\", \"layer2.3.conv2.weight_orig\", \"layer2.3.conv2.weight_mask\", \"layer2.3.bn2.weight_orig\", \"layer2.3.bn2.weight_mask\", \"layer2.3.conv3.weight_orig\", \"layer2.3.conv3.weight_mask\", \"layer2.3.bn3.weight_orig\", \"layer2.3.bn3.weight_mask\", \"layer3.0.conv1.weight_orig\", \"layer3.0.conv1.weight_mask\", \"layer3.0.bn1.weight_orig\", \"layer3.0.bn1.weight_mask\", \"layer3.0.conv2.weight_orig\", \"layer3.0.conv2.weight_mask\", \"layer3.0.bn2.weight_orig\", \"layer3.0.bn2.weight_mask\", \"layer3.0.conv3.weight_orig\", \"layer3.0.conv3.weight_mask\", \"layer3.0.bn3.weight_orig\", \"layer3.0.bn3.weight_mask\", \"layer3.0.downsample.0.weight_orig\", \"layer3.0.downsample.0.weight_mask\", \"layer3.0.downsample.1.weight_orig\", \"layer3.0.downsample.1.weight_mask\", \"layer3.1.conv1.weight_orig\", \"layer3.1.conv1.weight_mask\", \"layer3.1.bn1.weight_orig\", \"layer3.1.bn1.weight_mask\", \"layer3.1.conv2.weight_orig\", \"layer3.1.conv2.weight_mask\", \"layer3.1.bn2.weight_orig\", \"layer3.1.bn2.weight_mask\", \"layer3.1.conv3.weight_orig\", \"layer3.1.conv3.weight_mask\", \"layer3.1.bn3.weight_orig\", \"layer3.1.bn3.weight_mask\", \"layer3.2.conv1.weight_orig\", \"layer3.2.conv1.weight_mask\", \"layer3.2.bn1.weight_orig\", \"layer3.2.bn1.weight_mask\", \"layer3.2.conv2.weight_orig\", \"layer3.2.conv2.weight_mask\", \"layer3.2.bn2.weight_orig\", \"layer3.2.bn2.weight_mask\", \"layer3.2.conv3.weight_orig\", \"layer3.2.conv3.weight_mask\", \"layer3.2.bn3.weight_orig\", \"layer3.2.bn3.weight_mask\", \"layer3.3.conv1.weight_orig\", \"layer3.3.conv1.weight_mask\", \"layer3.3.bn1.weight_orig\", \"layer3.3.bn1.weight_mask\", \"layer3.3.conv2.weight_orig\", \"layer3.3.conv2.weight_mask\", \"layer3.3.bn2.weight_orig\", \"layer3.3.bn2.weight_mask\", \"layer3.3.conv3.weight_orig\", \"layer3.3.conv3.weight_mask\", \"layer3.3.bn3.weight_orig\", \"layer3.3.bn3.weight_mask\", \"layer3.4.conv1.weight_orig\", \"layer3.4.conv1.weight_mask\", \"layer3.4.bn1.weight_orig\", \"layer3.4.bn1.weight_mask\", \"layer3.4.conv2.weight_orig\", \"layer3.4.conv2.weight_mask\", \"layer3.4.bn2.weight_orig\", \"layer3.4.bn2.weight_mask\", \"layer3.4.conv3.weight_orig\", \"layer3.4.conv3.weight_mask\", \"layer3.4.bn3.weight_orig\", \"layer3.4.bn3.weight_mask\", \"layer3.5.conv1.weight_orig\", \"layer3.5.conv1.weight_mask\", \"layer3.5.bn1.weight_orig\", \"layer3.5.bn1.weight_mask\", \"layer3.5.conv2.weight_orig\", \"layer3.5.conv2.weight_mask\", \"layer3.5.bn2.weight_orig\", \"layer3.5.bn2.weight_mask\", \"layer3.5.conv3.weight_orig\", \"layer3.5.conv3.weight_mask\", \"layer3.5.bn3.weight_orig\", \"layer3.5.bn3.weight_mask\", \"layer4.0.conv1.weight_orig\", \"layer4.0.conv1.weight_mask\", \"layer4.0.bn1.weight_orig\", \"layer4.0.bn1.weight_mask\", \"layer4.0.conv2.weight_orig\", \"layer4.0.conv2.weight_mask\", \"layer4.0.bn2.weight_orig\", \"layer4.0.bn2.weight_mask\", \"layer4.0.conv3.weight_orig\", \"layer4.0.conv3.weight_mask\", \"layer4.0.bn3.weight_orig\", \"layer4.0.bn3.weight_mask\", \"layer4.0.downsample.0.weight_orig\", \"layer4.0.downsample.0.weight_mask\", \"layer4.0.downsample.1.weight_orig\", \"layer4.0.downsample.1.weight_mask\", \"layer4.1.conv1.weight_orig\", \"layer4.1.conv1.weight_mask\", \"layer4.1.bn1.weight_orig\", \"layer4.1.bn1.weight_mask\", \"layer4.1.conv2.weight_orig\", \"layer4.1.conv2.weight_mask\", \"layer4.1.bn2.weight_orig\", \"layer4.1.bn2.weight_mask\", \"layer4.1.conv3.weight_orig\", \"layer4.1.conv3.weight_mask\", \"layer4.1.bn3.weight_orig\", \"layer4.1.bn3.weight_mask\", \"layer4.2.conv1.weight_orig\", \"layer4.2.conv1.weight_mask\", \"layer4.2.bn1.weight_orig\", \"layer4.2.bn1.weight_mask\", \"layer4.2.conv2.weight_orig\", \"layer4.2.conv2.weight_mask\", \"layer4.2.bn2.weight_orig\", \"layer4.2.bn2.weight_mask\", \"layer4.2.conv3.weight_orig\", \"layer4.2.conv3.weight_mask\", \"layer4.2.bn3.weight_orig\", \"layer4.2.bn3.weight_mask\", \"fc.weight_orig\", \"fc.weight_mask\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m6\u001b[39m):\n\u001b[1;32m      5\u001b[0m     model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./models/pruned_model_iteration_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 6\u001b[0m     model, devie \u001b[38;5;241m=\u001b[39m \u001b[43mload_asl_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     prune_params \u001b[38;5;241m=\u001b[39m [(m[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mnamed_modules() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mlist\u001b[39m(m[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mchildren()))\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(m[\u001b[38;5;241m1\u001b[39m], (nn\u001b[38;5;241m.\u001b[39mReLU, nn\u001b[38;5;241m.\u001b[39mMaxPool2d, nn\u001b[38;5;241m.\u001b[39mAdaptiveAvgPool2d))]\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(model[\u001b[38;5;241m0\u001b[39m])\n",
      "Cell \u001b[0;32mIn[8], line 14\u001b[0m, in \u001b[0;36mload_asl_model\u001b[0;34m(model_path)\u001b[0m\n\u001b[1;32m     11\u001b[0m model\u001b[38;5;241m.\u001b[39mfc \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(num_ftrs, num_classes)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Load the saved weights\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/Documents/GitHub/ODML_project/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2584\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2576\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2577\u001b[0m             \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   2578\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2579\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[1;32m   2580\u001b[0m             ),\n\u001b[1;32m   2581\u001b[0m         )\n\u001b[1;32m   2583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2584\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   2585\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2586\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)\n\u001b[1;32m   2587\u001b[0m         )\n\u001b[1;32m   2588\u001b[0m     )\n\u001b[1;32m   2589\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for ResNet:\n\tMissing key(s) in state_dict: \"conv1.weight\", \"bn1.weight\", \"layer1.0.conv1.weight\", \"layer1.0.bn1.weight\", \"layer1.0.conv2.weight\", \"layer1.0.bn2.weight\", \"layer1.0.conv3.weight\", \"layer1.0.bn3.weight\", \"layer1.0.downsample.0.weight\", \"layer1.0.downsample.1.weight\", \"layer1.1.conv1.weight\", \"layer1.1.bn1.weight\", \"layer1.1.conv2.weight\", \"layer1.1.bn2.weight\", \"layer1.1.conv3.weight\", \"layer1.1.bn3.weight\", \"layer1.2.conv1.weight\", \"layer1.2.bn1.weight\", \"layer1.2.conv2.weight\", \"layer1.2.bn2.weight\", \"layer1.2.conv3.weight\", \"layer1.2.bn3.weight\", \"layer2.0.conv1.weight\", \"layer2.0.bn1.weight\", \"layer2.0.conv2.weight\", \"layer2.0.bn2.weight\", \"layer2.0.conv3.weight\", \"layer2.0.bn3.weight\", \"layer2.0.downsample.0.weight\", \"layer2.0.downsample.1.weight\", \"layer2.1.conv1.weight\", \"layer2.1.bn1.weight\", \"layer2.1.conv2.weight\", \"layer2.1.bn2.weight\", \"layer2.1.conv3.weight\", \"layer2.1.bn3.weight\", \"layer2.2.conv1.weight\", \"layer2.2.bn1.weight\", \"layer2.2.conv2.weight\", \"layer2.2.bn2.weight\", \"layer2.2.conv3.weight\", \"layer2.2.bn3.weight\", \"layer2.3.conv1.weight\", \"layer2.3.bn1.weight\", \"layer2.3.conv2.weight\", \"layer2.3.bn2.weight\", \"layer2.3.conv3.weight\", \"layer2.3.bn3.weight\", \"layer3.0.conv1.weight\", \"layer3.0.bn1.weight\", \"layer3.0.conv2.weight\", \"layer3.0.bn2.weight\", \"layer3.0.conv3.weight\", \"layer3.0.bn3.weight\", \"layer3.0.downsample.0.weight\", \"layer3.0.downsample.1.weight\", \"layer3.1.conv1.weight\", \"layer3.1.bn1.weight\", \"layer3.1.conv2.weight\", \"layer3.1.bn2.weight\", \"layer3.1.conv3.weight\", \"layer3.1.bn3.weight\", \"layer3.2.conv1.weight\", \"layer3.2.bn1.weight\", \"layer3.2.conv2.weight\", \"layer3.2.bn2.weight\", \"layer3.2.conv3.weight\", \"layer3.2.bn3.weight\", \"layer3.3.conv1.weight\", \"layer3.3.bn1.weight\", \"layer3.3.conv2.weight\", \"layer3.3.bn2.weight\", \"layer3.3.conv3.weight\", \"layer3.3.bn3.weight\", \"layer3.4.conv1.weight\", \"layer3.4.bn1.weight\", \"layer3.4.conv2.weight\", \"layer3.4.bn2.weight\", \"layer3.4.conv3.weight\", \"layer3.4.bn3.weight\", \"layer3.5.conv1.weight\", \"layer3.5.bn1.weight\", \"layer3.5.conv2.weight\", \"layer3.5.bn2.weight\", \"layer3.5.conv3.weight\", \"layer3.5.bn3.weight\", \"layer4.0.conv1.weight\", \"layer4.0.bn1.weight\", \"layer4.0.conv2.weight\", \"layer4.0.bn2.weight\", \"layer4.0.conv3.weight\", \"layer4.0.bn3.weight\", \"layer4.0.downsample.0.weight\", \"layer4.0.downsample.1.weight\", \"layer4.1.conv1.weight\", \"layer4.1.bn1.weight\", \"layer4.1.conv2.weight\", \"layer4.1.bn2.weight\", \"layer4.1.conv3.weight\", \"layer4.1.bn3.weight\", \"layer4.2.conv1.weight\", \"layer4.2.bn1.weight\", \"layer4.2.conv2.weight\", \"layer4.2.bn2.weight\", \"layer4.2.conv3.weight\", \"layer4.2.bn3.weight\", \"fc.weight\". \n\tUnexpected key(s) in state_dict: \"conv1.weight_orig\", \"conv1.weight_mask\", \"bn1.weight_orig\", \"bn1.weight_mask\", \"layer1.0.conv1.weight_orig\", \"layer1.0.conv1.weight_mask\", \"layer1.0.bn1.weight_orig\", \"layer1.0.bn1.weight_mask\", \"layer1.0.conv2.weight_orig\", \"layer1.0.conv2.weight_mask\", \"layer1.0.bn2.weight_orig\", \"layer1.0.bn2.weight_mask\", \"layer1.0.conv3.weight_orig\", \"layer1.0.conv3.weight_mask\", \"layer1.0.bn3.weight_orig\", \"layer1.0.bn3.weight_mask\", \"layer1.0.downsample.0.weight_orig\", \"layer1.0.downsample.0.weight_mask\", \"layer1.0.downsample.1.weight_orig\", \"layer1.0.downsample.1.weight_mask\", \"layer1.1.conv1.weight_orig\", \"layer1.1.conv1.weight_mask\", \"layer1.1.bn1.weight_orig\", \"layer1.1.bn1.weight_mask\", \"layer1.1.conv2.weight_orig\", \"layer1.1.conv2.weight_mask\", \"layer1.1.bn2.weight_orig\", \"layer1.1.bn2.weight_mask\", \"layer1.1.conv3.weight_orig\", \"layer1.1.conv3.weight_mask\", \"layer1.1.bn3.weight_orig\", \"layer1.1.bn3.weight_mask\", \"layer1.2.conv1.weight_orig\", \"layer1.2.conv1.weight_mask\", \"layer1.2.bn1.weight_orig\", \"layer1.2.bn1.weight_mask\", \"layer1.2.conv2.weight_orig\", \"layer1.2.conv2.weight_mask\", \"layer1.2.bn2.weight_orig\", \"layer1.2.bn2.weight_mask\", \"layer1.2.conv3.weight_orig\", \"layer1.2.conv3.weight_mask\", \"layer1.2.bn3.weight_orig\", \"layer1.2.bn3.weight_mask\", \"layer2.0.conv1.weight_orig\", \"layer2.0.conv1.weight_mask\", \"layer2.0.bn1.weight_orig\", \"layer2.0.bn1.weight_mask\", \"layer2.0.conv2.weight_orig\", \"layer2.0.conv2.weight_mask\", \"layer2.0.bn2.weight_orig\", \"layer2.0.bn2.weight_mask\", \"layer2.0.conv3.weight_orig\", \"layer2.0.conv3.weight_mask\", \"layer2.0.bn3.weight_orig\", \"layer2.0.bn3.weight_mask\", \"layer2.0.downsample.0.weight_orig\", \"layer2.0.downsample.0.weight_mask\", \"layer2.0.downsample.1.weight_orig\", \"layer2.0.downsample.1.weight_mask\", \"layer2.1.conv1.weight_orig\", \"layer2.1.conv1.weight_mask\", \"layer2.1.bn1.weight_orig\", \"layer2.1.bn1.weight_mask\", \"layer2.1.conv2.weight_orig\", \"layer2.1.conv2.weight_mask\", \"layer2.1.bn2.weight_orig\", \"layer2.1.bn2.weight_mask\", \"layer2.1.conv3.weight_orig\", \"layer2.1.conv3.weight_mask\", \"layer2.1.bn3.weight_orig\", \"layer2.1.bn3.weight_mask\", \"layer2.2.conv1.weight_orig\", \"layer2.2.conv1.weight_mask\", \"layer2.2.bn1.weight_orig\", \"layer2.2.bn1.weight_mask\", \"layer2.2.conv2.weight_orig\", \"layer2.2.conv2.weight_mask\", \"layer2.2.bn2.weight_orig\", \"layer2.2.bn2.weight_mask\", \"layer2.2.conv3.weight_orig\", \"layer2.2.conv3.weight_mask\", \"layer2.2.bn3.weight_orig\", \"layer2.2.bn3.weight_mask\", \"layer2.3.conv1.weight_orig\", \"layer2.3.conv1.weight_mask\", \"layer2.3.bn1.weight_orig\", \"layer2.3.bn1.weight_mask\", \"layer2.3.conv2.weight_orig\", \"layer2.3.conv2.weight_mask\", \"layer2.3.bn2.weight_orig\", \"layer2.3.bn2.weight_mask\", \"layer2.3.conv3.weight_orig\", \"layer2.3.conv3.weight_mask\", \"layer2.3.bn3.weight_orig\", \"layer2.3.bn3.weight_mask\", \"layer3.0.conv1.weight_orig\", \"layer3.0.conv1.weight_mask\", \"layer3.0.bn1.weight_orig\", \"layer3.0.bn1.weight_mask\", \"layer3.0.conv2.weight_orig\", \"layer3.0.conv2.weight_mask\", \"layer3.0.bn2.weight_orig\", \"layer3.0.bn2.weight_mask\", \"layer3.0.conv3.weight_orig\", \"layer3.0.conv3.weight_mask\", \"layer3.0.bn3.weight_orig\", \"layer3.0.bn3.weight_mask\", \"layer3.0.downsample.0.weight_orig\", \"layer3.0.downsample.0.weight_mask\", \"layer3.0.downsample.1.weight_orig\", \"layer3.0.downsample.1.weight_mask\", \"layer3.1.conv1.weight_orig\", \"layer3.1.conv1.weight_mask\", \"layer3.1.bn1.weight_orig\", \"layer3.1.bn1.weight_mask\", \"layer3.1.conv2.weight_orig\", \"layer3.1.conv2.weight_mask\", \"layer3.1.bn2.weight_orig\", \"layer3.1.bn2.weight_mask\", \"layer3.1.conv3.weight_orig\", \"layer3.1.conv3.weight_mask\", \"layer3.1.bn3.weight_orig\", \"layer3.1.bn3.weight_mask\", \"layer3.2.conv1.weight_orig\", \"layer3.2.conv1.weight_mask\", \"layer3.2.bn1.weight_orig\", \"layer3.2.bn1.weight_mask\", \"layer3.2.conv2.weight_orig\", \"layer3.2.conv2.weight_mask\", \"layer3.2.bn2.weight_orig\", \"layer3.2.bn2.weight_mask\", \"layer3.2.conv3.weight_orig\", \"layer3.2.conv3.weight_mask\", \"layer3.2.bn3.weight_orig\", \"layer3.2.bn3.weight_mask\", \"layer3.3.conv1.weight_orig\", \"layer3.3.conv1.weight_mask\", \"layer3.3.bn1.weight_orig\", \"layer3.3.bn1.weight_mask\", \"layer3.3.conv2.weight_orig\", \"layer3.3.conv2.weight_mask\", \"layer3.3.bn2.weight_orig\", \"layer3.3.bn2.weight_mask\", \"layer3.3.conv3.weight_orig\", \"layer3.3.conv3.weight_mask\", \"layer3.3.bn3.weight_orig\", \"layer3.3.bn3.weight_mask\", \"layer3.4.conv1.weight_orig\", \"layer3.4.conv1.weight_mask\", \"layer3.4.bn1.weight_orig\", \"layer3.4.bn1.weight_mask\", \"layer3.4.conv2.weight_orig\", \"layer3.4.conv2.weight_mask\", \"layer3.4.bn2.weight_orig\", \"layer3.4.bn2.weight_mask\", \"layer3.4.conv3.weight_orig\", \"layer3.4.conv3.weight_mask\", \"layer3.4.bn3.weight_orig\", \"layer3.4.bn3.weight_mask\", \"layer3.5.conv1.weight_orig\", \"layer3.5.conv1.weight_mask\", \"layer3.5.bn1.weight_orig\", \"layer3.5.bn1.weight_mask\", \"layer3.5.conv2.weight_orig\", \"layer3.5.conv2.weight_mask\", \"layer3.5.bn2.weight_orig\", \"layer3.5.bn2.weight_mask\", \"layer3.5.conv3.weight_orig\", \"layer3.5.conv3.weight_mask\", \"layer3.5.bn3.weight_orig\", \"layer3.5.bn3.weight_mask\", \"layer4.0.conv1.weight_orig\", \"layer4.0.conv1.weight_mask\", \"layer4.0.bn1.weight_orig\", \"layer4.0.bn1.weight_mask\", \"layer4.0.conv2.weight_orig\", \"layer4.0.conv2.weight_mask\", \"layer4.0.bn2.weight_orig\", \"layer4.0.bn2.weight_mask\", \"layer4.0.conv3.weight_orig\", \"layer4.0.conv3.weight_mask\", \"layer4.0.bn3.weight_orig\", \"layer4.0.bn3.weight_mask\", \"layer4.0.downsample.0.weight_orig\", \"layer4.0.downsample.0.weight_mask\", \"layer4.0.downsample.1.weight_orig\", \"layer4.0.downsample.1.weight_mask\", \"layer4.1.conv1.weight_orig\", \"layer4.1.conv1.weight_mask\", \"layer4.1.bn1.weight_orig\", \"layer4.1.bn1.weight_mask\", \"layer4.1.conv2.weight_orig\", \"layer4.1.conv2.weight_mask\", \"layer4.1.bn2.weight_orig\", \"layer4.1.bn2.weight_mask\", \"layer4.1.conv3.weight_orig\", \"layer4.1.conv3.weight_mask\", \"layer4.1.bn3.weight_orig\", \"layer4.1.bn3.weight_mask\", \"layer4.2.conv1.weight_orig\", \"layer4.2.conv1.weight_mask\", \"layer4.2.bn1.weight_orig\", \"layer4.2.bn1.weight_mask\", \"layer4.2.conv2.weight_orig\", \"layer4.2.conv2.weight_mask\", \"layer4.2.bn2.weight_orig\", \"layer4.2.bn2.weight_mask\", \"layer4.2.conv3.weight_orig\", \"layer4.2.conv3.weight_mask\", \"layer4.2.bn3.weight_orig\", \"layer4.2.bn3.weight_mask\", \"fc.weight_orig\", \"fc.weight_mask\". "
     ]
    }
   ],
   "source": [
    "path = \"./models/\"\n",
    "\n",
    "\n",
    "for i in range(1,6):\n",
    "    model_path = f\"./models/pruned_model_iteration_{i}.pth\"\n",
    "    model, devie = load_asl_model(model_path)\n",
    "    prune_params = [(m[1], \"weight\") for m in model.named_modules() if len(list(m[1].children()))==0 and not isinstance(m[1], (nn.ReLU, nn.MaxPool2d, nn.AdaptiveAvgPool2d))]\n",
    "    print(model[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_coreml(model_path):\n",
    "\n",
    "    # Define the labels for the classes (A-Z, del, nothing, space)\n",
    "    labels = [\n",
    "        'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', \n",
    "        'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', \n",
    "        'del', 'nothing', 'space'\n",
    "    ]\n",
    "        \n",
    "    # Load the trained PyTorch model\n",
    "    # Ensure the model architecture matches the one used during training\n",
    "    num_classes = len(labels)  # 29 classes\n",
    "    model = models.resnet50(pretrained=False)\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "\n",
    "    prune_params = [(m[1], \"weight\") for m in model.named_modules() if len(list(m[1].children()))==0 and not isinstance(m[1], (nn.ReLU, nn.MaxPool2d, nn.AdaptiveAvgPool2d))]\n",
    "    for p in prune_params:\n",
    "        prune.identity(p[0], \"weight\")\n",
    "    # Copy the parameters\n",
    "    model.load_state_dict(torch.load(model_path, map_location=torch.device(\"cpu\")))\n",
    "    \n",
    "    copy_params = [(m[1], \"weight\") for m in model.named_modules() if len(list(m[1].children()))==0 and not isinstance(m[1], (nn.ReLU, nn.MaxPool2d, nn.AdaptiveAvgPool2d))]\n",
    "    # (we assume the same model architecture)\n",
    "    for p in copy_params:\n",
    "        prune.remove(*p)\n",
    "\n",
    "    # Load the saved weights\n",
    "    print(f\"Model weights loaded from {model_path}\")\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # 1. Create an example input tensor resized to 224x224\n",
    "    example_input = torch.rand(1, 3, 224, 224)  # Batch size 1, 3 color channels, 224x224 image\n",
    "\n",
    "    # 2. Convert the model to TorchScript\n",
    "    traced_model = torch.jit.trace(model, example_input)\n",
    "    print(\"Model converted to TorchScript format\")\n",
    "\n",
    "    # 3. Convert the TorchScript model to CoreML format\n",
    "    mlmodel = ct.convert(\n",
    "        traced_model,\n",
    "        inputs=[ct.ImageType(name=\"image\", shape=example_input.shape, channel_first=True)],\n",
    "        classifier_config=ct.ClassifierConfig(class_labels=labels, predicted_feature_name=\"classLabel\"),\n",
    "        convert_to=\"neuralnetwork\",  # Ensures compatibility with .mlmodel format\n",
    "    )\n",
    "    print(\"Model converted to CoreML format\")\n",
    "\n",
    "    # 4. Apply Post-Training Quantization to INT8\n",
    "    # quantized_mlmodel = ct.models.neural_network.quantization_utils.quantize_weights(\n",
    "    #     mlmodel, nbits=8  # Use 8-bit integer quantization\n",
    "    # )\n",
    "    # print(\"Model quantized to INT8\")\n",
    "\n",
    "    # 5. Save the CoreML model\n",
    "    mlmodel_path = model_path.replace(\".pth\", \".mlmodel\")  # Desired output filename\n",
    "    mlmodel.save(mlmodel_path)\n",
    "    print(f\"CoreML model saved as {mlmodel_path}\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/briancurtin/Documents/GitHub/ODML_project/.venv/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/briancurtin/Documents/GitHub/ODML_project/.venv/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "/var/folders/mv/q059yr4j7l18ms7xb987dwtm0000gn/T/ipykernel_14457/2160222615.py:21: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=torch.device(\"cpu\")))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weights loaded from models/pruned_model_iteration_1.pth\n",
      "Model converted to TorchScript format\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting PyTorch Frontend ==> MIL Ops: 100%|█████████▉| 440/441 [00:00<00:00, 6947.04 ops/s]\n",
      "Running MIL frontend_pytorch pipeline: 100%|██████████| 5/5 [00:00<00:00, 220.43 passes/s]\n",
      "Running MIL default pipeline: 100%|██████████| 86/86 [00:00<00:00, 179.31 passes/s]\n",
      "Running MIL backend_neuralnetwork pipeline: 100%|██████████| 9/9 [00:00<00:00, 361.91 passes/s]\n",
      "Translating MIL ==> NeuralNetwork Ops: 100%|██████████| 503/503 [00:03<00:00, 146.82 ops/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model converted to CoreML format\n",
      "CoreML model saved as models/pruned_model_iteration_1.mlmodel\n",
      "Model weights loaded from models/pruned_model_iteration_2.pth\n",
      "Model converted to TorchScript format\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting PyTorch Frontend ==> MIL Ops: 100%|█████████▉| 440/441 [00:00<00:00, 8648.45 ops/s]\n",
      "Running MIL frontend_pytorch pipeline: 100%|██████████| 5/5 [00:00<00:00, 231.05 passes/s]\n",
      "Running MIL default pipeline: 100%|██████████| 86/86 [00:00<00:00, 182.16 passes/s]\n",
      "Running MIL backend_neuralnetwork pipeline: 100%|██████████| 9/9 [00:00<00:00, 389.34 passes/s]\n",
      "Translating MIL ==> NeuralNetwork Ops: 100%|██████████| 503/503 [00:03<00:00, 165.53 ops/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model converted to CoreML format\n",
      "CoreML model saved as models/pruned_model_iteration_2.mlmodel\n",
      "Model weights loaded from models/pruned_model_iteration_3.pth\n",
      "Model converted to TorchScript format\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting PyTorch Frontend ==> MIL Ops: 100%|█████████▉| 440/441 [00:00<00:00, 8987.68 ops/s]\n",
      "Running MIL frontend_pytorch pipeline: 100%|██████████| 5/5 [00:00<00:00, 196.53 passes/s]\n",
      "Running MIL default pipeline: 100%|██████████| 86/86 [00:00<00:00, 186.10 passes/s]\n",
      "Running MIL backend_neuralnetwork pipeline: 100%|██████████| 9/9 [00:00<00:00, 378.14 passes/s]\n",
      "Translating MIL ==> NeuralNetwork Ops: 100%|██████████| 503/503 [00:03<00:00, 166.19 ops/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model converted to CoreML format\n",
      "CoreML model saved as models/pruned_model_iteration_3.mlmodel\n",
      "Model weights loaded from models/pruned_model_iteration_4.pth\n",
      "Model converted to TorchScript format\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting PyTorch Frontend ==> MIL Ops: 100%|█████████▉| 440/441 [00:00<00:00, 8988.77 ops/s]\n",
      "Running MIL frontend_pytorch pipeline: 100%|██████████| 5/5 [00:00<00:00, 223.01 passes/s]\n",
      "Running MIL default pipeline: 100%|██████████| 86/86 [00:00<00:00, 176.73 passes/s]\n",
      "Running MIL backend_neuralnetwork pipeline: 100%|██████████| 9/9 [00:00<00:00, 392.60 passes/s]\n",
      "Translating MIL ==> NeuralNetwork Ops: 100%|██████████| 503/503 [00:03<00:00, 164.13 ops/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model converted to CoreML format\n",
      "CoreML model saved as models/pruned_model_iteration_4.mlmodel\n",
      "Model weights loaded from models/pruned_model_iteration_5.pth\n",
      "Model converted to TorchScript format\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting PyTorch Frontend ==> MIL Ops: 100%|█████████▉| 440/441 [00:00<00:00, 8262.42 ops/s]\n",
      "Running MIL frontend_pytorch pipeline: 100%|██████████| 5/5 [00:00<00:00, 215.43 passes/s]\n",
      "Running MIL default pipeline: 100%|██████████| 86/86 [00:00<00:00, 175.20 passes/s]\n",
      "Running MIL backend_neuralnetwork pipeline: 100%|██████████| 9/9 [00:00<00:00, 392.58 passes/s]\n",
      "Translating MIL ==> NeuralNetwork Ops: 100%|██████████| 503/503 [00:03<00:00, 158.89 ops/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model converted to CoreML format\n",
      "CoreML model saved as models/pruned_model_iteration_5.mlmodel\n"
     ]
    }
   ],
   "source": [
    "# Convert each pytoch model to CoreML for testing\n",
    "for model_path in model_paths[1:]:\n",
    "    convert_to_coreml(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Iteration | Sparsity (%) | Accuracy | Latency (s) | Disk Size (MB) |\n",
    "| --------- | ------------ | -------- | ----------- | -------------- |\n",
    "|     0     |   0.0%       |  0.9990476190476191   |             |      847.00    |\n",
    "|     1     |   32.96%     |  0.9990476190476191   |             |      567.61    |\n",
    "|     2     |   55.05%     |  0.9990476190476191   |             |      380.44    |\n",
    "|     3     |   69.84%     |  0.9990476190476191   |             |      255.08    |\n",
    "|     4     |   79.76%     |  0.9990476190476191   |             |      171.10    |\n",
    "|     5     |   86.40%     |  0.9990476190476191   |             |      114.85    |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [2 points] 2. Choose two of the following three options to implement on your model, device, and data (1 point per option):\n",
    "\n",
    "1. Implement a structured pruning technique. You may prune dimensions of matrices, attention heads, entire layers, etc. Describe your strategy and report the results in a table, adjusting the \"sparsity rate\" column and as needed.\n",
    "\n",
    "    Fill in the following table with your results (choose any 2-3 pruned models to compare to the unpruned model):\n",
    "\n",
    "    | Structure Pruned | Sparsity Rate | Accuracy | Latency (s) | Disk Size (MB) |\n",
    "    | ---------------- | ------------- | -------- | ----------- | -------------- |\n",
    "    | Attention heads? |               |          |             |                |\n",
    "    | Layers?          |               |          |             |                |\n",
    "    | Other?           |               |          |             |                |\n",
    "\n",
    "\n",
    "\n",
    "2. Conduct a sensitivity analysis of pruning (structured or unstructured) different components of your model. For instance, what happens to your model's performance when you prune input embeddings vs hidden layer weights? Do earlier layers seem more or less important than later layers? You are not required to conduct a thorough study, but you should be able to draw a couple concrete conclusions.\n",
    "\n",
    "    Fill in the following table with your results (choose any 2-3 pruned models to compare to the unpruned model):\n",
    "\n",
    "    |        Pruning Technique        |  Sparsity Rate  | Accuracy | Latency (s) | Disk Size (MB) |\n",
    "    | ------------------------------- | --------------- | -------- | ----------- | -------------- |\n",
    "    | Unstructured, all non-embedding |  30% global     |          |             |                |\n",
    "    | Structured, attention heads     |  50% per module |          |             |                |\n",
    "\n",
    "\n",
    "\n",
    "3. Export and run your unpruned and a diverse sample of your pruned models on an inference runtime (ONNX runtime, TensorRT). Check out [the PyTorch ONNX docs](https://pytorch.org/docs/stable/onnx.html) and [this page](https://pytorch.org/tutorials/advanced/super_resolution_with_onnxruntime.html) for reference. Did you run into any challenges? Do you see latency benefits? Was anything surprising? Report inference latency and discuss.\n",
    "\n",
    "    Fill in the following table with your results (choose any 2-3 pruned models to compare to the unpruned model):\n",
    "\n",
    "    | Inference Runtime | Sparsity Rate | Latency (s) | Disk Size (MB) |\n",
    "    | ----------------- | ------------- | ----------- | -------------- |\n",
    "    | ONNX              |     0%        |             |                |\n",
    "    | ONNX (pruned)     |    30%        |             |                |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[2 points] 2. Choose two of the following three options to implement on your model, device, and data (1 point per option):\n",
    "#1. Implement a structured pruning technique. You may prune dimensions of matrices, attention heads, entire layers, etc. Describe your strategy and report the results in a table, adjusting the \"sparsity rate\" column and as needed.\n",
    "#2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
