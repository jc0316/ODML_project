{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Users/briancurtin/anaconda3/lib/python3.11/site-packages (2.0.1)\n",
      "Collecting torchvision\n",
      "  Obtaining dependency information for torchvision from https://files.pythonhosted.org/packages/28/57/4d7ad90be612f5ac6c4bdafcb0ff13e818e14a340a88c8ca00d9ed8c2dad/torchvision-0.20.1-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading torchvision-0.20.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: filelock in /Users/briancurtin/anaconda3/lib/python3.11/site-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /Users/briancurtin/anaconda3/lib/python3.11/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /Users/briancurtin/anaconda3/lib/python3.11/site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in /Users/briancurtin/anaconda3/lib/python3.11/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/briancurtin/anaconda3/lib/python3.11/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: numpy in /Users/briancurtin/anaconda3/lib/python3.11/site-packages (from torchvision) (1.23.2)\n",
      "Collecting torch\n",
      "  Obtaining dependency information for torch from https://files.pythonhosted.org/packages/d0/db/5d9cbfbc7968d79c5c09a0bc0bc3735da079f2fd07cc10498a62b320a480/torch-2.5.1-cp311-none-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading torch-2.5.1-cp311-none-macosx_11_0_arm64.whl.metadata (28 kB)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/briancurtin/anaconda3/lib/python3.11/site-packages (from torchvision) (9.4.0)\n",
      "Requirement already satisfied: fsspec in /Users/briancurtin/anaconda3/lib/python3.11/site-packages (from torch) (2023.4.0)\n",
      "Collecting sympy==1.13.1 (from torch)\n",
      "  Obtaining dependency information for sympy==1.13.1 from https://files.pythonhosted.org/packages/b2/fe/81695a1aa331a842b582453b605175f419fe8540355886031328089d840a/sympy-1.13.1-py3-none-any.whl.metadata\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/briancurtin/anaconda3/lib/python3.11/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/briancurtin/anaconda3/lib/python3.11/site-packages (from jinja2->torch) (2.1.1)\n",
      "Downloading torchvision-0.20.1-cp311-cp311-macosx_11_0_arm64.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.5.1-cp311-none-macosx_11_0_arm64.whl (63.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.9/63.9 MB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sympy, torch, torchvision\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.11.1\n",
      "    Uninstalling sympy-1.11.1:\n",
      "      Successfully uninstalled sympy-1.11.1\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.0.1\n",
      "    Uninstalling torch-2.0.1:\n",
      "      Successfully uninstalled torch-2.0.1\n",
      "Successfully installed sympy-1.13.1 torch-2.5.1 torchvision-0.20.1\n",
      "Requirement already satisfied: tqdm in /Users/briancurtin/anaconda3/lib/python3.11/site-packages (4.65.0)\n",
      "Requirement already satisfied: pandas in /Users/briancurtin/anaconda3/lib/python3.11/site-packages (1.5.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/briancurtin/anaconda3/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/briancurtin/anaconda3/lib/python3.11/site-packages (from pandas) (2022.7)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /Users/briancurtin/anaconda3/lib/python3.11/site-packages (from pandas) (1.23.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/briancurtin/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Collecting torch-pruning\n",
      "  Obtaining dependency information for torch-pruning from https://files.pythonhosted.org/packages/81/ec/13f593824a8161a3c5ebd3d16871b1f71f82f743be69e03fa78be6fce300/torch_pruning-1.5.0-py3-none-any.whl.metadata\n",
      "  Downloading torch_pruning-1.5.0-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: torch in /Users/briancurtin/anaconda3/lib/python3.11/site-packages (from torch-pruning) (2.5.1)\n",
      "Requirement already satisfied: numpy in /Users/briancurtin/anaconda3/lib/python3.11/site-packages (from torch-pruning) (1.23.2)\n",
      "Requirement already satisfied: filelock in /Users/briancurtin/anaconda3/lib/python3.11/site-packages (from torch->torch-pruning) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/briancurtin/anaconda3/lib/python3.11/site-packages (from torch->torch-pruning) (4.12.2)\n",
      "Requirement already satisfied: networkx in /Users/briancurtin/anaconda3/lib/python3.11/site-packages (from torch->torch-pruning) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/briancurtin/anaconda3/lib/python3.11/site-packages (from torch->torch-pruning) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /Users/briancurtin/anaconda3/lib/python3.11/site-packages (from torch->torch-pruning) (2023.4.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Users/briancurtin/anaconda3/lib/python3.11/site-packages (from torch->torch-pruning) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/briancurtin/anaconda3/lib/python3.11/site-packages (from sympy==1.13.1->torch->torch-pruning) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/briancurtin/anaconda3/lib/python3.11/site-packages (from jinja2->torch->torch-pruning) (2.1.1)\n",
      "Downloading torch_pruning-1.5.0-py3-none-any.whl (63 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: torch-pruning\n",
      "Successfully installed torch-pruning-1.5.0\n",
      "Collecting coremltools\n",
      "  Obtaining dependency information for coremltools from https://files.pythonhosted.org/packages/2b/e7/57229f49f3b5a970c1bf0bdcb47251ecda194435cddc0c107dd886288337/coremltools-8.1-cp311-none-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading coremltools-8.1-cp311-none-macosx_11_0_arm64.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: numpy>=1.14.5 in /Users/briancurtin/anaconda3/lib/python3.11/site-packages (from coremltools) (1.23.2)\n",
      "Collecting protobuf>=3.1.0 (from coremltools)\n",
      "  Obtaining dependency information for protobuf>=3.1.0 from https://files.pythonhosted.org/packages/1c/f2/baf397f3dd1d3e4af7e3f5a0382b868d25ac068eefe1ebde05132333436c/protobuf-5.28.3-cp38-abi3-macosx_10_9_universal2.whl.metadata\n",
      "  Downloading protobuf-5.28.3-cp38-abi3-macosx_10_9_universal2.whl.metadata (592 bytes)\n",
      "Requirement already satisfied: sympy in /Users/briancurtin/anaconda3/lib/python3.11/site-packages (from coremltools) (1.13.1)\n",
      "Requirement already satisfied: tqdm in /Users/briancurtin/anaconda3/lib/python3.11/site-packages (from coremltools) (4.65.0)\n",
      "Requirement already satisfied: packaging in /Users/briancurtin/anaconda3/lib/python3.11/site-packages (from coremltools) (23.0)\n",
      "Requirement already satisfied: attrs>=21.3.0 in /Users/briancurtin/anaconda3/lib/python3.11/site-packages (from coremltools) (24.2.0)\n",
      "Collecting cattrs (from coremltools)\n",
      "  Obtaining dependency information for cattrs from https://files.pythonhosted.org/packages/c8/d5/867e75361fc45f6de75fe277dd085627a9db5ebb511a87f27dc1396b5351/cattrs-24.1.2-py3-none-any.whl.metadata\n",
      "  Downloading cattrs-24.1.2-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting pyaml (from coremltools)\n",
      "  Obtaining dependency information for pyaml from https://files.pythonhosted.org/packages/a1/1b/16b79549afd3dec6d764456c07b8581134c74008d87303ba23906380fa57/pyaml-24.9.0-py3-none-any.whl.metadata\n",
      "  Downloading pyaml-24.9.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: PyYAML in /Users/briancurtin/anaconda3/lib/python3.11/site-packages (from pyaml->coremltools) (6.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/briancurtin/anaconda3/lib/python3.11/site-packages (from sympy->coremltools) (1.3.0)\n",
      "Downloading coremltools-8.1-cp311-none-macosx_11_0_arm64.whl (2.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-5.28.3-cp38-abi3-macosx_10_9_universal2.whl (414 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m414.7/414.7 kB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cattrs-24.1.2-py3-none-any.whl (66 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyaml-24.9.0-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: pyaml, protobuf, cattrs, coremltools\n",
      "Successfully installed cattrs-24.1.2 coremltools-8.1 protobuf-5.28.3 pyaml-24.9.0\n"
     ]
    }
   ],
   "source": [
    "# Install required libraries\n",
    "!pip install torch torchvision\n",
    "!pip install tqdm\n",
    "!pip install pandas\n",
    "!pip install torch-pruning\n",
    "!pip install -U coremltools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Torch version 2.5.1 has not been tested with coremltools. You may run into unexpected errors. Torch 2.4.0 is the most recent version that has been tested.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.utils.prune as prune\n",
    "import torch.nn.functional as F\n",
    "import torch_pruning as tp\n",
    "from copy import deepcopy\n",
    "import pandas as pd\n",
    "import os\n",
    "import heapq\n",
    "from tqdm import tqdm\n",
    "import coremltools as ct\n",
    "from coremltools.optimize.torch.pruning import ModuleMagnitudePrunerConfig, MagnitudePruner, MagnitudePrunerConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [3 points] Exercise 3: Your Model, Device, and Data\n",
    "\n",
    "\n",
    "In this section, you will repeat the simple experiments from Exercise 2 on your own model, device, and data. Additionally, you will choose two of three options for practical benefits to your pruned model's accuracy and latency. You may use a different sparsity level, higher or lower than 33%, if it makes sense for your settings. Make sure to report any changes you made and why you made them. Additionally, report any challenges encountered measuring latency or storage on your device.\n",
    "\n",
    "### [1 point] 1. Repeat Exercise 2.4 (repeated unstructured pruning) for your model, on your device and with your data.\n",
    "\n",
    "Keep performing the same unstructured magnitude pruning of your choice of sparsity level of the remaining weights on the same model (*without re-training or resetting the model*). You will apply the same function as above with the same 0.33 proportion parameter.\n",
    "\n",
    "Collect values for this table, keeping in mind that you will need to plot the results later. You might want to keep the values in Pandas DataFrames. Sparsity reported should be the percentage of *prunable* parameters pruned. \n",
    "\n",
    "| Iteration | Sparsity (%) | Accuracy | Latency (s) | Disk Size (MB) |\n",
    "| --------- | ------------ | -------- | ----------- | -------------- |\n",
    "|     0     |   0.0%       |          |             |                |\n",
    "|     1     |      ?       |          |             |                |\n",
    "|     2     |              |          |             |                |\n",
    "|     3     |              |          |             |                |\n",
    "|     4     |              |          |             |                |\n",
    "|     5     |              |          |             |                |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sparsity(model, print_results=False):\n",
    "    \"\"\"\n",
    "    Calculate the sparsity level (using the percent of elements that are 0) for:\n",
    "    - each parameter,\n",
    "    - all pruned parameters overall, and\n",
    "    - the model overall.\n",
    "    \n",
    "    Report each of these values: \n",
    "    - the sparsity level of each parameter, \n",
    "    - across all pruned parameters, and \n",
    "    - for the model overall. \n",
    "    \"\"\"\n",
    "    sparsity_per_parameter = {}\n",
    "    total_zero_count_pruned = 0\n",
    "    total_element_count_pruned = 0\n",
    "    total_zero_count_model = 0\n",
    "    total_element_count_model = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "    # Iterate over all buffers in the model\n",
    "    for name, buffer in model.named_buffers():\n",
    "        # Calculate the number of zero elements and total elements in the buffer\n",
    "        zero_count = (buffer == 0).sum().item()\n",
    "        total_elements = buffer.numel()\n",
    "        \n",
    "        # Calculate the sparsity level for this parameter\n",
    "        sparsity_per_parameter[name] = zero_count / total_elements * 100\n",
    "\n",
    "        # Check if this is a pruned parameter by looking for \"weight_mask\" or \"bias_mask\" in the name\n",
    "        if \"weight_mask\" in name or \"bias_mask\" in name:\n",
    "            total_zero_count_pruned += zero_count\n",
    "            total_element_count_pruned += total_elements\n",
    "\n",
    "        # Accumulate for overall model sparsity\n",
    "        total_zero_count_model += zero_count\n",
    "\n",
    "    # Calculate overall sparsity for pruned parameters and the entire model\n",
    "    sparsity_pruned_parameters = (total_zero_count_pruned / total_element_count_pruned * 100\n",
    "                                  if total_element_count_pruned > 0 else 0)\n",
    "    sparsity_model = total_zero_count_model / total_element_count_model * 100\n",
    "\n",
    "    # Print or return the results\n",
    "    if print_results:\n",
    "        print(\"Sparsity per parameter:\")\n",
    "        for name, sparsity in sparsity_per_parameter.items():\n",
    "            print(f\"  {name}: {sparsity:.2f}%\")\n",
    "        \n",
    "        print(f\"Sparsity across all pruned parameters: {sparsity_pruned_parameters:.2f}%\")\n",
    "        print(f\"Sparsity for the model overall: {sparsity_model:.2f}%\")\n",
    "\n",
    "    # Optionally, return the values for further use\n",
    "    return {\n",
    "        \"sparsity_per_parameter\": sparsity_per_parameter,\n",
    "        \"sparsity_pruned_parameters\": sparsity_pruned_parameters,\n",
    "        \"sparsity_model\": sparsity_model\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_size_of_model(model, label=\"\"):\n",
    "    sd = model.state_dict()\n",
    "    for item in sd:\n",
    "        sd[item] = model.state_dict()[item].to_sparse()\n",
    "    \n",
    "    torch.save(sd, \"temp.pt\")\n",
    "    size=os.path.getsize(\"temp.pt\")\n",
    "    #print(\"model: \",label,' \\t','Size (MB):', size/1e6)\n",
    "\n",
    "    os.remove('temp.pt')\n",
    "    return size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model):\n",
    "    model.eval()\n",
    "\n",
    "    size = print_size_of_model(model, \"sparse\")\n",
    "    \n",
    "    return size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_evaluate(model, device):\n",
    "    model.to(device)\n",
    "    model_params = [(m[1], \"weight\") for m in model.named_modules() if len(list(m[1].children()))==0 and not isinstance(m[1], (nn.ReLU, nn.MaxPool2d, nn.AdaptiveAvgPool2d))]\n",
    "\n",
    "    model_copy = safe_deepcopy(model, model_params)\n",
    "    model_copy_params = [(m[1], \"weight\") for m in model_copy.named_modules() if len(list(m[1].children()))==0 and not isinstance(m[1], (nn.ReLU, nn.MaxPool2d, nn.AdaptiveAvgPool2d))]\n",
    "\n",
    "    for p in model_copy_params:\n",
    "        prune.remove(*p)\n",
    "    \n",
    "    return evaluate_model(model_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_asl_model(model_path):\n",
    "    labels = [\n",
    "        'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', \n",
    "        'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', \n",
    "        'del', 'nothing', 'space'\n",
    "    ]\n",
    "    # Create model\n",
    "    num_classes = len(labels)  # 29 classes\n",
    "    model = models.resnet50(weights=False)\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "\n",
    "    # Load the saved weights\n",
    "    model.load_state_dict(torch.load(model_path, map_location=torch.device(\"cpu\")))\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    return model, device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_deepcopy(model, params_to_prune):\n",
    "\n",
    "    # detach the parameters in params_to_prune\n",
    "    for module, name in params_to_prune:\n",
    "        param = getattr(module, name)\n",
    "        setattr(module, name, param.detach())\n",
    "\n",
    "    return deepcopy(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_copy_of_model(model, model_path):\n",
    "    \n",
    "    model_params = [(m[1], \"weight\") for m in model.named_modules() if len(list(m[1].children()))==0 and not isinstance(m[1], (nn.ReLU, nn.MaxPool2d, nn.AdaptiveAvgPool2d))]\n",
    "\n",
    "    model_copy = safe_deepcopy(model, model_params)\n",
    "    model_copy_params = [(m[1], \"weight\") for m in model_copy.named_modules() if len(list(m[1].children()))==0 and not isinstance(m[1], (nn.ReLU, nn.MaxPool2d, nn.AdaptiveAvgPool2d))]\n",
    "\n",
    "    for p in model_copy_params:\n",
    "        prune.remove(*p)\n",
    "\n",
    "    torch.save(model_copy.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_repeated_pruning(model_path, sparsity, iterations):\n",
    "\n",
    "    # Create empty DataFrame to store results\n",
    "    results_df = pd.DataFrame(columns=[\n",
    "        'iteration',\n",
    "        'sparsity_model',\n",
    "        'size_mb'\n",
    "    ])\n",
    "\n",
    "    # Define the labels for the classes (A-Z, del, nothing, space)\n",
    "\n",
    "    model, device = load_asl_model(model_path)\n",
    "    print(f\"Model weights loaded from {model_path}\")\n",
    "\n",
    "    prune_params = [(m[1], \"weight\") for m in model.named_modules() if len(list(m[1].children()))==0 and not isinstance(m[1], (nn.ReLU, nn.MaxPool2d, nn.AdaptiveAvgPool2d))]\n",
    "\n",
    "    # Store initial results (iteration 0)\n",
    "    sparsity_results = calculate_sparsity(model)\n",
    "    size = evaluate_model(model)\n",
    "    \n",
    "    results_df.loc[0] = {\n",
    "        'iteration': 0,\n",
    "        'sparsity_model': sparsity_results['sparsity_model'],\n",
    "        'size_mb': size/1e6\n",
    "    }\n",
    "\n",
    "    model_path_list = [model_path]\n",
    "\n",
    "    model, device = load_asl_model(model_path)\n",
    "    prune_params = [(m[1], \"weight\") for m in model.named_modules() if len(list(m[1].children()))==0 and not isinstance(m[1], (nn.ReLU, nn.MaxPool2d, nn.AdaptiveAvgPool2d))]\n",
    "\n",
    "    print(f\"Iteration 0 - Model Sparsity: {sparsity_results['sparsity_model']:.2f}%\")\n",
    "\n",
    "    for i in range(1, 6):\n",
    "        print(f\"Pruning iteration {i}\")\n",
    "\n",
    "        prune.global_unstructured(prune_params, pruning_method=prune.L1Unstructured, amount=sparsity)\n",
    "\n",
    "        saved_model_path = f\"models/pruned_model_{i}.pth\"\n",
    "        save_copy_of_model(model, saved_model_path)\n",
    "        model_path_list.append(saved_model_path)\n",
    "\n",
    "        size = sparse_evaluate(model, device)\n",
    "        sparsity_results = calculate_sparsity(model)\n",
    "\n",
    "\n",
    "        # Store results in DataFrame\n",
    "        results_df.loc[i] = {\n",
    "            'iteration': i,\n",
    "            'sparsity_model': sparsity_results['sparsity_model'],\n",
    "            'size_mb': size/1e6\n",
    "        }\n",
    "\n",
    "        print(f\"Sparsity for the model overall at Iteration {i}: {sparsity_results['sparsity_model']:.2f}%, Size MB: {size/1e6}\")\n",
    "    \n",
    "    \n",
    "    # Display the DataFrame\n",
    "    print(\"\\nFinal Results DataFrame:\")\n",
    "    print(results_df)\n",
    "    \n",
    "    return results_df, model_path_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/briancurtin/anaconda3/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "/var/folders/mv/q059yr4j7l18ms7xb987dwtm0000gn/T/ipykernel_95287/2142981156.py:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=torch.device(\"cpu\")))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weights loaded from ./models/model_weights_ResNet50_224_resize.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/briancurtin/anaconda3/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "/var/folders/mv/q059yr4j7l18ms7xb987dwtm0000gn/T/ipykernel_95287/2142981156.py:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=torch.device(\"cpu\")))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 - Model Sparsity: 0.00%\n",
      "Pruning iteration 1\n",
      "Sparsity for the model overall at Iteration 1: 32.96%, Size MB: 567.609062\n",
      "Pruning iteration 2\n",
      "Sparsity for the model overall at Iteration 2: 55.05%, Size MB: 380.443878\n",
      "Pruning iteration 3\n",
      "Sparsity for the model overall at Iteration 3: 69.84%, Size MB: 255.07703\n",
      "Pruning iteration 4\n",
      "Sparsity for the model overall at Iteration 4: 79.76%, Size MB: 171.102054\n",
      "Pruning iteration 5\n",
      "Sparsity for the model overall at Iteration 5: 86.40%, Size MB: 114.853414\n",
      "\n",
      "Final Results DataFrame:\n",
      "   iteration  sparsity_model     size_mb\n",
      "0          0        0.000000  847.002150\n",
      "1          1       32.962769  567.609062\n",
      "2          2       55.047824  380.443878\n",
      "3          3       69.844811  255.077030\n",
      "4          4       79.758793  171.102054\n",
      "5          5       86.401161  114.853414\n"
     ]
    }
   ],
   "source": [
    "# [1 point] 1. Repeat Exercise 2.4 (repeated unstructured pruning) for your model, on your device and with your data.\n",
    "# https://apple.github.io/coremltools/source/coremltools.optimize.torch.pruning.html\n",
    "pruning_results, model_paths = model_repeated_pruning(\"./models/model_weights_ResNet50_224_resize.pth\", sparsity=0.33, iterations=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./models/\"\n",
    "\n",
    "\n",
    "for i in range(1,6):\n",
    "    model_path = f\"./models/pruned_model_{i}.pth\"\n",
    "    print(f\"Model: {model_path}\")\n",
    "    model, devie = load_asl_model(model_path)\n",
    "    print(model)\n",
    "    prune_params = [(m[1], \"weight\") for m in model.named_modules() if len(list(m[1].children()))==0 and not isinstance(m[1], (nn.ReLU, nn.MaxPool2d, nn.AdaptiveAvgPool2d))]\n",
    "    print(prune_params[0][0].weight[2][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_coreml(model_path):\n",
    "\n",
    "    # Define the labels for the classes (A-Z, del, nothing, space)\n",
    "    labels = [\n",
    "        'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', \n",
    "        'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', \n",
    "        'del', 'nothing', 'space'\n",
    "    ]\n",
    "        \n",
    "    # Load the trained PyTorch model\n",
    "    # Ensure the model architecture matches the one used during training\n",
    "    num_classes = len(labels)  # 29 classes\n",
    "    model = models.resnet50(pretrained=False)\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "\n",
    "\n",
    "    model.load_state_dict(torch.load(model_path, map_location=torch.device(\"cpu\")))\n",
    "    model.eval()\n",
    "\n",
    "    preprocess = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize the image to 224x224\n",
    "    transforms.ToTensor(),          # Convert image to tensor\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),  # Normalize for ResNet\n",
    "    ])\n",
    "\n",
    "\n",
    "    # 1. Create an example input tensor resized to 224x224\n",
    "    example_input = torch.rand(1, 3, 224, 224)  # Batch size 1, 3 color channels, 224x224 image\n",
    "\n",
    "    # 2. Convert the model to TorchScript\n",
    "    traced_model = torch.jit.trace(model, example_input)\n",
    "    print(\"Model converted to TorchScript format\")\n",
    "\n",
    "    # 3. Convert the TorchScript model to CoreML format\n",
    "    mlmodel = ct.convert(\n",
    "        traced_model,\n",
    "        inputs=[ct.ImageType(\n",
    "        name=\"image\",\n",
    "        shape=example_input.shape,\n",
    "        channel_first=True,\n",
    "        scale=1/255.0,  # Scale input to [0,1]\n",
    "        bias=[-0.485/0.229, -0.456/0.224, -0.406/0.225],  # Mean divided by std for normalization\n",
    "        color_layout=\"RGB\",\n",
    "        )],\n",
    "        classifier_config=ct.ClassifierConfig(class_labels=labels, predicted_feature_name=\"classLabel\"),\n",
    "        convert_to=\"neuralnetwork\",\n",
    "    )\n",
    "    print(\"Model converted to CoreML format\")\n",
    "\n",
    "    # 4. Apply Post-Training Quantization to INT8\n",
    "    # quantized_mlmodel = ct.models.neural_network.quantization_utils.quantize_weights(\n",
    "    #     mlmodel, nbits=8  # Use 8-bit integer quantization\n",
    "    # )\n",
    "    # print(\"Model quantized to INT8\")\n",
    "\n",
    "    # 5. Save the CoreML model\n",
    "    mlmodel_path = model_path.replace(\".pth\", \".mlmodel\")  # Desired output filename\n",
    "    mlmodel.save(mlmodel_path)\n",
    "    print(f\"CoreML model saved as {mlmodel_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/briancurtin/anaconda3/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/briancurtin/anaconda3/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "/var/folders/mv/q059yr4j7l18ms7xb987dwtm0000gn/T/ipykernel_95287/1637419765.py:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=torch.device(\"cpu\")))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model converted to TorchScript format\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting PyTorch Frontend ==> MIL Ops: 100%|█████████▉| 440/441 [00:00<00:00, 6265.02 ops/s]\n",
      "Running MIL frontend_pytorch pipeline: 100%|██████████| 5/5 [00:00<00:00, 240.49 passes/s]\n",
      "Running MIL default pipeline: 100%|██████████| 87/87 [00:00<00:00, 191.33 passes/s]\n",
      "Running MIL backend_neuralnetwork pipeline: 100%|██████████| 9/9 [00:00<00:00, 421.58 passes/s]\n",
      "Translating MIL ==> NeuralNetwork Ops: 100%|██████████| 503/503 [00:03<00:00, 149.82 ops/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model converted to CoreML format\n",
      "CoreML model saved as models/pruned_model_1.mlmodel\n",
      "Model converted to TorchScript format\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting PyTorch Frontend ==> MIL Ops: 100%|█████████▉| 440/441 [00:00<00:00, 8305.81 ops/s]\n",
      "Running MIL frontend_pytorch pipeline: 100%|██████████| 5/5 [00:00<00:00, 241.20 passes/s]\n",
      "Running MIL default pipeline: 100%|██████████| 87/87 [00:00<00:00, 178.63 passes/s]\n",
      "Running MIL backend_neuralnetwork pipeline: 100%|██████████| 9/9 [00:00<00:00, 396.86 passes/s]\n",
      "Translating MIL ==> NeuralNetwork Ops: 100%|██████████| 503/503 [00:03<00:00, 144.19 ops/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model converted to CoreML format\n",
      "CoreML model saved as models/pruned_model_2.mlmodel\n",
      "Model converted to TorchScript format\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting PyTorch Frontend ==> MIL Ops: 100%|█████████▉| 440/441 [00:00<00:00, 7631.84 ops/s]\n",
      "Running MIL frontend_pytorch pipeline: 100%|██████████| 5/5 [00:00<00:00, 227.77 passes/s]\n",
      "Running MIL default pipeline: 100%|██████████| 87/87 [00:00<00:00, 174.69 passes/s]\n",
      "Running MIL backend_neuralnetwork pipeline: 100%|██████████| 9/9 [00:00<00:00, 402.72 passes/s]\n",
      "Translating MIL ==> NeuralNetwork Ops: 100%|██████████| 503/503 [00:03<00:00, 139.77 ops/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model converted to CoreML format\n",
      "CoreML model saved as models/pruned_model_3.mlmodel\n",
      "Model converted to TorchScript format\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting PyTorch Frontend ==> MIL Ops: 100%|█████████▉| 440/441 [00:00<00:00, 8646.10 ops/s]\n",
      "Running MIL frontend_pytorch pipeline: 100%|██████████| 5/5 [00:00<00:00, 225.59 passes/s]\n",
      "Running MIL default pipeline: 100%|██████████| 87/87 [00:00<00:00, 188.55 passes/s]\n",
      "Running MIL backend_neuralnetwork pipeline: 100%|██████████| 9/9 [00:00<00:00, 418.53 passes/s]\n",
      "Translating MIL ==> NeuralNetwork Ops: 100%|██████████| 503/503 [00:03<00:00, 149.14 ops/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model converted to CoreML format\n",
      "CoreML model saved as models/pruned_model_4.mlmodel\n",
      "Model converted to TorchScript format\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting PyTorch Frontend ==> MIL Ops: 100%|█████████▉| 440/441 [00:00<00:00, 8130.82 ops/s]\n",
      "Running MIL frontend_pytorch pipeline: 100%|██████████| 5/5 [00:00<00:00, 237.53 passes/s]\n",
      "Running MIL default pipeline: 100%|██████████| 87/87 [00:00<00:00, 191.80 passes/s]\n",
      "Running MIL backend_neuralnetwork pipeline: 100%|██████████| 9/9 [00:00<00:00, 408.18 passes/s]\n",
      "Translating MIL ==> NeuralNetwork Ops: 100%|██████████| 503/503 [00:03<00:00, 144.66 ops/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model converted to CoreML format\n",
      "CoreML model saved as models/pruned_model_5.mlmodel\n"
     ]
    }
   ],
   "source": [
    "# Convert each pytoch model to CoreML for testing\n",
    "for model_path in model_paths[1:]:\n",
    "    convert_to_coreml(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Iteration | Sparsity (%) | Accuracy | Latency (s) | Disk Size (MB) |\n",
    "| --------- | ------------ | -------- | ----------- | -------------- |\n",
    "|     0     |   0.0%       |  0.7304  |  0.0067168  |      847.00    |\n",
    "|     1     |   32.96%     |  0.7035  |  0.0049252  |      567.61    |\n",
    "|     2     |   55.05%     |  0.3448  |  0.0052006  |      380.44    |\n",
    "|     3     |   69.84%     |  0.0379  |  0.0038571  |      255.08    |\n",
    "|     4     |   79.76%     |  0.0357  |  0.0040812  |      171.10    |\n",
    "|     5     |   86.40%     |  0.0357  |  0.0037301  |      114.85    |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [2 points] 2. Choose two of the following three options to implement on your model, device, and data (1 point per option):\n",
    "\n",
    "1. Implement a structured pruning technique. You may prune dimensions of matrices, attention heads, entire layers, etc. Describe your strategy and report the results in a table, adjusting the \"sparsity rate\" column and as needed.\n",
    "\n",
    "    Fill in the following table with your results (choose any 2-3 pruned models to compare to the unpruned model):\n",
    "\n",
    "    Channel (dimension) pruning\n",
    "\n",
    "    | Structure Pruned | Sparsity Rate | Accuracy | Latency (s) | Disk Size (MB) |\n",
    "    | ---------------- | ------------- | -------- | ----------- | -------------- |\n",
    "    | Conv Layers      |    10%        |  0.6755  |  0.004057   |     75.6       |\n",
    "    | Conv Layers      |    20%        |  0.3775  |  0.002604   |     59.8       |\n",
    "    | Conv Layers      |    30%        |  0.1478  |  0.001871   |     45.5       |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[2 points] 2. Choose two of the following three options to implement on your model, device, and data (1 point per option):\n",
    "#1. Implement a structured pruning technique. You may prune dimensions of matrices, attention heads, entire layers, etc. Describe your strategy and report the results in a table, adjusting the \"sparsity rate\" column and as needed.\n",
    "# Prune by magnitutude of combined parameters in a Conv2d filer a) 64x64 b) 128x128 c)256x256 and its following batch norm\n",
    "#2. Unstructured magnitutude pruning of a)64 input layers b)128 input layers c)256 input layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_structured_pruning(model_path, sparsity, pruned_layer):\n",
    "    model, device = load_asl_model(model_path)\n",
    "    model.eval()\n",
    "\n",
    "    labels = [\n",
    "        'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', \n",
    "        'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', \n",
    "        'del', 'nothing', 'space'\n",
    "    ]\n",
    "        \n",
    "    # Load the trained PyTorch model\n",
    "    # Ensure the model architecture matches the one used during training\n",
    "    num_classes = len(labels)  # 29 classes\n",
    "\n",
    "    example_inputs = torch.randn(1, 3, 224, 224)\n",
    "\n",
    "    # 1. Importance criterion\n",
    "    imp = tp.importance.GroupNormImportance(p=2) # or GroupTaylorImportance(), GroupHessianImportance(), etc.\n",
    "\n",
    "    # 2. Initialize a pruner with the model and the importance criterion\n",
    "    ignored_layers = []\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, (torch.nn.Linear)):\n",
    "            ignored_layers.append(m) # DO NOT prune the final classifier!\n",
    "\n",
    "    print(ignored_layers)\n",
    "\n",
    "    pruner = tp.pruner.MetaPruner( # We can always choose MetaPruner if sparse training is not required.\n",
    "        model,\n",
    "        example_inputs,\n",
    "        importance=imp,\n",
    "        pruning_ratio=sparsity, # remove 50% channels, ResNet18 = {64, 128, 256, 512} => ResNet18_Half = {32, 64, 128, 256}\n",
    "        # pruning_ratio_dict = {model.conv1: 0.2, model.layer2: 0.8}, # customized pruning ratios for layers or blocks\n",
    "        ignored_layers=ignored_layers,\n",
    "        round_to=4, # It's recommended to round dims/channels to 4x or 8x for acceleration. Please see: https://docs.nvidia.com/deeplearning/performance/dl-performance-convolutional/index.html\n",
    "    )\n",
    "\n",
    "    # 3. Prune Model\n",
    "    base_macs, base_nparams = tp.utils.count_ops_and_params(model, example_inputs)\n",
    "    pruner.step()\n",
    "    macs, nparams = tp.utils.count_ops_and_params(model, example_inputs)\n",
    "    print(f\"MACs: {base_macs/1e9} G -> {macs/1e9} G, #Params: {base_nparams/1e6} M -> {nparams/1e6} M\")\n",
    "\n",
    "    save_path = f\"models/structued_pruning_{pruned_layer}.pth\"\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    print(f\"Saved model to {save_path}\")\n",
    "\n",
    "    # 1. Create an example input tensor resized to 224x224\n",
    "    example_input = torch.rand(1, 3, 224, 224)  # Batch size 1, 3 color channels, 224x224 image\n",
    "\n",
    "    # 2. Convert the model to TorchScript\n",
    "    traced_model = torch.jit.trace(model, example_input)\n",
    "    print(\"Model converted to TorchScript format\")\n",
    "\n",
    "    # 3. Convert the TorchScript model to CoreML format\n",
    "    mlmodel = ct.convert(\n",
    "        traced_model,\n",
    "        inputs=[ct.ImageType(\n",
    "        name=\"image\",\n",
    "        shape=example_input.shape,\n",
    "        channel_first=True,\n",
    "        scale=1/255.0,  # Scale input to [0,1]\n",
    "        bias=[-0.485/0.229, -0.456/0.224, -0.406/0.225],  # Mean divided by std for normalization\n",
    "        color_layout=\"RGB\",\n",
    "        )],\n",
    "        classifier_config=ct.ClassifierConfig(class_labels=labels, predicted_feature_name=\"classLabel\"),\n",
    "        convert_to=\"neuralnetwork\",\n",
    "    )\n",
    "    print(\"Model converted to CoreML format\")\n",
    "\n",
    "    # 4. Apply Post-Training Quantization to INT8\n",
    "    # quantized_mlmodel = ct.models.neural_network.quantization_utils.quantize_weights(\n",
    "    #     mlmodel, nbits=8  # Use 8-bit integer quantization\n",
    "    # )\n",
    "    # print(\"Model quantized to INT8\")\n",
    "\n",
    "    # 5. Save the CoreML model\n",
    "    mlmodel_path = save_path.replace(\".pth\", \".mlmodel\")  # Desired output filename\n",
    "    mlmodel.save(mlmodel_path)\n",
    "    print(f\"CoreML model saved as {mlmodel_path}\")\n",
    "    \n",
    "    return save_path\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mv/q059yr4j7l18ms7xb987dwtm0000gn/T/ipykernel_95287/2142981156.py:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=torch.device(\"cpu\")))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Linear(in_features=2048, out_features=29, bias=True)]\n",
      "MACs: 4.119935517 G -> 3.267928657 G, #Params: 23.567453 M -> 18.920045 M\n",
      "Saved model to models/structued_pruning_10.pth\n",
      "Model converted to TorchScript format\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting PyTorch Frontend ==> MIL Ops: 100%|█████████▉| 440/441 [00:00<00:00, 5374.18 ops/s]\n",
      "Running MIL frontend_pytorch pipeline: 100%|██████████| 5/5 [00:00<00:00, 180.14 passes/s]\n",
      "Running MIL default pipeline: 100%|██████████| 87/87 [00:00<00:00, 166.92 passes/s]\n",
      "Running MIL backend_neuralnetwork pipeline: 100%|██████████| 9/9 [00:00<00:00, 389.96 passes/s]\n",
      "Translating MIL ==> NeuralNetwork Ops: 100%|██████████| 503/503 [00:02<00:00, 179.45 ops/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model converted to CoreML format\n",
      "CoreML model saved as models/structued_pruning_10.mlmodel\n",
      "[Linear(in_features=2048, out_features=29, bias=True)]\n",
      "MACs: 4.119935517 G -> 2.580514873 G, #Params: 23.567453 M -> 14.963713 M\n",
      "Saved model to models/structued_pruning_20.pth\n",
      "Model converted to TorchScript format\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting PyTorch Frontend ==> MIL Ops: 100%|█████████▉| 440/441 [00:00<00:00, 8324.06 ops/s]\n",
      "Running MIL frontend_pytorch pipeline: 100%|██████████| 5/5 [00:00<00:00, 224.17 passes/s]\n",
      "Running MIL default pipeline: 100%|██████████| 87/87 [00:00<00:00, 191.64 passes/s]\n",
      "Running MIL backend_neuralnetwork pipeline: 100%|██████████| 9/9 [00:00<00:00, 203.63 passes/s]\n",
      "Translating MIL ==> NeuralNetwork Ops: 100%|██████████| 503/503 [00:02<00:00, 225.87 ops/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model converted to CoreML format\n",
      "CoreML model saved as models/structued_pruning_20.mlmodel\n",
      "[Linear(in_features=2048, out_features=29, bias=True)]\n",
      "MACs: 4.119935517 G -> 1.999082993 G, #Params: 23.567453 M -> 11.389361 M\n",
      "Saved model to models/structued_pruning_30.pth\n",
      "Model converted to TorchScript format\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting PyTorch Frontend ==> MIL Ops: 100%|█████████▉| 440/441 [00:00<00:00, 7338.97 ops/s]\n",
      "Running MIL frontend_pytorch pipeline: 100%|██████████| 5/5 [00:00<00:00, 245.19 passes/s]\n",
      "Running MIL default pipeline: 100%|██████████| 87/87 [00:00<00:00, 188.28 passes/s]\n",
      "Running MIL backend_neuralnetwork pipeline: 100%|██████████| 9/9 [00:00<00:00, 390.32 passes/s]\n",
      "Translating MIL ==> NeuralNetwork Ops: 100%|██████████| 503/503 [00:01<00:00, 294.45 ops/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model converted to CoreML format\n",
      "CoreML model saved as models/structued_pruning_30.mlmodel\n"
     ]
    }
   ],
   "source": [
    "strucuted_model_paths = []\n",
    "strucuted_model_paths.append(model_structured_pruning(model_path=\"./models/model_weights_ResNet50_224_resize.pth\", sparsity=0.1, pruned_layer=\"10\"))\n",
    "strucuted_model_paths.append(model_structured_pruning(model_path=\"./models/model_weights_ResNet50_224_resize.pth\", sparsity=0.2, pruned_layer=\"20\"))\n",
    "strucuted_model_paths.append(model_structured_pruning(model_path=\"./models/model_weights_ResNet50_224_resize.pth\", sparsity=0.3, pruned_layer=\"30\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Conduct a sensitivity analysis of pruning (structured or unstructured) different components of your model. For instance, what happens to your model's performance when you prune input embeddings vs hidden layer weights? Do earlier layers seem more or less important than later layers? You are not required to conduct a thorough study, but you should be able to draw a couple concrete conclusions.\n",
    "\n",
    "    Fill in the following table with your results (choose any 2-3 pruned models to compare to the unpruned model):\n",
    "\n",
    "    |        Pruning Technique        |  Sparsity Rate  | Accuracy | Latency (s) | Disk Size (MB) |\n",
    "    | ------------------------------- | --------------- | -------- | ----------- | -------------- |\n",
    "    | Unstructured, LayerNorm         |  20%      |          |             |                |\n",
    "    | Unstructured, LayerNorm         |  40%      |          |             |                |\n",
    "    | Unstructured, Conv2d            |  20%      |          |             |                |\n",
    "    | Unstructured, Conv2d            |  40%      |          |             |                |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_sensitivity_pruning(model_path, sparsity_list):\n",
    "\n",
    "    # Create empty DataFrame to store results\n",
    "    results_df = pd.DataFrame(columns=[\n",
    "        'iteration',\n",
    "        'sparsity_model',\n",
    "        'size_mb'\n",
    "    ])\n",
    "\n",
    "    # Define the labels for the classes (A-Z, del, nothing, space)\n",
    "\n",
    "    model, device = load_asl_model(model_path)\n",
    "    print(f\"Model weights loaded from {model_path}\")\n",
    "\n",
    "    prune_params = [(m[1], \"weight\") for m in model.named_modules() if len(list(m[1].children()))==0 and not isinstance(m[1], (nn.ReLU, nn.MaxPool2d, nn.AdaptiveAvgPool2d))]\n",
    "\n",
    "    # Store initial results (iteration 0)\n",
    "    sparsity_results = calculate_sparsity(model)\n",
    "    size = evaluate_model(model)\n",
    "    \n",
    "    results_df.loc[0] = {\n",
    "        'iteration': 0,\n",
    "        'sparsity_model': sparsity_results['sparsity_model'],\n",
    "        'size_mb': size/1e6\n",
    "    }\n",
    "\n",
    "    model_path_list = [model_path]\n",
    "\n",
    "    model, device = load_asl_model(model_path)\n",
    "    prune_params = [(m[1], \"weight\") for m in model.named_modules() if len(list(m[1].children()))==0 and not isinstance(m[1], (nn.ReLU, nn.MaxPool2d, nn.AdaptiveAvgPool2d))]\n",
    "\n",
    "    print(f\"Iteration 0 - Model Sparsity: {sparsity_results['sparsity_model']:.2f}%\")\n",
    "    for layer in [nn.Conv2d, nn.LayerNorm]\n",
    "    for sparsity in sparsity_list:\n",
    "        print(f\"Pruning iteration {i}\")\n",
    "\n",
    "        prune.global_unstructured(prune_params, pruning_method=prune.L1Unstructured, amount=sparsity)\n",
    "\n",
    "        saved_model_path = f\"models/pruned_model_{i}.pth\"\n",
    "        save_copy_of_model(model, saved_model_path)\n",
    "        model_path_list.append(saved_model_path)\n",
    "\n",
    "        size = sparse_evaluate(model, device)\n",
    "        sparsity_results = calculate_sparsity(model)\n",
    "\n",
    "\n",
    "        # Store results in DataFrame\n",
    "        results_df.loc[i] = {\n",
    "            'iteration': i,\n",
    "            'sparsity_model': sparsity_results['sparsity_model'],\n",
    "            'size_mb': size/1e6\n",
    "        }\n",
    "\n",
    "        print(f\"Sparsity for the model overall at Iteration {i}: {sparsity_results['sparsity_model']:.2f}%, Size MB: {size/1e6}\")\n",
    "    \n",
    "    \n",
    "    # Display the DataFrame\n",
    "    print(\"\\nFinal Results DataFrame:\")\n",
    "    print(results_df)\n",
    "    \n",
    "    return results_df, model_path_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sens_list = model_sensitivity_pruning(\"./models/model_weights_ResNet50_224_resize.pth\", sparsity_list=[0.2,0.4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
