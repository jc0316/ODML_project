{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in ./.venv/lib/python3.11/site-packages (2.5.1)\n",
      "Requirement already satisfied: torchvision in ./.venv/lib/python3.11/site-packages (0.20.1)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.11/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./.venv/lib/python3.11/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.11/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.11/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.11/site-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./.venv/lib/python3.11/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.11/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.11/site-packages (from torchvision) (2.1.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./.venv/lib/python3.11/site-packages (from torchvision) (11.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.11/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.11/site-packages (4.67.0)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.11/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.23.2 in ./.venv/lib/python3.11/site-packages (from pandas) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.11/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.11/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Collecting torch-pruning\n",
      "  Downloading torch_pruning-1.5.0-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: torch in ./.venv/lib/python3.11/site-packages (from torch-pruning) (2.5.1)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.11/site-packages (from torch-pruning) (2.1.3)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.11/site-packages (from torch->torch-pruning) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./.venv/lib/python3.11/site-packages (from torch->torch-pruning) (4.12.2)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.11/site-packages (from torch->torch-pruning) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.11/site-packages (from torch->torch-pruning) (3.1.4)\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.11/site-packages (from torch->torch-pruning) (2024.10.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./.venv/lib/python3.11/site-packages (from torch->torch-pruning) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.11/site-packages (from sympy==1.13.1->torch->torch-pruning) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.11/site-packages (from jinja2->torch->torch-pruning) (3.0.2)\n",
      "Downloading torch_pruning-1.5.0-py3-none-any.whl (63 kB)\n",
      "Installing collected packages: torch-pruning\n",
      "Successfully installed torch-pruning-1.5.0\n",
      "Requirement already satisfied: coremltools in ./.venv/lib/python3.11/site-packages (8.0)\n",
      "Collecting coremltools\n",
      "  Downloading coremltools-8.1-cp311-none-macosx_11_0_arm64.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: numpy>=1.14.5 in ./.venv/lib/python3.11/site-packages (from coremltools) (2.1.3)\n",
      "Requirement already satisfied: protobuf>=3.1.0 in ./.venv/lib/python3.11/site-packages (from coremltools) (5.28.3)\n",
      "Requirement already satisfied: sympy in ./.venv/lib/python3.11/site-packages (from coremltools) (1.13.1)\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.11/site-packages (from coremltools) (4.67.0)\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.11/site-packages (from coremltools) (24.2)\n",
      "Requirement already satisfied: attrs>=21.3.0 in ./.venv/lib/python3.11/site-packages (from coremltools) (24.2.0)\n",
      "Requirement already satisfied: cattrs in ./.venv/lib/python3.11/site-packages (from coremltools) (24.1.2)\n",
      "Requirement already satisfied: pyaml in ./.venv/lib/python3.11/site-packages (from coremltools) (24.9.0)\n",
      "Requirement already satisfied: PyYAML in ./.venv/lib/python3.11/site-packages (from pyaml->coremltools) (6.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.11/site-packages (from sympy->coremltools) (1.3.0)\n",
      "Downloading coremltools-8.1-cp311-none-macosx_11_0_arm64.whl (2.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: coremltools\n",
      "  Attempting uninstall: coremltools\n",
      "    Found existing installation: coremltools 8.0\n",
      "    Uninstalling coremltools-8.0:\n",
      "      Successfully uninstalled coremltools-8.0\n",
      "Successfully installed coremltools-8.1\n"
     ]
    }
   ],
   "source": [
    "# Install required libraries\n",
    "!pip install torch torchvision\n",
    "!pip install tqdm\n",
    "!pip install pandas\n",
    "!pip install torch-pruning\n",
    "!pip install -U coremltools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.utils.prune as prune\n",
    "import torch.nn.functional as F\n",
    "import torch_pruning as tp\n",
    "from copy import deepcopy\n",
    "import pandas as pd\n",
    "import os\n",
    "import heapq\n",
    "from tqdm import tqdm\n",
    "import coremltools as ct\n",
    "from coremltools.optimize.torch.pruning import ModuleMagnitudePrunerConfig, MagnitudePruner, MagnitudePrunerConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [3 points] Exercise 3: Your Model, Device, and Data\n",
    "\n",
    "\n",
    "In this section, you will repeat the simple experiments from Exercise 2 on your own model, device, and data. Additionally, you will choose two of three options for practical benefits to your pruned model's accuracy and latency. You may use a different sparsity level, higher or lower than 33%, if it makes sense for your settings. Make sure to report any changes you made and why you made them. Additionally, report any challenges encountered measuring latency or storage on your device.\n",
    "\n",
    "### [1 point] 1. Repeat Exercise 2.4 (repeated unstructured pruning) for your model, on your device and with your data.\n",
    "\n",
    "Keep performing the same unstructured magnitude pruning of your choice of sparsity level of the remaining weights on the same model (*without re-training or resetting the model*). You will apply the same function as above with the same 0.33 proportion parameter.\n",
    "\n",
    "Collect values for this table, keeping in mind that you will need to plot the results later. You might want to keep the values in Pandas DataFrames. Sparsity reported should be the percentage of *prunable* parameters pruned. \n",
    "\n",
    "| Iteration | Sparsity (%) | Accuracy | Latency (s) | Disk Size (MB) |\n",
    "| --------- | ------------ | -------- | ----------- | -------------- |\n",
    "|     0     |   0.0%       |          |             |                |\n",
    "|     1     |      ?       |          |             |                |\n",
    "|     2     |              |          |             |                |\n",
    "|     3     |              |          |             |                |\n",
    "|     4     |              |          |             |                |\n",
    "|     5     |              |          |             |                |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sparsity(model, print_results=False):\n",
    "    \"\"\"\n",
    "    Calculate the sparsity level (using the percent of elements that are 0) for:\n",
    "    - each parameter,\n",
    "    - all pruned parameters overall, and\n",
    "    - the model overall.\n",
    "    \n",
    "    Report each of these values: \n",
    "    - the sparsity level of each parameter, \n",
    "    - across all pruned parameters, and \n",
    "    - for the model overall. \n",
    "    \"\"\"\n",
    "    sparsity_per_parameter = {}\n",
    "    total_zero_count_pruned = 0\n",
    "    total_element_count_pruned = 0\n",
    "    total_zero_count_model = 0\n",
    "    total_element_count_model = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "    # Iterate over all buffers in the model\n",
    "    for name, buffer in model.named_buffers():\n",
    "        # Calculate the number of zero elements and total elements in the buffer\n",
    "        zero_count = (buffer == 0).sum().item()\n",
    "        total_elements = buffer.numel()\n",
    "        \n",
    "        # Calculate the sparsity level for this parameter\n",
    "        sparsity_per_parameter[name] = zero_count / total_elements * 100\n",
    "\n",
    "        # Check if this is a pruned parameter by looking for \"weight_mask\" or \"bias_mask\" in the name\n",
    "        if \"weight_mask\" in name or \"bias_mask\" in name:\n",
    "            total_zero_count_pruned += zero_count\n",
    "            total_element_count_pruned += total_elements\n",
    "\n",
    "        # Accumulate for overall model sparsity\n",
    "        total_zero_count_model += zero_count\n",
    "\n",
    "    # Calculate overall sparsity for pruned parameters and the entire model\n",
    "    sparsity_pruned_parameters = (total_zero_count_pruned / total_element_count_pruned * 100\n",
    "                                  if total_element_count_pruned > 0 else 0)\n",
    "    sparsity_model = total_zero_count_model / total_element_count_model * 100\n",
    "\n",
    "    # Print or return the results\n",
    "    if print_results:\n",
    "        print(\"Sparsity per parameter:\")\n",
    "        for name, sparsity in sparsity_per_parameter.items():\n",
    "            print(f\"  {name}: {sparsity:.2f}%\")\n",
    "        \n",
    "        print(f\"Sparsity across all pruned parameters: {sparsity_pruned_parameters:.2f}%\")\n",
    "        print(f\"Sparsity for the model overall: {sparsity_model:.2f}%\")\n",
    "\n",
    "    # Optionally, return the values for further use\n",
    "    return {\n",
    "        \"sparsity_per_parameter\": sparsity_per_parameter,\n",
    "        \"sparsity_pruned_parameters\": sparsity_pruned_parameters,\n",
    "        \"sparsity_model\": sparsity_model\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_size_of_model(model, label=\"\"):\n",
    "    sd = model.state_dict()\n",
    "    for item in sd:\n",
    "        sd[item] = model.state_dict()[item].to_sparse()\n",
    "    \n",
    "    torch.save(sd, \"temp.pt\")\n",
    "    size=os.path.getsize(\"temp.pt\")\n",
    "    #print(\"model: \",label,' \\t','Size (MB):', size/1e6)\n",
    "\n",
    "    os.remove('temp.pt')\n",
    "    return size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model):\n",
    "    model.eval()\n",
    "\n",
    "    size = print_size_of_model(model, \"sparse\")\n",
    "    \n",
    "    return size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_evaluate(model, device):\n",
    "    model.to(device)\n",
    "    model_params = [(m[1], \"weight\") for m in model.named_modules() if len(list(m[1].children()))==0 and not isinstance(m[1], (nn.ReLU, nn.MaxPool2d, nn.AdaptiveAvgPool2d))]\n",
    "\n",
    "    model_copy = safe_deepcopy(model, model_params)\n",
    "    model_copy_params = [(m[1], \"weight\") for m in model_copy.named_modules() if len(list(m[1].children()))==0 and not isinstance(m[1], (nn.ReLU, nn.MaxPool2d, nn.AdaptiveAvgPool2d))]\n",
    "\n",
    "    for p in model_copy_params:\n",
    "        prune.remove(*p)\n",
    "    \n",
    "    return evaluate_model(model_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_asl_model(model_path):\n",
    "    labels = [\n",
    "        'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', \n",
    "        'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', \n",
    "        'del', 'nothing', 'space'\n",
    "    ]\n",
    "    # Create model\n",
    "    num_classes = len(labels)  # 29 classes\n",
    "    model = models.resnet50(weights=False)\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "\n",
    "    # Load the saved weights\n",
    "    model.load_state_dict(torch.load(model_path, map_location=torch.device(\"cpu\")))\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    return model, device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_deepcopy(model, params_to_prune):\n",
    "\n",
    "    # detach the parameters in params_to_prune\n",
    "    for module, name in params_to_prune:\n",
    "        param = getattr(module, name)\n",
    "        setattr(module, name, param.detach())\n",
    "\n",
    "    return deepcopy(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_copy_of_model(model, model_path):\n",
    "    \n",
    "    model_params = [(m[1], \"weight\") for m in model.named_modules() if len(list(m[1].children()))==0 and not isinstance(m[1], (nn.ReLU, nn.MaxPool2d, nn.AdaptiveAvgPool2d))]\n",
    "\n",
    "    model_copy = safe_deepcopy(model, model_params)\n",
    "    model_copy_params = [(m[1], \"weight\") for m in model_copy.named_modules() if len(list(m[1].children()))==0 and not isinstance(m[1], (nn.ReLU, nn.MaxPool2d, nn.AdaptiveAvgPool2d))]\n",
    "\n",
    "    for p in model_copy_params:\n",
    "        prune.remove(*p)\n",
    "\n",
    "    torch.save(model_copy.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_repeated_pruning(model_path, sparsity, iterations):\n",
    "\n",
    "    # Create empty DataFrame to store results\n",
    "    results_df = pd.DataFrame(columns=[\n",
    "        'iteration',\n",
    "        'sparsity_model',\n",
    "        'size_mb'\n",
    "    ])\n",
    "\n",
    "    # Define the labels for the classes (A-Z, del, nothing, space)\n",
    "\n",
    "    model, device = load_asl_model(model_path)\n",
    "    print(f\"Model weights loaded from {model_path}\")\n",
    "\n",
    "    prune_params = [(m[1], \"weight\") for m in model.named_modules() if len(list(m[1].children()))==0 and not isinstance(m[1], (nn.ReLU, nn.MaxPool2d, nn.AdaptiveAvgPool2d))]\n",
    "\n",
    "    # Store initial results (iteration 0)\n",
    "    sparsity_results = calculate_sparsity(model)\n",
    "    size = evaluate_model(model)\n",
    "    \n",
    "    results_df.loc[0] = {\n",
    "        'iteration': 0,\n",
    "        'sparsity_model': sparsity_results['sparsity_model'],\n",
    "        'size_mb': size/1e6\n",
    "    }\n",
    "\n",
    "    model_path_list = [model_path]\n",
    "\n",
    "    model, device = load_asl_model(model_path)\n",
    "    prune_params = [(m[1], \"weight\") for m in model.named_modules() if len(list(m[1].children()))==0 and not isinstance(m[1], (nn.ReLU, nn.MaxPool2d, nn.AdaptiveAvgPool2d))]\n",
    "\n",
    "    print(f\"Iteration 0 - Model Sparsity: {sparsity_results['sparsity_model']:.2f}%\")\n",
    "\n",
    "    for i in range(1, 6):\n",
    "        print(f\"Pruning iteration {i}\")\n",
    "\n",
    "        prune.global_unstructured(prune_params, pruning_method=prune.L1Unstructured, amount=sparsity)\n",
    "\n",
    "        saved_model_path = f\"models/pruned_model_{i}.pth\"\n",
    "        save_copy_of_model(model, saved_model_path)\n",
    "        model_path_list.append(saved_model_path)\n",
    "\n",
    "        size = sparse_evaluate(model, device)\n",
    "        sparsity_results = calculate_sparsity(model)\n",
    "\n",
    "\n",
    "        # Store results in DataFrame\n",
    "        results_df.loc[i] = {\n",
    "            'iteration': i,\n",
    "            'sparsity_model': sparsity_results['sparsity_model'],\n",
    "            'size_mb': size/1e6\n",
    "        }\n",
    "\n",
    "        print(f\"Sparsity for the model overall at Iteration {i}: {sparsity_results['sparsity_model']:.2f}%, Size MB: {size/1e6}\")\n",
    "    \n",
    "    \n",
    "    # Display the DataFrame\n",
    "    print(\"\\nFinal Results DataFrame:\")\n",
    "    print(results_df)\n",
    "    \n",
    "    return results_df, model_path_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/briancurtin/Documents/GitHub/ODML_project/.venv/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "/var/folders/mv/q059yr4j7l18ms7xb987dwtm0000gn/T/ipykernel_40481/2142981156.py:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=torch.device(\"cpu\")))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weights loaded from ./models/model_weights_ResNet50_224_resize.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/briancurtin/Documents/GitHub/ODML_project/.venv/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "/var/folders/mv/q059yr4j7l18ms7xb987dwtm0000gn/T/ipykernel_40481/2142981156.py:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=torch.device(\"cpu\")))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 - Model Sparsity: 0.00%\n",
      "Pruning iteration 1\n",
      "Sparsity for the model overall at Iteration 1: 32.96%, Size MB: 567.609062\n",
      "Pruning iteration 2\n",
      "Sparsity for the model overall at Iteration 2: 55.05%, Size MB: 380.443878\n",
      "Pruning iteration 3\n",
      "Sparsity for the model overall at Iteration 3: 69.84%, Size MB: 255.07703\n",
      "Pruning iteration 4\n",
      "Sparsity for the model overall at Iteration 4: 79.76%, Size MB: 171.102054\n",
      "Pruning iteration 5\n",
      "Sparsity for the model overall at Iteration 5: 86.40%, Size MB: 114.853414\n",
      "\n",
      "Final Results DataFrame:\n",
      "   iteration  sparsity_model     size_mb\n",
      "0          0        0.000000  847.002150\n",
      "1          1       32.962769  567.609062\n",
      "2          2       55.047824  380.443878\n",
      "3          3       69.844811  255.077030\n",
      "4          4       79.758793  171.102054\n",
      "5          5       86.401161  114.853414\n"
     ]
    }
   ],
   "source": [
    "# [1 point] 1. Repeat Exercise 2.4 (repeated unstructured pruning) for your model, on your device and with your data.\n",
    "# https://apple.github.io/coremltools/source/coremltools.optimize.torch.pruning.html\n",
    "pruning_results, model_paths = model_repeated_pruning(\"./models/model_weights_ResNet50_224_resize.pth\", sparsity=0.33, iterations=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: ./models/pruned_model_1.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/briancurtin/Documents/GitHub/ODML_project/.venv/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "/var/folders/mv/q059yr4j7l18ms7xb987dwtm0000gn/T/ipykernel_40481/2142981156.py:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=torch.device(\"cpu\")))\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './models/pruned_model_1.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[94], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./models/pruned_model_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m model, devie \u001b[38;5;241m=\u001b[39m \u001b[43mload_asl_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(model)\n\u001b[1;32m      9\u001b[0m prune_params \u001b[38;5;241m=\u001b[39m [(m[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mnamed_modules() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mlist\u001b[39m(m[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mchildren()))\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(m[\u001b[38;5;241m1\u001b[39m], (nn\u001b[38;5;241m.\u001b[39mReLU, nn\u001b[38;5;241m.\u001b[39mMaxPool2d, nn\u001b[38;5;241m.\u001b[39mAdaptiveAvgPool2d))]\n",
      "Cell \u001b[0;32mIn[74], line 14\u001b[0m, in \u001b[0;36mload_asl_model\u001b[0;34m(model_path)\u001b[0m\n\u001b[1;32m     11\u001b[0m model\u001b[38;5;241m.\u001b[39mfc \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(num_ftrs, num_classes)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Load the saved weights\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     16\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/Documents/GitHub/ODML_project/.venv/lib/python3.11/site-packages/torch/serialization.py:1319\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m   1317\u001b[0m     pickle_load_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1319\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m   1321\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m   1322\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m   1323\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m   1324\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/Documents/GitHub/ODML_project/.venv/lib/python3.11/site-packages/torch/serialization.py:659\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    658\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 659\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    660\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    661\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/Documents/GitHub/ODML_project/.venv/lib/python3.11/site-packages/torch/serialization.py:640\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 640\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mopen\u001b[39m(name, mode))\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './models/pruned_model_1.pth'"
     ]
    }
   ],
   "source": [
    "path = \"./models/\"\n",
    "\n",
    "\n",
    "for i in range(1,6):\n",
    "    model_path = f\"./models/pruned_model_{i}.pth\"\n",
    "    print(f\"Model: {model_path}\")\n",
    "    model, devie = load_asl_model(model_path)\n",
    "    print(model)\n",
    "    prune_params = [(m[1], \"weight\") for m in model.named_modules() if len(list(m[1].children()))==0 and not isinstance(m[1], (nn.ReLU, nn.MaxPool2d, nn.AdaptiveAvgPool2d))]\n",
    "    print(prune_params[0][0].weight[2][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_coreml(model_path):\n",
    "\n",
    "    # Define the labels for the classes (A-Z, del, nothing, space)\n",
    "    labels = [\n",
    "        'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', \n",
    "        'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', \n",
    "        'del', 'nothing', 'space'\n",
    "    ]\n",
    "        \n",
    "    # Load the trained PyTorch model\n",
    "    # Ensure the model architecture matches the one used during training\n",
    "    num_classes = len(labels)  # 29 classes\n",
    "    model = models.resnet50(pretrained=False)\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "\n",
    "    #prune_params = [(m[1], \"weight\") for m in model.named_modules() if len(list(m[1].children()))==0 and not isinstance(m[1], (nn.ReLU, nn.MaxPool2d, nn.AdaptiveAvgPool2d))]\n",
    "    #for p in prune_params:\n",
    "    #    prune.identity(p[0], \"weight\")\n",
    "    # Copy the parameters\n",
    "    #model.load_state_dict(torch.load(model_path, map_location=torch.device(\"cpu\")))\n",
    "    \n",
    "    #copy_params = [(m[1], \"weight\") for m in model.named_modules() if len(list(m[1].children()))==0 and not isinstance(m[1], (nn.ReLU, nn.MaxPool2d, nn.AdaptiveAvgPool2d))]\n",
    "    # (we assume the same model architecture)\n",
    "    #for p in copy_params:\n",
    "    #    prune.remove(*p)\n",
    "\n",
    "    # Load the saved weights\n",
    "    #print(f\"Model weights loaded from {model_path}\")\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # 1. Create an example input tensor resized to 224x224\n",
    "    example_input = torch.rand(1, 3, 224, 224)  # Batch size 1, 3 color channels, 224x224 image\n",
    "\n",
    "    # 2. Convert the model to TorchScript\n",
    "    traced_model = torch.jit.trace(model, example_input)\n",
    "    print(\"Model converted to TorchScript format\")\n",
    "\n",
    "    # 3. Convert the TorchScript model to CoreML format\n",
    "    mlmodel = ct.convert(\n",
    "        traced_model,\n",
    "        inputs=[ct.ImageType(name=\"image\", shape=example_input.shape, channel_first=True)],\n",
    "        classifier_config=ct.ClassifierConfig(class_labels=labels, predicted_feature_name=\"classLabel\"),\n",
    "        convert_to=\"neuralnetwork\",  # Ensures compatibility with .mlmodel format\n",
    "    )\n",
    "    print(\"Model converted to CoreML format\")\n",
    "\n",
    "    # 4. Apply Post-Training Quantization to INT8\n",
    "    # quantized_mlmodel = ct.models.neural_network.quantization_utils.quantize_weights(\n",
    "    #     mlmodel, nbits=8  # Use 8-bit integer quantization\n",
    "    # )\n",
    "    # print(\"Model quantized to INT8\")\n",
    "\n",
    "    # 5. Save the CoreML model\n",
    "    mlmodel_path = model_path.replace(\".pth\", \".mlmodel\")  # Desired output filename\n",
    "    mlmodel.save(mlmodel_path)\n",
    "    print(f\"CoreML model saved as {mlmodel_path}\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model is not in eval mode. Consider calling '.eval()' on your model prior to conversion\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model converted to TorchScript format\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting PyTorch Frontend ==> MIL Ops: 100%|█████████▉| 498/499 [00:00<00:00, 3742.95 ops/s]\n",
      "Running MIL frontend_pytorch pipeline: 100%|██████████| 5/5 [00:00<00:00, 121.63 passes/s]\n",
      "Running MIL default pipeline: 100%|██████████| 86/86 [00:00<00:00, 126.33 passes/s]\n",
      "Running MIL backend_neuralnetwork pipeline: 100%|██████████| 9/9 [00:00<00:00, 196.64 passes/s]\n",
      "Translating MIL ==> NeuralNetwork Ops: 100%|██████████| 1038/1038 [00:03<00:00, 285.10 ops/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model converted to CoreML format\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 22] Invalid argument: '.'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[203], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Convert each pytoch model to CoreML for testing\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model_path \u001b[38;5;129;01min\u001b[39;00m model_paths[\u001b[38;5;241m0\u001b[39m]:\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mconvert_to_coreml\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[101], line 58\u001b[0m, in \u001b[0;36mconvert_to_coreml\u001b[0;34m(model_path)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# 4. Apply Post-Training Quantization to INT8\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# quantized_mlmodel = ct.models.neural_network.quantization_utils.quantize_weights(\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m#     mlmodel, nbits=8  # Use 8-bit integer quantization\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     55\u001b[0m \n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# 5. Save the CoreML model\u001b[39;00m\n\u001b[1;32m     57\u001b[0m mlmodel_path \u001b[38;5;241m=\u001b[39m model_path\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.mlmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Desired output filename\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m \u001b[43mmlmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmlmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCoreML model saved as \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmlmodel_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/GitHub/ODML_project/.venv/lib/python3.11/site-packages/coremltools/models/model.py:576\u001b[0m, in \u001b[0;36msave\u001b[0;34m(self, save_path)\u001b[0m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/shutil.py:738\u001b[0m, in \u001b[0;36mrmtree\u001b[0;34m(path, ignore_errors, onerror, dir_fd)\u001b[0m\n\u001b[1;32m    736\u001b[0m         os\u001b[38;5;241m.\u001b[39mrmdir(path, dir_fd\u001b[38;5;241m=\u001b[39mdir_fd)\n\u001b[1;32m    737\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[0;32m--> 738\u001b[0m         \u001b[43monerror\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrmdir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    739\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    740\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    741\u001b[0m         \u001b[38;5;66;03m# symlinks to directories are forbidden, see bug #1669\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/shutil.py:736\u001b[0m, in \u001b[0;36mrmtree\u001b[0;34m(path, ignore_errors, onerror, dir_fd)\u001b[0m\n\u001b[1;32m    734\u001b[0m     os\u001b[38;5;241m.\u001b[39mclose(fd)\n\u001b[1;32m    735\u001b[0m     fd_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 736\u001b[0m     \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrmdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdir_fd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdir_fd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    737\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m    738\u001b[0m     onerror(os\u001b[38;5;241m.\u001b[39mrmdir, path, sys\u001b[38;5;241m.\u001b[39mexc_info())\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 22] Invalid argument: '.'"
     ]
    }
   ],
   "source": [
    "# Convert each pytoch model to CoreML for testing\n",
    "for model_path in model_paths[0]:\n",
    "    convert_to_coreml(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Iteration | Sparsity (%) | Accuracy | Latency (s) | Disk Size (MB) |\n",
    "| --------- | ------------ | -------- | ----------- | -------------- |\n",
    "|     0     |   0.0%       |     |             |      847.00    |\n",
    "|     1     |   32.96%     |     |             |      567.61    |\n",
    "|     2     |   55.05%     |     |             |      380.44    |\n",
    "|     3     |   69.84%     |     |             |      255.08    |\n",
    "|     4     |   79.76%     |     |             |      171.10    |\n",
    "|     5     |   86.40%     |     |             |      114.85    |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [2 points] 2. Choose two of the following three options to implement on your model, device, and data (1 point per option):\n",
    "\n",
    "1. Implement a structured pruning technique. You may prune dimensions of matrices, attention heads, entire layers, etc. Describe your strategy and report the results in a table, adjusting the \"sparsity rate\" column and as needed.\n",
    "\n",
    "    Fill in the following table with your results (choose any 2-3 pruned models to compare to the unpruned model):\n",
    "\n",
    "    Channel (dimension) pruning\n",
    "\n",
    "    | Structure Pruned | Sparsity Rate | Accuracy | Latency (s) | Disk Size (MB) |\n",
    "    | ---------------- | ------------- | -------- | ----------- | -------------- |\n",
    "    | Conv Layers      |    30%        |          |             |    45.8        |\n",
    "    | Conv Layers      |    60%        |          |             |    15.1        |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[2 points] 2. Choose two of the following three options to implement on your model, device, and data (1 point per option):\n",
    "#1. Implement a structured pruning technique. You may prune dimensions of matrices, attention heads, entire layers, etc. Describe your strategy and report the results in a table, adjusting the \"sparsity rate\" column and as needed.\n",
    "# Prune by magnitutude of combined parameters in a Conv2d filer a) 64x64 b) 128x128 c)256x256 and its following batch norm\n",
    "#2. Unstructured magnitutude pruning of a)64 input layers b)128 input layers c)256 input layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_structured_pruning(model_path, layer_type, sparsity, pruned_layer):\n",
    "    \"\"\"\n",
    "    layer_types = [nn.Conv2d, nn.BatchNorm2d]\n",
    "    model_path_list = []\n",
    "\n",
    "    for layer_type in layer_types:\n",
    "\n",
    "        # Load Model and Select Weights\n",
    "        model, device = load_asl_model(model_path)\n",
    "        prune_params = [(m[1], \"weight\") for m in model.named_modules() if len(list(m[1].children()))==0 and isinstance(m[1], layer_type)]\n",
    "        if issubclass(layer_type, nn.Conv2d):\n",
    "            prune_params = [x for x in prune_params if x[0].in_channels == x[0].out_channels]\n",
    "        print(f\"Pruning iteration {i}\")\n",
    "        print(model)\n",
    "\n",
    "        # Prune Model\n",
    "        layer_weight_magnitude = []\n",
    "        for ind, layer_param in enumerate(prune_params):\n",
    "            weights = layer_param[0].weight\n",
    "            combined_absolute_magnitude = torch.sum(torch.abs(weights)).item() / weights.numel()\n",
    "            layer_weight_magnitude.append(combined_absolute_magnitude)\n",
    "\n",
    "        desired_prune = int(len(layer_weight_magnitude) * sparsity)\n",
    "        smallest_params = heapq.nsmallest(desired_prune, range(len(layer_weight_magnitude)), key=lambda i: layer_weight_magnitude[i])\n",
    "\n",
    "        for ind in smallest_params:\n",
    "            layer_name, layer_module = prune_params[ind]\n",
    "            print(f\"Pruning layer: {layer_name}\")\n",
    "            print(layer_module)\n",
    "            # Split name to navigate the model hierarchy\n",
    "            if issubclass(layer_type, nn.Conv2d):\n",
    "                layer_name = \"conv2\"\n",
    "            else:\n",
    "                layer_name = \"bn2\"\n",
    "            setattr(layer_module, layer_name, nn.Identity())\n",
    "\n",
    "        # Save Model\n",
    "        saved_model_path = f\"models/structured_pruned_model_{i}.pth\"\n",
    "        #save_copy_of_model(model, saved_model_path)\n",
    "        model_path_list.append(saved_model_path)\n",
    "    \"\"\"\n",
    "\n",
    "    model, device = load_asl_model(model_path)\n",
    "\n",
    "    example_inputs = torch.randn(1, 3, 224, 224)\n",
    "\n",
    "    # 1. Importance criterion\n",
    "    imp = tp.importance.GroupNormImportance(p=2) # or GroupTaylorImportance(), GroupHessianImportance(), etc.\n",
    "\n",
    "    # 2. Initialize a pruner with the model and the importance criterion\n",
    "    ignored_layers = []\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, (torch.nn.Linear)):\n",
    "            ignored_layers.append(m) # DO NOT prune the final classifier!\n",
    "\n",
    "    print(ignored_layers)\n",
    "\n",
    "    pruner = tp.pruner.MetaPruner( # We can always choose MetaPruner if sparse training is not required.\n",
    "        model,\n",
    "        example_inputs,\n",
    "        importance=imp,\n",
    "        pruning_ratio=sparsity, # remove 50% channels, ResNet18 = {64, 128, 256, 512} => ResNet18_Half = {32, 64, 128, 256}\n",
    "        # pruning_ratio_dict = {model.conv1: 0.2, model.layer2: 0.8}, # customized pruning ratios for layers or blocks\n",
    "        ignored_layers=ignored_layers,\n",
    "        round_to=4, # It's recommended to round dims/channels to 4x or 8x for acceleration. Please see: https://docs.nvidia.com/deeplearning/performance/dl-performance-convolutional/index.html\n",
    "    )\n",
    "\n",
    "    # 3. Prune Model\n",
    "    base_macs, base_nparams = tp.utils.count_ops_and_params(model, example_inputs)\n",
    "    pruner.step()\n",
    "    macs, nparams = tp.utils.count_ops_and_params(model, example_inputs)\n",
    "    print(f\"MACs: {base_macs/1e9} G -> {macs/1e9} G, #Params: {base_nparams/1e6} M -> {nparams/1e6} M\")\n",
    "\n",
    "    save_path = f\"models/structued_pruning_{pruned_layer}.pth\"\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    print(f\"Saved model to {save_path}\")\n",
    "    \n",
    "    return save_path\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mv/q059yr4j7l18ms7xb987dwtm0000gn/T/ipykernel_40481/2142981156.py:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=torch.device(\"cpu\")))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Linear(in_features=2048, out_features=29, bias=True)]\n",
      "MACs: 4.119935517 G -> 1.999082993 G, #Params: 23.567453 M -> 11.389361 M\n",
      "Saved model to models/structued_pruning_0.3.pth\n",
      "[Linear(in_features=2048, out_features=29, bias=True)]\n",
      "MACs: 4.119935517 G -> 0.657128785 G, #Params: 23.567453 M -> 3.720013 M\n",
      "Saved model to models/structued_pruning_0.6.pth\n"
     ]
    }
   ],
   "source": [
    "strucuted_model_paths = []\n",
    "strucuted_model_paths.append(model_structured_pruning(model_path=\"./models/model_weights_ResNet50_224_resize.pth\", sparsity=0.3, layer_type=nn.Conv2d, pruned_layer=\"0.3\"))\n",
    "strucuted_model_paths.append(model_structured_pruning(model_path=\"./models/model_weights_ResNet50_224_resize.pth\", sparsity=0.6, layer_type=nn.BatchNorm2d, pruned_layer=\"0.6\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/briancurtin/Documents/GitHub/ODML_project/.venv/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/briancurtin/Documents/GitHub/ODML_project/.venv/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "Model is not in eval mode. Consider calling '.eval()' on your model prior to conversion\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model converted to TorchScript format\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting PyTorch Frontend ==> MIL Ops: 100%|█████████▉| 498/499 [00:00<00:00, 572.91 ops/s]\n",
      "Running MIL frontend_pytorch pipeline: 100%|██████████| 5/5 [00:00<00:00, 114.43 passes/s]\n",
      "Running MIL default pipeline: 100%|██████████| 86/86 [00:00<00:00, 118.10 passes/s]\n",
      "Running MIL backend_neuralnetwork pipeline: 100%|██████████| 9/9 [00:00<00:00, 185.45 passes/s]\n",
      "Translating MIL ==> NeuralNetwork Ops: 100%|██████████| 1038/1038 [00:03<00:00, 300.77 ops/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model converted to CoreML format\n",
      "CoreML model saved as models/structued_pruning_0.3.mlmodel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model is not in eval mode. Consider calling '.eval()' on your model prior to conversion\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model converted to TorchScript format\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting PyTorch Frontend ==> MIL Ops: 100%|█████████▉| 498/499 [00:00<00:00, 4728.13 ops/s]\n",
      "Running MIL frontend_pytorch pipeline: 100%|██████████| 5/5 [00:00<00:00, 126.10 passes/s]\n",
      "Running MIL default pipeline: 100%|██████████| 86/86 [00:00<00:00, 129.78 passes/s]\n",
      "Running MIL backend_neuralnetwork pipeline: 100%|██████████| 9/9 [00:00<00:00, 197.62 passes/s]\n",
      "Translating MIL ==> NeuralNetwork Ops: 100%|██████████| 1038/1038 [00:03<00:00, 300.80 ops/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model converted to CoreML format\n",
      "CoreML model saved as models/structued_pruning_0.6.mlmodel\n"
     ]
    }
   ],
   "source": [
    "for model_path in strucuted_model_paths:\n",
    "    convert_to_coreml(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Conduct a sensitivity analysis of pruning (structured or unstructured) different components of your model. For instance, what happens to your model's performance when you prune input embeddings vs hidden layer weights? Do earlier layers seem more or less important than later layers? You are not required to conduct a thorough study, but you should be able to draw a couple concrete conclusions.\n",
    "\n",
    "    Fill in the following table with your results (choose any 2-3 pruned models to compare to the unpruned model):\n",
    "\n",
    "    |        Pruning Technique        |  Sparsity Rate  | Accuracy | Latency (s) | Disk Size (MB) |\n",
    "    | ------------------------------- | --------------- | -------- | ----------- | -------------- |\n",
    "    | Unstructured, LayerNorm         |  20% global     |          |             |                |\n",
    "    | Unstructured, LayerNorm         |  40% global     |          |             |                |\n",
    "    | Unstructured, Conv2d            |  40% global     |          |             |                |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_sensitivity_pruning(model_path, sparsity, iterations):\n",
    "\n",
    "    # Create empty DataFrame to store results\n",
    "    results_df = pd.DataFrame(columns=[\n",
    "        'iteration',\n",
    "        'sparsity_model',\n",
    "        'size_mb'\n",
    "    ])\n",
    "\n",
    "    # Define the labels for the classes (A-Z, del, nothing, space)\n",
    "\n",
    "    model, device = load_asl_model(model_path)\n",
    "    print(f\"Model weights loaded from {model_path}\")\n",
    "\n",
    "    prune_params = [(m[1], \"weight\") for m in model.named_modules() if len(list(m[1].children()))==0 and not isinstance(m[1], (nn.ReLU, nn.MaxPool2d, nn.AdaptiveAvgPool2d))]\n",
    "\n",
    "    # Store initial results (iteration 0)\n",
    "    sparsity_results = calculate_sparsity(model)\n",
    "    size = evaluate_model(model)\n",
    "    \n",
    "    results_df.loc[0] = {\n",
    "        'iteration': 0,\n",
    "        'sparsity_model': sparsity_results['sparsity_model'],\n",
    "        'size_mb': size/1e6\n",
    "    }\n",
    "\n",
    "    model_path_list = [model_path]\n",
    "\n",
    "    model, device = load_asl_model(model_path)\n",
    "    prune_params = [(m[1], \"weight\") for m in model.named_modules() if len(list(m[1].children()))==0 and not isinstance(m[1], (nn.ReLU, nn.MaxPool2d, nn.AdaptiveAvgPool2d))]\n",
    "\n",
    "    print(f\"Iteration 0 - Model Sparsity: {sparsity_results['sparsity_model']:.2f}%\")\n",
    "\n",
    "    for i in range(1, 6):\n",
    "        print(f\"Pruning iteration {i}\")\n",
    "\n",
    "        prune.global_unstructured(prune_params, pruning_method=prune.L1Unstructured, amount=sparsity)\n",
    "\n",
    "        saved_model_path = f\"models/pruned_model_{i}.pth\"\n",
    "        save_copy_of_model(model, saved_model_path)\n",
    "        model_path_list.append(saved_model_path)\n",
    "\n",
    "        size = sparse_evaluate(model, device)\n",
    "        sparsity_results = calculate_sparsity(model)\n",
    "\n",
    "\n",
    "        # Store results in DataFrame\n",
    "        results_df.loc[i] = {\n",
    "            'iteration': i,\n",
    "            'sparsity_model': sparsity_results['sparsity_model'],\n",
    "            'size_mb': size/1e6\n",
    "        }\n",
    "\n",
    "        print(f\"Sparsity for the model overall at Iteration {i}: {sparsity_results['sparsity_model']:.2f}%, Size MB: {size/1e6}\")\n",
    "    \n",
    "    \n",
    "    # Display the DataFrame\n",
    "    print(\"\\nFinal Results DataFrame:\")\n",
    "    print(results_df)\n",
    "    \n",
    "    return results_df, model_path_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
