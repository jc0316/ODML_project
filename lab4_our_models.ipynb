{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "!pip install torch torchvision\n",
    "!pip install tqdm\n",
    "!pip install pandas\n",
    "!pip install -U coremltools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Torch version 2.5.1 has not been tested with coremltools. You may run into unexpected errors. Torch 2.4.0 is the most recent version that has been tested.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.utils.prune as prune\n",
    "import torch.nn.functional as F\n",
    "from copy import deepcopy\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import coremltools as ct\n",
    "from coremltools.optimize.torch.pruning import ModuleMagnitudePrunerConfig, MagnitudePruner, MagnitudePrunerConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [3 points] Exercise 3: Your Model, Device, and Data\n",
    "\n",
    "\n",
    "In this section, you will repeat the simple experiments from Exercise 2 on your own model, device, and data. Additionally, you will choose two of three options for practical benefits to your pruned model's accuracy and latency. You may use a different sparsity level, higher or lower than 33%, if it makes sense for your settings. Make sure to report any changes you made and why you made them. Additionally, report any challenges encountered measuring latency or storage on your device.\n",
    "\n",
    "### [1 point] 1. Repeat Exercise 2.4 (repeated unstructured pruning) for your model, on your device and with your data.\n",
    "\n",
    "Keep performing the same unstructured magnitude pruning of your choice of sparsity level of the remaining weights on the same model (*without re-training or resetting the model*). You will apply the same function as above with the same 0.33 proportion parameter.\n",
    "\n",
    "Collect values for this table, keeping in mind that you will need to plot the results later. You might want to keep the values in Pandas DataFrames. Sparsity reported should be the percentage of *prunable* parameters pruned. \n",
    "\n",
    "| Iteration | Sparsity (%) | Accuracy | Latency (s) | Disk Size (MB) |\n",
    "| --------- | ------------ | -------- | ----------- | -------------- |\n",
    "|     0     |   0.0%       |          |             |                |\n",
    "|     1     |      ?       |          |             |                |\n",
    "|     2     |              |          |             |                |\n",
    "|     3     |              |          |             |                |\n",
    "|     4     |              |          |             |                |\n",
    "|     5     |              |          |             |                |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sparsity(model, print_results=False):\n",
    "    \"\"\"\n",
    "    Calculate the sparsity level (using the percent of elements that are 0) for:\n",
    "    - each parameter,\n",
    "    - all pruned parameters overall, and\n",
    "    - the model overall.\n",
    "    \n",
    "    Report each of these values: \n",
    "    - the sparsity level of each parameter, \n",
    "    - across all pruned parameters, and \n",
    "    - for the model overall. \n",
    "    \"\"\"\n",
    "    sparsity_per_parameter = {}\n",
    "    total_zero_count_pruned = 0\n",
    "    total_element_count_pruned = 0\n",
    "    total_zero_count_model = 0\n",
    "    total_element_count_model = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "    # Iterate over all buffers in the model\n",
    "    for name, buffer in model.named_buffers():\n",
    "        # Calculate the number of zero elements and total elements in the buffer\n",
    "        zero_count = (buffer == 0).sum().item()\n",
    "        total_elements = buffer.numel()\n",
    "        \n",
    "        # Calculate the sparsity level for this parameter\n",
    "        sparsity_per_parameter[name] = zero_count / total_elements * 100\n",
    "\n",
    "        # Check if this is a pruned parameter by looking for \"weight_mask\" or \"bias_mask\" in the name\n",
    "        if \"weight_mask\" in name or \"bias_mask\" in name:\n",
    "            total_zero_count_pruned += zero_count\n",
    "            total_element_count_pruned += total_elements\n",
    "\n",
    "        # Accumulate for overall model sparsity\n",
    "        total_zero_count_model += zero_count\n",
    "\n",
    "    # Calculate overall sparsity for pruned parameters and the entire model\n",
    "    sparsity_pruned_parameters = (total_zero_count_pruned / total_element_count_pruned * 100\n",
    "                                  if total_element_count_pruned > 0 else 0)\n",
    "    sparsity_model = total_zero_count_model / total_element_count_model * 100\n",
    "\n",
    "    # Print or return the results\n",
    "    if print_results:\n",
    "        print(\"Sparsity per parameter:\")\n",
    "        for name, sparsity in sparsity_per_parameter.items():\n",
    "            print(f\"  {name}: {sparsity:.2f}%\")\n",
    "        \n",
    "        print(f\"Sparsity across all pruned parameters: {sparsity_pruned_parameters:.2f}%\")\n",
    "        print(f\"Sparsity for the model overall: {sparsity_model:.2f}%\")\n",
    "\n",
    "    # Optionally, return the values for further use\n",
    "    return {\n",
    "        \"sparsity_per_parameter\": sparsity_per_parameter,\n",
    "        \"sparsity_pruned_parameters\": sparsity_pruned_parameters,\n",
    "        \"sparsity_model\": sparsity_model\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_size_of_model(model, label=\"\"):\n",
    "    sd = model.state_dict()\n",
    "    for item in sd:\n",
    "        sd[item] = model.state_dict()[item].to_sparse()\n",
    "    \n",
    "    torch.save(sd, \"temp.pt\")\n",
    "    size=os.path.getsize(\"temp.pt\")\n",
    "    #print(\"model: \",label,' \\t','Size (MB):', size/1e6)\n",
    "\n",
    "    os.remove('temp.pt')\n",
    "    return size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model):\n",
    "    model.eval()\n",
    "\n",
    "    size = print_size_of_model(model, \"sparse\")\n",
    "    \n",
    "    return size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_evaluate(model, device):\n",
    "    model.to(device)\n",
    "    model_params = [(m[1], \"weight\") for m in model.named_modules() if len(list(m[1].children()))==0 and not isinstance(m[1], (nn.ReLU, nn.MaxPool2d, nn.AdaptiveAvgPool2d))]\n",
    "\n",
    "    model_copy = safe_deepcopy(model, model_params)\n",
    "    model_copy_params = [(m[1], \"weight\") for m in model_copy.named_modules() if len(list(m[1].children()))==0 and not isinstance(m[1], (nn.ReLU, nn.MaxPool2d, nn.AdaptiveAvgPool2d))]\n",
    "\n",
    "    for p in model_copy_params:\n",
    "        prune.remove(*p)\n",
    "    \n",
    "    return evaluate_model(model_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_asl_model(model_path):\n",
    "    labels = [\n",
    "        'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', \n",
    "        'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', \n",
    "        'del', 'nothing', 'space'\n",
    "    ]\n",
    "    # Create model\n",
    "    num_classes = len(labels)  # 29 classes\n",
    "    model = models.resnet50(weights=False)\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "\n",
    "    # Load the saved weights\n",
    "    model.load_state_dict(torch.load(model_path, map_location=torch.device(\"cpu\")))\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    return model, device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_deepcopy(model, params_to_prune):\n",
    "\n",
    "    # detach the parameters in params_to_prune\n",
    "    for module, name in params_to_prune:\n",
    "        param = getattr(module, name)\n",
    "        setattr(module, name, param.detach())\n",
    "\n",
    "    return deepcopy(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_copy_of_model(model, model_path):\n",
    "    \n",
    "    model_params = [(m[1], \"weight\") for m in model.named_modules() if len(list(m[1].children()))==0 and not isinstance(m[1], (nn.ReLU, nn.MaxPool2d, nn.AdaptiveAvgPool2d))]\n",
    "\n",
    "    model_copy = safe_deepcopy(model, model_params)\n",
    "    model_copy_params = [(m[1], \"weight\") for m in model_copy.named_modules() if len(list(m[1].children()))==0 and not isinstance(m[1], (nn.ReLU, nn.MaxPool2d, nn.AdaptiveAvgPool2d))]\n",
    "\n",
    "    for p in model_copy_params:\n",
    "        prune.remove(*p)\n",
    "\n",
    "    torch.save(model_copy.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_repeated_pruning(model_path, sparsity, iterations):\n",
    "\n",
    "    # Create empty DataFrame to store results\n",
    "    results_df = pd.DataFrame(columns=[\n",
    "        'iteration',\n",
    "        'sparsity_model',\n",
    "        'size_mb'\n",
    "    ])\n",
    "\n",
    "    # Define the labels for the classes (A-Z, del, nothing, space)\n",
    "\n",
    "    model, device = load_asl_model(model_path)\n",
    "    print(f\"Model weights loaded from {model_path}\")\n",
    "\n",
    "    prune_params = [(m[1], \"weight\") for m in model.named_modules() if len(list(m[1].children()))==0 and not isinstance(m[1], (nn.ReLU, nn.MaxPool2d, nn.AdaptiveAvgPool2d))]\n",
    "\n",
    "    # Store initial results (iteration 0)\n",
    "    sparsity_results = calculate_sparsity(model)\n",
    "    size = evaluate_model(model)\n",
    "    \n",
    "    results_df.loc[0] = {\n",
    "        'iteration': 0,\n",
    "        'sparsity_model': sparsity_results['sparsity_model'],\n",
    "        'size_mb': size/1e6\n",
    "    }\n",
    "\n",
    "    model_path_list = [model_path]\n",
    "\n",
    "    model, device = load_asl_model(model_path)\n",
    "    prune_params = [(m[1], \"weight\") for m in model.named_modules() if len(list(m[1].children()))==0 and not isinstance(m[1], (nn.ReLU, nn.MaxPool2d, nn.AdaptiveAvgPool2d))]\n",
    "\n",
    "    print(f\"Iteration 0 - Model Sparsity: {sparsity_results['sparsity_model']:.2f}%\")\n",
    "\n",
    "    for i in range(1, 6):\n",
    "        print(f\"Pruning iteration {i}\")\n",
    "\n",
    "        prune.global_unstructured(prune_params, pruning_method=prune.L1Unstructured, amount=sparsity)\n",
    "\n",
    "        saved_model_path = f\"models/pruned_model_{i}.pth\"\n",
    "        save_copy_of_model(model, saved_model_path)\n",
    "        model_path_list.append(saved_model_path)\n",
    "\n",
    "        size = sparse_evaluate(model, device)\n",
    "        sparsity_results = calculate_sparsity(model)\n",
    "\n",
    "\n",
    "        # Store results in DataFrame\n",
    "        results_df.loc[i] = {\n",
    "            'iteration': i,\n",
    "            'sparsity_model': sparsity_results['sparsity_model'],\n",
    "            'size_mb': size/1e6\n",
    "        }\n",
    "\n",
    "        print(f\"Sparsity for the model overall at Iteration {i}: {sparsity_results['sparsity_model']:.2f}%, Size MB: {size/1e6}\")\n",
    "    \n",
    "    \n",
    "    # Display the DataFrame\n",
    "    print(\"\\nFinal Results DataFrame:\")\n",
    "    print(results_df)\n",
    "    \n",
    "    return results_df, model_path_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/briancurtin/Documents/GitHub/ODML_project/.venv/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "/var/folders/mv/q059yr4j7l18ms7xb987dwtm0000gn/T/ipykernel_40481/2142981156.py:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=torch.device(\"cpu\")))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weights loaded from ./models/model_weights_ResNet50_224_resize.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/briancurtin/Documents/GitHub/ODML_project/.venv/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "/var/folders/mv/q059yr4j7l18ms7xb987dwtm0000gn/T/ipykernel_40481/2142981156.py:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=torch.device(\"cpu\")))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 - Model Sparsity: 0.00%\n",
      "Pruning iteration 1\n",
      "Sparsity for the model overall at Iteration 1: 32.96%, Size MB: 567.609062\n",
      "Pruning iteration 2\n",
      "Sparsity for the model overall at Iteration 2: 55.05%, Size MB: 380.443878\n",
      "Pruning iteration 3\n",
      "Sparsity for the model overall at Iteration 3: 69.84%, Size MB: 255.07703\n",
      "Pruning iteration 4\n",
      "Sparsity for the model overall at Iteration 4: 79.76%, Size MB: 171.102054\n",
      "Pruning iteration 5\n",
      "Sparsity for the model overall at Iteration 5: 86.40%, Size MB: 114.853414\n",
      "Pruning iteration 6\n",
      "Sparsity for the model overall at Iteration 6: 90.85%, Size MB: 77.173222\n",
      "Pruning iteration 7\n",
      "Sparsity for the model overall at Iteration 7: 93.83%, Size MB: 51.93751\n",
      "\n",
      "Final Results DataFrame:\n",
      "   iteration  sparsity_model     size_mb\n",
      "0          0        0.000000  847.002150\n",
      "1          1       32.962769  567.609062\n",
      "2          2       55.047824  380.443878\n",
      "3          3       69.844811  255.077030\n",
      "4          4       79.758793  171.102054\n",
      "5          5       86.401161  114.853414\n",
      "6          6       90.851549   77.173222\n",
      "7          7       93.833305   51.937510\n"
     ]
    }
   ],
   "source": [
    "# [1 point] 1. Repeat Exercise 2.4 (repeated unstructured pruning) for your model, on your device and with your data.\n",
    "# https://apple.github.io/coremltools/source/coremltools.optimize.torch.pruning.html\n",
    "pruning_results, model_paths = model_repeated_pruning(\"./models/model_weights_ResNet50_224_resize.pth\", sparsity=0.33, iterations=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: ./models/pruned_model_1.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/briancurtin/Documents/GitHub/ODML_project/.venv/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "/var/folders/mv/q059yr4j7l18ms7xb987dwtm0000gn/T/ipykernel_40481/2142981156.py:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=torch.device(\"cpu\")))\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './models/pruned_model_1.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[94], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./models/pruned_model_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m model, devie \u001b[38;5;241m=\u001b[39m \u001b[43mload_asl_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(model)\n\u001b[1;32m      9\u001b[0m prune_params \u001b[38;5;241m=\u001b[39m [(m[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mnamed_modules() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mlist\u001b[39m(m[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mchildren()))\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(m[\u001b[38;5;241m1\u001b[39m], (nn\u001b[38;5;241m.\u001b[39mReLU, nn\u001b[38;5;241m.\u001b[39mMaxPool2d, nn\u001b[38;5;241m.\u001b[39mAdaptiveAvgPool2d))]\n",
      "Cell \u001b[0;32mIn[74], line 14\u001b[0m, in \u001b[0;36mload_asl_model\u001b[0;34m(model_path)\u001b[0m\n\u001b[1;32m     11\u001b[0m model\u001b[38;5;241m.\u001b[39mfc \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(num_ftrs, num_classes)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Load the saved weights\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     16\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/Documents/GitHub/ODML_project/.venv/lib/python3.11/site-packages/torch/serialization.py:1319\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m   1317\u001b[0m     pickle_load_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1319\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m   1321\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m   1322\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m   1323\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m   1324\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/Documents/GitHub/ODML_project/.venv/lib/python3.11/site-packages/torch/serialization.py:659\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    658\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 659\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    660\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    661\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/Documents/GitHub/ODML_project/.venv/lib/python3.11/site-packages/torch/serialization.py:640\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 640\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mopen\u001b[39m(name, mode))\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './models/pruned_model_1.pth'"
     ]
    }
   ],
   "source": [
    "path = \"./models/\"\n",
    "\n",
    "\n",
    "for i in range(1,6):\n",
    "    model_path = f\"./models/pruned_model_{i}.pth\"\n",
    "    print(f\"Model: {model_path}\")\n",
    "    model, devie = load_asl_model(model_path)\n",
    "    print(model)\n",
    "    prune_params = [(m[1], \"weight\") for m in model.named_modules() if len(list(m[1].children()))==0 and not isinstance(m[1], (nn.ReLU, nn.MaxPool2d, nn.AdaptiveAvgPool2d))]\n",
    "    print(prune_params[0][0].weight[2][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_coreml(model_path):\n",
    "\n",
    "    # Define the labels for the classes (A-Z, del, nothing, space)\n",
    "    labels = [\n",
    "        'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', \n",
    "        'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', \n",
    "        'del', 'nothing', 'space'\n",
    "    ]\n",
    "        \n",
    "    # Load the trained PyTorch model\n",
    "    # Ensure the model architecture matches the one used during training\n",
    "    num_classes = len(labels)  # 29 classes\n",
    "    model = models.resnet50(pretrained=False)\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "\n",
    "    #prune_params = [(m[1], \"weight\") for m in model.named_modules() if len(list(m[1].children()))==0 and not isinstance(m[1], (nn.ReLU, nn.MaxPool2d, nn.AdaptiveAvgPool2d))]\n",
    "    #for p in prune_params:\n",
    "    #    prune.identity(p[0], \"weight\")\n",
    "    # Copy the parameters\n",
    "    #model.load_state_dict(torch.load(model_path, map_location=torch.device(\"cpu\")))\n",
    "    \n",
    "    #copy_params = [(m[1], \"weight\") for m in model.named_modules() if len(list(m[1].children()))==0 and not isinstance(m[1], (nn.ReLU, nn.MaxPool2d, nn.AdaptiveAvgPool2d))]\n",
    "    # (we assume the same model architecture)\n",
    "    #for p in copy_params:\n",
    "    #    prune.remove(*p)\n",
    "\n",
    "    # Load the saved weights\n",
    "    #print(f\"Model weights loaded from {model_path}\")\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    #model.eval()\n",
    "\n",
    "    # 1. Create an example input tensor resized to 224x224\n",
    "    example_input = torch.rand(1, 3, 224, 224)  # Batch size 1, 3 color channels, 224x224 image\n",
    "\n",
    "    # 2. Convert the model to TorchScript\n",
    "    traced_model = torch.jit.trace(model, example_input)\n",
    "    print(\"Model converted to TorchScript format\")\n",
    "\n",
    "    # 3. Convert the TorchScript model to CoreML format\n",
    "    mlmodel = ct.convert(\n",
    "        traced_model,\n",
    "        inputs=[ct.ImageType(name=\"image\", shape=example_input.shape, channel_first=True)],\n",
    "        classifier_config=ct.ClassifierConfig(class_labels=labels, predicted_feature_name=\"classLabel\"),\n",
    "        convert_to=\"neuralnetwork\",  # Ensures compatibility with .mlmodel format\n",
    "    )\n",
    "    print(\"Model converted to CoreML format\")\n",
    "\n",
    "    # 4. Apply Post-Training Quantization to INT8\n",
    "    # quantized_mlmodel = ct.models.neural_network.quantization_utils.quantize_weights(\n",
    "    #     mlmodel, nbits=8  # Use 8-bit integer quantization\n",
    "    # )\n",
    "    # print(\"Model quantized to INT8\")\n",
    "\n",
    "    # 5. Save the CoreML model\n",
    "    mlmodel_path = model_path.replace(\".pth\", \".mlmodel\")  # Desired output filename\n",
    "    mlmodel.save(mlmodel_path)\n",
    "    print(f\"CoreML model saved as {mlmodel_path}\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/briancurtin/Documents/GitHub/ODML_project/.venv/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/briancurtin/Documents/GitHub/ODML_project/.venv/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "Model is not in eval mode. Consider calling '.eval()' on your model prior to conversion\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model converted to TorchScript format\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting PyTorch Frontend ==> MIL Ops: 100%|█████████▉| 498/499 [00:00<00:00, 1306.32 ops/s]\n",
      "Running MIL frontend_pytorch pipeline: 100%|██████████| 5/5 [00:00<00:00, 124.66 passes/s]\n",
      "Running MIL default pipeline: 100%|██████████| 86/86 [00:00<00:00, 130.69 passes/s]\n",
      "Running MIL backend_neuralnetwork pipeline: 100%|██████████| 9/9 [00:00<00:00, 194.84 passes/s]\n",
      "Translating MIL ==> NeuralNetwork Ops: 100%|██████████| 1038/1038 [00:03<00:00, 318.48 ops/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model converted to CoreML format\n",
      "CoreML model saved as models/pruned_model_1.mlmodel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model is not in eval mode. Consider calling '.eval()' on your model prior to conversion\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model converted to TorchScript format\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting PyTorch Frontend ==> MIL Ops: 100%|█████████▉| 498/499 [00:00<00:00, 4698.24 ops/s]\n",
      "Running MIL frontend_pytorch pipeline: 100%|██████████| 5/5 [00:00<00:00, 116.19 passes/s]\n",
      "Running MIL default pipeline: 100%|██████████| 86/86 [00:00<00:00, 106.36 passes/s]\n",
      "Running MIL backend_neuralnetwork pipeline: 100%|██████████| 9/9 [00:00<00:00, 185.57 passes/s]\n",
      "Translating MIL ==> NeuralNetwork Ops: 100%|██████████| 1038/1038 [00:03<00:00, 280.82 ops/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model converted to CoreML format\n",
      "CoreML model saved as models/pruned_model_2.mlmodel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model is not in eval mode. Consider calling '.eval()' on your model prior to conversion\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model converted to TorchScript format\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting PyTorch Frontend ==> MIL Ops: 100%|█████████▉| 498/499 [00:00<00:00, 4784.14 ops/s]\n",
      "Running MIL frontend_pytorch pipeline: 100%|██████████| 5/5 [00:00<00:00, 114.37 passes/s]\n",
      "Running MIL default pipeline: 100%|██████████| 86/86 [00:00<00:00, 110.94 passes/s]\n",
      "Running MIL backend_neuralnetwork pipeline: 100%|██████████| 9/9 [00:00<00:00, 180.44 passes/s]\n",
      "Translating MIL ==> NeuralNetwork Ops: 100%|██████████| 1038/1038 [00:03<00:00, 270.26 ops/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model converted to CoreML format\n",
      "CoreML model saved as models/pruned_model_3.mlmodel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model is not in eval mode. Consider calling '.eval()' on your model prior to conversion\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model converted to TorchScript format\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting PyTorch Frontend ==> MIL Ops: 100%|█████████▉| 498/499 [00:00<00:00, 4806.30 ops/s]\n",
      "Running MIL frontend_pytorch pipeline: 100%|██████████| 5/5 [00:00<00:00, 118.76 passes/s]\n",
      "Running MIL default pipeline: 100%|██████████| 86/86 [00:00<00:00, 106.28 passes/s]\n",
      "Running MIL backend_neuralnetwork pipeline: 100%|██████████| 9/9 [00:00<00:00, 174.58 passes/s]\n",
      "Translating MIL ==> NeuralNetwork Ops: 100%|██████████| 1038/1038 [00:03<00:00, 282.58 ops/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model converted to CoreML format\n",
      "CoreML model saved as models/pruned_model_4.mlmodel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model is not in eval mode. Consider calling '.eval()' on your model prior to conversion\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model converted to TorchScript format\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting PyTorch Frontend ==> MIL Ops: 100%|█████████▉| 498/499 [00:00<00:00, 5041.72 ops/s]\n",
      "Running MIL frontend_pytorch pipeline: 100%|██████████| 5/5 [00:00<00:00, 121.94 passes/s]\n",
      "Running MIL default pipeline: 100%|██████████| 86/86 [00:00<00:00, 121.63 passes/s]\n",
      "Running MIL backend_neuralnetwork pipeline: 100%|██████████| 9/9 [00:00<00:00, 187.06 passes/s]\n",
      "Translating MIL ==> NeuralNetwork Ops: 100%|██████████| 1038/1038 [00:03<00:00, 295.58 ops/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model converted to CoreML format\n",
      "CoreML model saved as models/pruned_model_5.mlmodel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model is not in eval mode. Consider calling '.eval()' on your model prior to conversion\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model converted to TorchScript format\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting PyTorch Frontend ==> MIL Ops: 100%|█████████▉| 498/499 [00:00<00:00, 4846.06 ops/s]\n",
      "Running MIL frontend_pytorch pipeline: 100%|██████████| 5/5 [00:00<00:00, 118.95 passes/s]\n",
      "Running MIL default pipeline: 100%|██████████| 86/86 [00:00<00:00, 119.94 passes/s]\n",
      "Running MIL backend_neuralnetwork pipeline: 100%|██████████| 9/9 [00:00<00:00, 180.43 passes/s]\n",
      "Translating MIL ==> NeuralNetwork Ops: 100%|██████████| 1038/1038 [00:03<00:00, 276.24 ops/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model converted to CoreML format\n",
      "CoreML model saved as models/pruned_model_6.mlmodel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model is not in eval mode. Consider calling '.eval()' on your model prior to conversion\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model converted to TorchScript format\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting PyTorch Frontend ==> MIL Ops: 100%|█████████▉| 498/499 [00:00<00:00, 5161.70 ops/s]\n",
      "Running MIL frontend_pytorch pipeline: 100%|██████████| 5/5 [00:00<00:00, 122.64 passes/s]\n",
      "Running MIL default pipeline: 100%|██████████| 86/86 [00:00<00:00, 121.00 passes/s]\n",
      "Running MIL backend_neuralnetwork pipeline: 100%|██████████| 9/9 [00:00<00:00, 190.46 passes/s]\n",
      "Translating MIL ==> NeuralNetwork Ops: 100%|██████████| 1038/1038 [00:03<00:00, 311.27 ops/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model converted to CoreML format\n",
      "CoreML model saved as models/pruned_model_7.mlmodel\n"
     ]
    }
   ],
   "source": [
    "# Convert each pytoch model to CoreML for testing\n",
    "for model_path in model_paths[1:]:\n",
    "    convert_to_coreml(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Iteration | Sparsity (%) | Accuracy | Latency (s) | Disk Size (MB) |\n",
    "| --------- | ------------ | -------- | ----------- | -------------- |\n",
    "|     0     |   0.0%       |  0.9990476190476191   |             |      847.00    |\n",
    "|     1     |   32.96%     |  0.9990476190476191   |             |      567.61    |\n",
    "|     2     |   55.05%     |  0.9990476190476191   |             |      380.44    |\n",
    "|     3     |   69.84%     |  0.9990476190476191   |             |      255.08    |\n",
    "|     4     |   79.76%     |  0.9990476190476191   |             |      171.10    |\n",
    "|     5     |   86.40%     |  0.9990476190476191   |             |      114.85    |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [2 points] 2. Choose two of the following three options to implement on your model, device, and data (1 point per option):\n",
    "\n",
    "1. Implement a structured pruning technique. You may prune dimensions of matrices, attention heads, entire layers, etc. Describe your strategy and report the results in a table, adjusting the \"sparsity rate\" column and as needed.\n",
    "\n",
    "    Fill in the following table with your results (choose any 2-3 pruned models to compare to the unpruned model):\n",
    "\n",
    "    | Structure Pruned | Sparsity Rate | Accuracy | Latency (s) | Disk Size (MB) |\n",
    "    | ---------------- | ------------- | -------- | ----------- | -------------- |\n",
    "    | Attention heads? |               |          |             |                |\n",
    "    | Layers?          |               |          |             |                |\n",
    "    | Other?           |               |          |             |                |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[2 points] 2. Choose two of the following three options to implement on your model, device, and data (1 point per option):\n",
    "#1. Implement a structured pruning technique. You may prune dimensions of matrices, attention heads, entire layers, etc. Describe your strategy and report the results in a table, adjusting the \"sparsity rate\" column and as needed.\n",
    "# Prune by magnitutude of combined parameters in a Conv2d filer a) 64x64 b) 128x128 c)256x256\n",
    "#2. Unstructured magnitutude pruning of a)64 input layers b)128 input layers c)256 input layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_structured_pruning(model_path, sparsity):\n",
    "\n",
    "    # Create empty DataFrame to store results\n",
    "    results_df = pd.DataFrame(columns=[\n",
    "        'iteration',\n",
    "        'sparsity_model',\n",
    "        'size_mb'\n",
    "    ])\n",
    "\n",
    "    model_path_list = [model_path]\n",
    "\n",
    "    layer_types = \n",
    "\n",
    "    for i in range(1, 8):\n",
    "\n",
    "        model, device = load_asl_model(model_path)\n",
    "        prune_params = [(m[1], \"weight\") for m in model.named_modules() if len(list(m[1].children()))==0 and not isinstance(m[1], (nn.ReLU, nn.MaxPool2d, nn.AdaptiveAvgPool2d))]\n",
    "        print(f\"Pruning iteration {i}\")\n",
    "\n",
    "        prune.global_unstructured(prune_params, pruning_method=prune.L1Unstructured, amount=sparsity)\n",
    "\n",
    "        saved_model_path = f\"models/pruned_model_{i}.pth\"\n",
    "        save_copy_of_model(model, saved_model_path)\n",
    "        model_path_list.append(saved_model_path)\n",
    "\n",
    "        size = sparse_evaluate(model, device)\n",
    "        sparsity_results = calculate_sparsity(model)\n",
    "\n",
    "\n",
    "        # Store results in DataFrame\n",
    "        results_df.loc[i] = {\n",
    "            'iteration': i,\n",
    "            'sparsity_model': sparsity_results['sparsity_model'],\n",
    "            'size_mb': size/1e6\n",
    "        }\n",
    "\n",
    "        print(f\"Sparsity for the model overall at Iteration {i}: {sparsity_results['sparsity_model']:.2f}%, Size MB: {size/1e6}\")\n",
    "    \n",
    "    \n",
    "    # Display the DataFrame\n",
    "    print(\"\\nFinal Results DataFrame:\")\n",
    "    print(results_df)\n",
    "    \n",
    "    return results_df, model_path_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruning_results, model_paths = model_structured_pruning(\"./models/model_weights_ResNet50_224_resize.pth\", sparsity=0.20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Conduct a sensitivity analysis of pruning (structured or unstructured) different components of your model. For instance, what happens to your model's performance when you prune input embeddings vs hidden layer weights? Do earlier layers seem more or less important than later layers? You are not required to conduct a thorough study, but you should be able to draw a couple concrete conclusions.\n",
    "\n",
    "    Fill in the following table with your results (choose any 2-3 pruned models to compare to the unpruned model):\n",
    "\n",
    "    |        Pruning Technique        |  Sparsity Rate  | Accuracy | Latency (s) | Disk Size (MB) |\n",
    "    | ------------------------------- | --------------- | -------- | ----------- | -------------- |\n",
    "    | Unstructured, all non-embedding |  30% global     |          |             |                |\n",
    "    | Structured, attention heads     |  50% per module |          |             |                |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
