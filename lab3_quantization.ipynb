{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _calculate_scale_and_zeropoint(\n",
    "    min_val: float, max_val: float, num_bits: int) -> Tuple[float, int]:\n",
    "    qmin = 0.\n",
    "    qmax = 2.**num_bits - 1.\n",
    "\n",
    "    scale = (max_val - min_val) / (qmax - qmin)\n",
    "\n",
    "    initial_zero_point = qmin - min_val / scale\n",
    "\n",
    "    zero_point = 0\n",
    "    if initial_zero_point < qmin:\n",
    "        zero_point = int(qmin)\n",
    "    elif initial_zero_point > qmax:\n",
    "        zero_point = int(qmax)\n",
    "    else:\n",
    "        zero_point = int(initial_zero_point)\n",
    "    \n",
    "    return scale, zero_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize(x: torch.Tensor, scale: float, zero_point: int, dtype=torch.uint8):\n",
    "    q_x = zero_point + x / scale\n",
    "    q_x.clamp_(0, 255).round_()\n",
    "    q_x = q_x.to(dtype)\n",
    "    return q_x\n",
    "\n",
    "def dequantize(x: torch.Tensor, scale: float, zero_point: int):\n",
    "    return scale * (x.float() - zero_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "def test_case_0():\n",
    "  torch.manual_seed(999)\n",
    "  test_input = torch.randn((4,4))\n",
    "\n",
    "  min_val, max_val = torch.min(test_input), torch.max(test_input)\n",
    "  scale, zero_point = _calculate_scale_and_zeropoint(min_val, max_val, 8)\n",
    "\n",
    "  your_quant = quantize(test_input, scale, zero_point)\n",
    "  your_dequant = dequantize(your_quant, scale, zero_point)\n",
    "\n",
    "  test_case_0 = torch.Tensor([\n",
    "      [-0.2623,  1.3991,  0.2842,  1.0275],\n",
    "      [-0.9838, -3.4104,  1.4866,  0.2405],\n",
    "      [ 1.4866, -0.3716,  0.0874,  2.1424],\n",
    "      [ 0.6340, -1.1587, -0.7870,  0.0656]])\n",
    "\n",
    "  assert torch.allclose(your_dequant, test_case_0, atol=1e-4)\n",
    "  assert torch.allclose(your_dequant, test_input, atol=5e-2)\n",
    "\n",
    "  return test_input, your_dequant, your_quant\n",
    "\n",
    "\n",
    "\n",
    "### Test Case 1\n",
    "def test_case_1():\n",
    "  torch.manual_seed(999)\n",
    "  test_input = torch.randn((8,8))\n",
    "\n",
    "  min_val, max_val = torch.min(test_input), torch.max(test_input)\n",
    "  scale, zero_point = _calculate_scale_and_zeropoint(min_val, max_val, 8)\n",
    "\n",
    "  your_quant = quantize(test_input, scale, zero_point)\n",
    "  your_dequant = dequantize(your_quant, scale, zero_point)\n",
    "\n",
    "  test_case_1 = torch.Tensor(\n",
    "      [[-0.2623,  1.3991,  0.2842,  1.0275, -0.9838, -3.4104,  1.4866,  0.2405],\n",
    "      [ 1.4866, -0.3716,  0.0874,  2.1424,  0.6340, -1.1587, -0.7870,  0.0656],\n",
    "      [ 0.0000, -0.6558, -1.0056,  0.3061,  0.6340, -1.0931, -1.6178,  1.5740],\n",
    "      [-1.7927,  0.6121, -0.7214,  0.6121,  0.3279, -1.5959, -0.5247,  0.3498],\n",
    "      [-1.3773,  1.1149, -0.7870,  0.2842,  0.9182, -1.1805, -0.7433, -1.5522],\n",
    "      [ 1.0056, -0.1093,  1.3991, -0.9182, -1.1805, -0.6777, -0.3061,  0.9838],\n",
    "      [ 0.2186,  1.6396,  1.0712,  1.7489,  0.0874,  0.3498,  0.9838,  1.2024],\n",
    "      [-0.3935, -0.6340,  1.9238,  1.2898,  0.0219,  0.3935,  1.4866, -0.9401]])\n",
    "\n",
    "  assert torch.allclose(your_dequant, test_case_1, atol=1e-4)\n",
    "  assert torch.allclose(your_dequant, test_input, atol=5e-2)\n",
    "\n",
    "  return test_input, your_dequant, your_quant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0059), tensor(0.0115))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Empirically, report the average and maximum quantization error for the test cases\n",
    "def test():\n",
    "  test_input, your_dequant, your_quant = test_case_0()\n",
    "  test_input, your_dequant, your_quant = test_case_1()\n",
    "\n",
    "  avg_error = torch.mean(torch.abs(test_input - your_dequant))\n",
    "  max_error = torch.max(torch.abs(test_input - your_dequant))\n",
    "\n",
    "  return avg_error, max_error\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the original fp32 tensor and quantized tensor to disk with torch.save. Report the difference in disk utilization\n",
    "output_folder = \"data/lab3\"\n",
    "\n",
    "def save_to_disk(test_input, your_quant, output_folder):\n",
    "    torch.save(test_input, f\"{output_folder}/test_input.pt\")\n",
    "    torch.save(your_quant, f\"{output_folder}/your_quant.pt\")\n",
    "    \n",
    "    test_input_size = test_input.element_size() * test_input.nelement()\n",
    "    your_quant_size = your_quant.element_size() * your_quant.nelement()\n",
    "    \n",
    "    return test_input_size, your_quant_size\n",
    "\n",
    "test_input, your_dequant, your_quant = test_case_1()\n",
    "save_to_disk(test_input, your_quant, output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from itertools import product\n",
    "\n",
    "import torch.ao.quantization as quantization\n",
    "from torch.ao.quantization import get_default_qconfig\n",
    "from torch.ao.quantization.quantize_fx import prepare_fx, convert_fx, prepare_qat_fx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform=None):\n",
    "        data = pd.read_csv(csv_file)\n",
    "        self.labels = data.iloc[:, 0].values\n",
    "        self.pixels = data.iloc[:, 1:].values.astype('float32')\n",
    "        self.pixels = self.pixels.reshape(-1, 28, 28)  # Reshape to 28x28 images\n",
    "\n",
    "        # Normalize the pixel values\n",
    "        self.pixels_mean = self.pixels.mean()\n",
    "        self.pixels_std = self.pixels.std()\n",
    "        self.pixels = (self.pixels - self.pixels_mean) / self.pixels_std\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.pixels[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(torch.tensor(image).unsqueeze(0))\n",
    "\n",
    "        return image.squeeze(0), torch.tensor(label)\n",
    "    \n",
    "\n",
    "class FFNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes, num_hidden_layers):\n",
    "        super(FFNN, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(nn.Linear(input_size, hidden_size))\n",
    "        for _ in range(num_hidden_layers - 1):\n",
    "            self.layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "        self.layers.append(nn.Linear(hidden_size, num_classes))\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten the input\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = self.relu(layer(x))\n",
    "        return self.layers[-1](x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader(dataset_path, batch_size, is_train=True):\n",
    "    # Create center crop transform\n",
    "    transform = transforms.Compose([\n",
    "        transforms.CenterCrop(20)  # Crop to 20x20 as specified\n",
    "    ])\n",
    "    \n",
    "    # Create dataset and dataloader\n",
    "    dataset = MNISTDataset(dataset_path, transform=transform)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=is_train)\n",
    "\n",
    "def print_size_of_model(model, label=\"\"):\n",
    "    torch.save(model.state_dict(), \"temp.p\")\n",
    "    size=os.path.getsize(\"temp.p\")\n",
    "    print(\"model: \",label,' \\t','Size (MB):', size/1e6)\n",
    "    os.remove('temp.p')\n",
    "    return size\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, epochs, learning_rate, device):\n",
    "    print(f\"Training normal precision model for {epochs} epochs\")\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for data, target in val_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                outputs = model(data)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += target.size(0)\n",
    "                correct += (predicted == target).sum().item()\n",
    "        \n",
    "        print(f'Epoch {epoch+1}, Accuracy: {100 * correct / total:.2f}%')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_mixed_precision(model, train_loader, val_loader, epochs, learning_rate, device):\n",
    "    print(f\"Training model with mixed precision for {epochs} epochs\")\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Do mixed precision training with torch.autocast and GradScaler\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            with torch.cuda.amp.autocast():\n",
    "                output = model(data)\n",
    "                loss = criterion(output, target)\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for data, target in val_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                outputs = model(data)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += target.size(0)\n",
    "                correct += (predicted == target).sum().item()\n",
    "        \n",
    "        print(f'Epoch {epoch+1}, Accuracy: {100 * correct / total:.2f}%')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_inference_time(model, test_loader, batch_size, num_runs=5):\n",
    "    model.eval()\n",
    "    times = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_runs):\n",
    "            data, _ = next(iter(test_loader))\n",
    "            if batch_size == 1:\n",
    "                data = data[0:1]\n",
    "                \n",
    "            start_time = time.time()\n",
    "            _ = model(data)\n",
    "            end_time = time.time()\n",
    "            times.append(end_time - start_time)\n",
    "    \n",
    "    mean_time = np.mean(times)\n",
    "    std_time = np.std(times)\n",
    "    return mean_time, std_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, mixed_precision=False):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            if mixed_precision:\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    outputs = model(data)\n",
    "            else:\n",
    "                outputs = model(data)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "    return 100 * correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamic Quantization\n",
    "def apply_dynamic_quantization(model):\n",
    "    return torch.quantization.quantize_dynamic(\n",
    "        model,\n",
    "        {nn.Linear},\n",
    "        dtype=torch.qint8\n",
    "    )\n",
    "\n",
    "# Static Quantization\n",
    "def apply_static_quantization(model, calibration_loader):\n",
    "    model.eval()\n",
    "    \n",
    "    # Set the qconfig\n",
    "    qconfig = get_default_qconfig(\"fbgemm\")\n",
    "    qconfig_dict = {\"\": qconfig}\n",
    "    \n",
    "    example_input = torch.randn(1, 1, 20, 20)  # 1-channel (grayscale), 20x20\n",
    "    \n",
    "    # Prepare the model for static quantization\n",
    "    prepared_model = prepare_fx(model, qconfig_dict, example_input)\n",
    "    \n",
    "    # Calibrate with the training data\n",
    "    with torch.no_grad():\n",
    "        for data, _ in calibration_loader:\n",
    "            prepared_model(data.unsqueeze(1))  # Ensure correct input shape for calibration\n",
    "    \n",
    "    # Convert to quantized model\n",
    "    quantized_model = convert_fx(prepared_model)\n",
    "    \n",
    "    return quantized_model\n",
    "\n",
    "\n",
    "# Quantization Aware Training\n",
    "def apply_qat(model, train_loader, val_loader, epochs=2):\n",
    "    model.train()\n",
    "    \n",
    "    # Set the qconfig\n",
    "    qconfig = get_default_qconfig(\"fbgemm\")\n",
    "    qconfig_dict = {\"\": qconfig}\n",
    "\n",
    "    example_input = torch.randn(1, 1, 20, 20)  # 1-channel (grayscale), 20x20\n",
    "    \n",
    "    # Prepare the model for QAT\n",
    "    prepared_model = prepare_qat_fx(model, qconfig_dict, example_input)\n",
    "    \n",
    "    # Train the model\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(prepared_model.parameters(), lr=0.001)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for data, target in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = prepared_model(data.unsqueeze(1))  # Ensure correct input shape\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    # Convert to final quantized model\n",
    "    quantized_model = convert_fx(prepared_model)\n",
    "    \n",
    "    return quantized_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Hyperparameters\n",
    "    input_size = 20 * 20  # 20x20 pixels\n",
    "    hidden_size = 1024\n",
    "    num_classes = 10\n",
    "    num_hidden_layers = 2\n",
    "    batch_size = 64\n",
    "    learning_rate = 0.001\n",
    "    epochs = 2\n",
    "    \n",
    "    # Create model\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = FFNN(input_size, hidden_size, num_classes, num_hidden_layers)\n",
    "    mixed_model = deepcopy(model)\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader = create_dataloader('data/mnist_train.csv', batch_size, True)\n",
    "    test_loader = create_dataloader('data/mnist_test.csv', batch_size, False)\n",
    "    \n",
    "    # Train base model\n",
    "    \n",
    "    model = model.to(device)\n",
    "    model = train_model(model, train_loader, test_loader, epochs, learning_rate, device)\n",
    "    model = model.to(\"cpu\")\n",
    "\n",
    "    # Mixed Precision Training\n",
    "    mixed_model = mixed_model.to(device)\n",
    "    mixed_precision_model = train_model_mixed_precision(mixed_model, train_loader, test_loader, epochs, learning_rate, device)\n",
    "    mixed_precision_model = mixed_precision_model.to(\"cpu\")\n",
    "    \n",
    "    # Baseline evaluation\n",
    "    base_acc = evaluate_model(model, test_loader)\n",
    "    base_size = print_size_of_model(model, \"Baseline\")\n",
    "    base_latency_b1, base_std_b1 = measure_inference_time(model, test_loader, 1)\n",
    "    base_latency_b64, base_std_b64 = measure_inference_time(model, test_loader, 64)\n",
    "    base_params = count_parameters(model)\n",
    "\n",
    "    # Mixed Precision evaluation\n",
    "    mixed_precision_acc = evaluate_model(mixed_precision_model, test_loader, mixed_precision=True)\n",
    "    mixed_precision_size = print_size_of_model(mixed_precision_model, \"Mixed Precision\")\n",
    "    mixed_precision_latency_b1, mixed_precision_std_b1 = measure_inference_time(mixed_precision_model, test_loader, 1)\n",
    "    mixed_precision_latency_b64, mixed_precision_std_b64 = measure_inference_time(mixed_precision_model, test_loader, 64)\n",
    "    mixed_precision_params = count_parameters(mixed_precision_model)\n",
    "    \n",
    "    # Dynamic Quantization\n",
    "    model = model.cpu()\n",
    "    quantized_dynamic = apply_dynamic_quantization(model)\n",
    "    dynamic_acc = evaluate_model(quantized_dynamic, test_loader)\n",
    "    dynamic_size = print_size_of_model(quantized_dynamic, \"Dynamic Quantized\")\n",
    "    dynamic_latency_b1, dynamic_std_b1 = measure_inference_time(quantized_dynamic, test_loader, 1)\n",
    "    dynamic_latency_b64, dynamic_std_b64 = measure_inference_time(quantized_dynamic, test_loader, 64)\n",
    "    dynamic_params = count_parameters(quantized_dynamic)\n",
    "    \n",
    "    # Static Quantization\n",
    "    static_quantized = apply_static_quantization(model, test_loader)\n",
    "    static_acc = evaluate_model(static_quantized, test_loader)\n",
    "    static_size = print_size_of_model(static_quantized, \"Static Quantized\")\n",
    "    static_latency_b1, static_std_b1 = measure_inference_time(static_quantized, test_loader, 1)\n",
    "    static_latency_b64, static_std_b64 = measure_inference_time(static_quantized, test_loader, 64)\n",
    "    static_params = count_parameters(static_quantized)\n",
    "    \n",
    "    # QAT\n",
    "    qat_model = apply_qat(model, train_loader, test_loader)\n",
    "    qat_acc = evaluate_model(qat_model, test_loader)\n",
    "    qat_size = print_size_of_model(qat_model, \"QAT\")\n",
    "    qat_latency_b1, qat_std_b1 = measure_inference_time(qat_model, test_loader, 1)\n",
    "    qat_latency_b64, qat_std_b64 = measure_inference_time(qat_model, test_loader, 64)\n",
    "    qat_params = count_parameters(qat_model)\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\nResults Summary:\")\n",
    "    print(f\"{'Model Type':<15} {'Accuracy':<10} {'Size (MB)':<12} {'Latency B1':<15} {'Latency B64':<15} {'Parameters':<12}\")\n",
    "    print(\"-\" * 90)\n",
    "    print(f\"{'Baseline':<15} {base_acc:.2f}% {base_size/1e6:.2f} {base_latency_b1*1000:.2f}±{base_std_b1*1000:.2f}ms {base_latency_b64*1000:.2f}±{base_std_b64*1000:.2f}ms {base_params}\")\n",
    "    print(f\"{'Mixed Precision':<15} {mixed_precision_acc:.2f}% {mixed_precision_size/1e6:.2f} {mixed_precision_latency_b1*1000:.2f}±{mixed_precision_std_b1*1000:.2f}ms {mixed_precision_latency_b64*1000:.2f}±{mixed_precision_std_b64*1000:.2f}ms {mixed_precision_params}\")\n",
    "    print(f\"{'Dynamic':<15} {dynamic_acc:.2f}% {dynamic_size/1e6:.2f} {dynamic_latency_b1*1000:.2f}±{dynamic_std_b1*1000:.2f}ms {dynamic_latency_b64*1000:.2f}±{dynamic_std_b64*1000:.2f}ms {dynamic_params}\")\n",
    "    print(f\"{'Static':<15} {static_acc:.2f}% {static_size/1e6:.2f} {static_latency_b1*1000:.2f}±{static_std_b1*1000:.2f}ms {static_latency_b64*1000:.2f}±{static_std_b64*1000:.2f}ms {static_params}\")\n",
    "    print(f\"{'QAT':<15} {qat_acc:.2f}% {qat_size/1e6:.2f} {qat_latency_b1*1000:.2f}±{qat_std_b1*1000:.2f}ms {qat_latency_b64*1000:.2f}±{qat_std_b64*1000:.2f}ms {qat_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training normal precision model for 2 epochs\n",
      "Epoch 1, Accuracy: 97.17%\n",
      "Epoch 2, Accuracy: 97.41%\n",
      "Training model with mixed precision for 2 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Johnny Chen\\AppData\\Local\\Temp\\ipykernel_10916\\2402911138.py:7: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n",
      "e:\\anaconda3\\envs\\p3-env\\lib\\site-packages\\torch\\amp\\grad_scaler.py:132: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Johnny Chen\\AppData\\Local\\Temp\\ipykernel_10916\\2402911138.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "e:\\anaconda3\\envs\\p3-env\\lib\\site-packages\\torch\\amp\\autocast_mode.py:265: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Accuracy: 96.74%\n",
      "Epoch 2, Accuracy: 97.14%\n",
      "model:  Baseline  \t Size (MB): 5.884202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Johnny Chen\\AppData\\Local\\Temp\\ipykernel_10916\\3545966847.py:8: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model:  Mixed Precision  \t Size (MB): 5.884202\n",
      "model:  Dynamic Quantized  \t Size (MB): 1.480898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\anaconda3\\envs\\p3-env\\lib\\site-packages\\torch\\ao\\quantization\\quantize_fx.py:147: FutureWarning: Passing a QConfig dictionary to prepare is deprecated and will not be supported in a future version. Please pass in a QConfigMapping instead.\n",
      "  prepared = prepare(\n",
      "e:\\anaconda3\\envs\\p3-env\\lib\\site-packages\\torch\\ao\\quantization\\observer.py:221: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model:  Static Quantized  \t Size (MB): 1.515474\n",
      "model:  QAT  \t Size (MB): 1.515474\n",
      "\n",
      "Results Summary:\n",
      "Model Type      Accuracy   Size (MB)    Latency B1      Latency B64     Parameters  \n",
      "------------------------------------------------------------------------------------------\n",
      "Baseline        97.41% 5.88 0.90±0.20ms 1.54±0.50ms 1470474\n",
      "Mixed Precision 97.14% 5.88 0.20±0.40ms 1.75±0.45ms 1470474\n",
      "Dynamic         97.39% 1.48 0.60±0.49ms 1.20±0.50ms 0\n",
      "Static          97.28% 1.52 1.00±0.55ms 0.80±0.40ms 0\n",
      "QAT             97.42% 1.52 0.60±0.49ms 0.79±0.40ms 0\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sensitivity_analysis(calibration_loader):\n",
    "    \"\"\"\n",
    "    Analyze accuracy and inference time in the SST/MNIST model from quantizing just one module or layer at a time. \n",
    "    Your results will depend in part on the structure of your model; looping through the named_modules as in the documentation code will include modules as well as \"leaf\" layers.\n",
    "    \"\"\"\n",
    "\n",
    "    # Hyperparameters\n",
    "    input_size = 20 * 20  # 20x20 pixels\n",
    "    hidden_size = 1024\n",
    "    num_classes = 10\n",
    "    num_hidden_layers = 2\n",
    "    batch_size = 64\n",
    "    learning_rate = 0.001\n",
    "    epochs = 2\n",
    "    \n",
    "    # Create model\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = FFNN(input_size, hidden_size, num_classes, num_hidden_layers)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader = create_dataloader('data/mnist_train.csv', batch_size, True)\n",
    "    test_loader = create_dataloader('data/mnist_test.csv', batch_size, False)\n",
    "    \n",
    "    # Train base model\n",
    "    model = train_model(model, train_loader, test_loader, epochs, learning_rate, device)\n",
    "    \n",
    "    # Evaluate base model\n",
    "    model = model.to(\"cpu\")\n",
    "    base_acc = evaluate_model(model, test_loader)\n",
    "    base_latency_b1, base_std_b1 = measure_inference_time(model, test_loader, 1)\n",
    "    base_latency_b64, base_std_b64 = measure_inference_time(model, test_loader, 64)\n",
    "    \n",
    "    # Quantize one module at a time\n",
    "    results = []\n",
    "    model = model.cpu()\n",
    "    example_input = torch.randn(1, 1, 20, 20)  # 1-channel (grayscale), 20x20\n",
    "    for name, module in model.named_modules():\n",
    "        \n",
    "        print(\"Only quantizing part: \", name)\n",
    "\n",
    "        # The module_name key allows module-specific qconfigs. \n",
    "        qconfig_dict = {\"\": None, \n",
    "                        \"module_name\": [(name, torch.ao.quantization.get_default_qconfig(\"fbgemm\"))]}\n",
    "\n",
    "        model_prepared = prepare_fx(model, qconfig_dict, example_input)\n",
    "        # Calibrate with the training data\n",
    "        model_prepared.eval()\n",
    "        with torch.no_grad():\n",
    "            for data, _ in calibration_loader:\n",
    "                model_prepared(data.unsqueeze(1))  # Ensure correct input shape for calibration\n",
    "\n",
    "        model_quantized = convert_fx(model_prepared)\n",
    "        \n",
    "        # Evaluate quantized model\n",
    "        acc = evaluate_model(model_quantized, test_loader)\n",
    "        latency_b1, std_b1 = measure_inference_time(model_quantized, test_loader, 1)\n",
    "        latency_b64, std_b64 = measure_inference_time(model_quantized, test_loader, 64)\n",
    "        \n",
    "        results.append({\n",
    "            \"layer\": name,\n",
    "            \"accuracy\": acc,\n",
    "            \"latency_b1\": latency_b1,\n",
    "            \"std_b1\": std_b1,\n",
    "            \"latency_b64\": latency_b64,\n",
    "            \"std_b64\": std_b64\n",
    "        })\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\nSensitivity Analysis Results:\")\n",
    "    print(f\"{'Layer':<20} {'Accuracy':<10} {'Latency B1':<15} {'Std B1':<15} {'Latency B64':<15} {'Std B64':<15}\")\n",
    "    print(\"-\" * 90)\n",
    "    for result in results:\n",
    "        print(f\"{result['layer']:<20} {result['accuracy']:.2f}% {result['latency_b1']*1000:.2f}±{result['std_b1']*1000:.2f}ms {result['latency_b64']*1000:.2f}±{result['std_b64']*1000:.2f}ms\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training normal precision model for 2 epochs\n",
      "Epoch 1, Accuracy: 97.05%\n",
      "Epoch 2, Accuracy: 97.17%\n",
      "Only quantizing part:  \n",
      "Only quantizing part:  layers\n",
      "Only quantizing part:  layers.0\n",
      "Only quantizing part:  layers.1\n",
      "Only quantizing part:  layers.2\n",
      "Only quantizing part:  relu\n",
      "\n",
      "Sensitivity Analysis Results:\n",
      "Layer                Accuracy   Latency B1      Std B1          Latency B64     Std B64        \n",
      "------------------------------------------------------------------------------------------\n",
      "                     97.17% 0.00±0.00ms 1.22±0.23ms\n",
      "layers               97.07% 0.70±0.40ms 0.60±0.49ms\n",
      "layers.0             97.17% 1.10±0.49ms 5.00±3.04ms\n",
      "layers.1             97.13% 1.03±0.32ms 1.26±0.39ms\n",
      "layers.2             97.06% 1.00±0.00ms 2.91±0.74ms\n",
      "relu                 97.17% 1.00±0.00ms 6.72±2.73ms\n"
     ]
    }
   ],
   "source": [
    "test_loader = create_dataloader('data/mnist_test.csv', 64, False)\n",
    "sensitivity_analysis(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sensitivity_analysis_exclude(calibration_loader):\n",
    "    \"\"\"\n",
    "    Analyze accuracy and inference time in the SST/MNIST model by quantizing all but one module at a time.\n",
    "    Only modules that do not have child modules will be looped through for this analysis.\n",
    "    \"\"\"\n",
    "\n",
    "    # Hyperparameters\n",
    "    input_size = 20 * 20  # 20x20 pixels\n",
    "    hidden_size = 1024\n",
    "    num_classes = 10\n",
    "    num_hidden_layers = 2\n",
    "    batch_size = 64\n",
    "    learning_rate = 0.001\n",
    "    epochs = 2\n",
    "    \n",
    "    # Create model\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = FFNN(input_size, hidden_size, num_classes, num_hidden_layers)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader = create_dataloader('data/mnist_train.csv', batch_size, True)\n",
    "    test_loader = create_dataloader('data/mnist_test.csv', batch_size, False)\n",
    "    \n",
    "    # Train base model\n",
    "    model = train_model(model, train_loader, test_loader, epochs, learning_rate, device)\n",
    "    \n",
    "    # Evaluate base model\n",
    "    model = model.to(\"cpu\")\n",
    "    base_acc = evaluate_model(model, test_loader)\n",
    "    base_latency_b1, base_std_b1 = measure_inference_time(model, test_loader, 1)\n",
    "    base_latency_b64, base_std_b64 = measure_inference_time(model, test_loader, 64)\n",
    "    \n",
    "    # Quantize all but one module at a time\n",
    "    results = []\n",
    "    model = model.cpu()\n",
    "    example_input = torch.randn(1, 1, 20, 20)  # 1-channel (grayscale), 20x20\n",
    "    \n",
    "    # Identify leaf layers (layers without child modules)\n",
    "    leaf_layers = [name for name, module in model.named_modules() if len(list(module.children())) == 0]\n",
    "    \n",
    "    for exclude_layer in leaf_layers:\n",
    "        print(f\"Quantizing all except layer: {exclude_layer}\")\n",
    "\n",
    "        # Set up qconfig dictionary\n",
    "        qconfig_dict = {\"\": torch.ao.quantization.get_default_qconfig(\"fbgemm\")}\n",
    "        for layer in leaf_layers:\n",
    "            if layer == exclude_layer:\n",
    "                qconfig_dict[\"module_name\"] = [(layer, None)]  # Skip quantization for this layer\n",
    "\n",
    "        # Prepare and convert the model for quantization\n",
    "        model_prepared = prepare_fx(model, qconfig_dict, example_input)\n",
    "\n",
    "        # Calibrate\n",
    "        model_prepared.eval()\n",
    "        with torch.no_grad():\n",
    "            for data, _ in calibration_loader:\n",
    "                model_prepared(data.unsqueeze(1))\n",
    "\n",
    "        model_quantized = convert_fx(model_prepared)\n",
    "        \n",
    "        # Evaluate the quantized model\n",
    "        acc = evaluate_model(model_quantized, test_loader)\n",
    "        latency_b1, std_b1 = measure_inference_time(model_quantized, test_loader, 1)\n",
    "        latency_b64, std_b64 = measure_inference_time(model_quantized, test_loader, 64)\n",
    "        \n",
    "        results.append({\n",
    "            \"excluded_layer\": exclude_layer,\n",
    "            \"accuracy\": acc,\n",
    "            \"latency_b1\": latency_b1,\n",
    "            \"std_b1\": std_b1,\n",
    "            \"latency_b64\": latency_b64,\n",
    "            \"std_b64\": std_b64\n",
    "        })\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\nSensitivity Analysis Results:\")\n",
    "    print(f\"{'Excluded Layer':<20} {'Accuracy':<10} {'Latency B1':<15} {'Latency B64':<15}\")\n",
    "    print(\"-\" * 60)\n",
    "    for result in results:\n",
    "        print(f\"{result['excluded_layer']:<20} {result['accuracy']:.2f}% {result['latency_b1']*1000:.2f}±{result['std_b1']*1000:.2f}ms {result['latency_b64']*1000:.2f}±{result['std_b64']*1000:.2f}ms\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training normal precision model for 2 epochs\n",
      "Epoch 1, Accuracy: 97.54%\n",
      "Epoch 2, Accuracy: 97.55%\n",
      "Quantizing all except layer: layers.0\n",
      "Quantizing all except layer: layers.1\n",
      "Quantizing all except layer: layers.2\n",
      "Quantizing all except layer: relu\n",
      "\n",
      "Sensitivity Analysis Results:\n",
      "Excluded Layer       Accuracy   Latency B1      Latency B64    \n",
      "------------------------------------------------------------\n",
      "layers.0             97.43% 1.10±0.20ms 1.40±0.49ms\n",
      "layers.1             97.44% 0.60±0.49ms 0.90±0.49ms\n",
      "layers.2             97.53% 0.60±0.49ms 1.30±0.40ms\n",
      "relu                 97.44% 0.80±0.40ms 1.09±0.64ms\n"
     ]
    }
   ],
   "source": [
    "test_loader = create_dataloader('data/mnist_test.csv', 64, False)\n",
    "sensitivity_analysis_exclude(test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p3-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
