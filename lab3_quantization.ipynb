{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _calculate_scale_and_zeropoint(\n",
    "    min_val: float, max_val: float, num_bits: int) -> Tuple[float, int]:\n",
    "    qmin = 0.\n",
    "    qmax = 2.**num_bits - 1.\n",
    "\n",
    "    scale = (max_val - min_val) / (qmax - qmin)\n",
    "\n",
    "    initial_zero_point = qmin - min_val / scale\n",
    "\n",
    "    zero_point = 0\n",
    "    if initial_zero_point < qmin:\n",
    "        zero_point = int(qmin)\n",
    "    elif initial_zero_point > qmax:\n",
    "        zero_point = int(qmax)\n",
    "    else:\n",
    "        zero_point = int(initial_zero_point)\n",
    "    \n",
    "    return scale, zero_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize(x: torch.Tensor, scale: float, zero_point: int, dtype=torch.uint8):\n",
    "    q_x = zero_point + x / scale\n",
    "    q_x.clamp_(0, 255).round_()\n",
    "    q_x = q_x.to(dtype)\n",
    "    return q_x\n",
    "\n",
    "def dequantize(x: torch.Tensor, scale: float, zero_point: int):\n",
    "    return scale * (x.float() - zero_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "def test_case_0():\n",
    "  torch.manual_seed(999)\n",
    "  test_input = torch.randn((4,4))\n",
    "\n",
    "  min_val, max_val = torch.min(test_input), torch.max(test_input)\n",
    "  scale, zero_point = _calculate_scale_and_zeropoint(min_val, max_val, 8)\n",
    "\n",
    "  your_quant = quantize(test_input, scale, zero_point)\n",
    "  your_dequant = dequantize(your_quant, scale, zero_point)\n",
    "\n",
    "  test_case_0 = torch.Tensor([\n",
    "      [-0.2623,  1.3991,  0.2842,  1.0275],\n",
    "      [-0.9838, -3.4104,  1.4866,  0.2405],\n",
    "      [ 1.4866, -0.3716,  0.0874,  2.1424],\n",
    "      [ 0.6340, -1.1587, -0.7870,  0.0656]])\n",
    "\n",
    "  assert torch.allclose(your_dequant, test_case_0, atol=1e-4)\n",
    "  assert torch.allclose(your_dequant, test_input, atol=5e-2)\n",
    "\n",
    "  return test_input, your_dequant, your_quant\n",
    "\n",
    "\n",
    "\n",
    "### Test Case 1\n",
    "def test_case_1():\n",
    "  torch.manual_seed(999)\n",
    "  test_input = torch.randn((8,8))\n",
    "\n",
    "  min_val, max_val = torch.min(test_input), torch.max(test_input)\n",
    "  scale, zero_point = _calculate_scale_and_zeropoint(min_val, max_val, 8)\n",
    "\n",
    "  your_quant = quantize(test_input, scale, zero_point)\n",
    "  your_dequant = dequantize(your_quant, scale, zero_point)\n",
    "\n",
    "  test_case_1 = torch.Tensor(\n",
    "      [[-0.2623,  1.3991,  0.2842,  1.0275, -0.9838, -3.4104,  1.4866,  0.2405],\n",
    "      [ 1.4866, -0.3716,  0.0874,  2.1424,  0.6340, -1.1587, -0.7870,  0.0656],\n",
    "      [ 0.0000, -0.6558, -1.0056,  0.3061,  0.6340, -1.0931, -1.6178,  1.5740],\n",
    "      [-1.7927,  0.6121, -0.7214,  0.6121,  0.3279, -1.5959, -0.5247,  0.3498],\n",
    "      [-1.3773,  1.1149, -0.7870,  0.2842,  0.9182, -1.1805, -0.7433, -1.5522],\n",
    "      [ 1.0056, -0.1093,  1.3991, -0.9182, -1.1805, -0.6777, -0.3061,  0.9838],\n",
    "      [ 0.2186,  1.6396,  1.0712,  1.7489,  0.0874,  0.3498,  0.9838,  1.2024],\n",
    "      [-0.3935, -0.6340,  1.9238,  1.2898,  0.0219,  0.3935,  1.4866, -0.9401]])\n",
    "\n",
    "  assert torch.allclose(your_dequant, test_case_1, atol=1e-4)\n",
    "  assert torch.allclose(your_dequant, test_input, atol=5e-2)\n",
    "\n",
    "  return test_input, your_dequant, your_quant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0059), tensor(0.0115))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Empirically, report the average and maximum quantization error for the test cases\n",
    "def test():\n",
    "  test_input, your_dequant, your_quant = test_case_0()\n",
    "  test_input, your_dequant, your_quant = test_case_1()\n",
    "\n",
    "  avg_error = torch.mean(torch.abs(test_input - your_dequant))\n",
    "  max_error = torch.max(torch.abs(test_input - your_dequant))\n",
    "\n",
    "  return avg_error, max_error\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the original fp32 tensor and quantized tensor to disk with torch.save. Report the difference in disk utilization\n",
    "output_folder = \"data/lab3\"\n",
    "\n",
    "def save_to_disk(test_input, your_quant, output_folder):\n",
    "    torch.save(test_input, f\"{output_folder}/test_input.pt\")\n",
    "    torch.save(your_quant, f\"{output_folder}/your_quant.pt\")\n",
    "    \n",
    "    test_input_size = test_input.element_size() * test_input.nelement()\n",
    "    your_quant_size = your_quant.element_size() * your_quant.nelement()\n",
    "    \n",
    "    return test_input_size, your_quant_size\n",
    "\n",
    "test_input, your_dequant, your_quant = test_case_1()\n",
    "save_to_disk(test_input, your_quant, output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from itertools import product\n",
    "\n",
    "import torch.ao.quantization as quantization\n",
    "from torch.ao.quantization import get_default_qconfig\n",
    "from torch.ao.quantization.quantize_fx import prepare_fx, convert_fx, prepare_qat_fx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform=None):\n",
    "        data = pd.read_csv(csv_file)\n",
    "        self.labels = data.iloc[:, 0].values\n",
    "        self.pixels = data.iloc[:, 1:].values.astype('float32')\n",
    "        self.pixels = self.pixels.reshape(-1, 28, 28)  # Reshape to 28x28 images\n",
    "\n",
    "        # Normalize the pixel values\n",
    "        self.pixels_mean = self.pixels.mean()\n",
    "        self.pixels_std = self.pixels.std()\n",
    "        self.pixels = (self.pixels - self.pixels_mean) / self.pixels_std\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.pixels[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(torch.tensor(image).unsqueeze(0))\n",
    "\n",
    "        return image.squeeze(0), torch.tensor(label)\n",
    "    \n",
    "\n",
    "class FFNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes, num_hidden_layers):\n",
    "        super(FFNN, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(nn.Linear(input_size, hidden_size))\n",
    "        for _ in range(num_hidden_layers - 1):\n",
    "            self.layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "        self.layers.append(nn.Linear(hidden_size, num_classes))\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten the input\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = self.relu(layer(x))\n",
    "        return self.layers[-1](x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader(dataset_path, batch_size, is_train=True):\n",
    "    # Create center crop transform\n",
    "    transform = transforms.Compose([\n",
    "        transforms.CenterCrop(20)  # Crop to 20x20 as specified\n",
    "    ])\n",
    "    \n",
    "    # Create dataset and dataloader\n",
    "    dataset = MNISTDataset(dataset_path, transform=transform)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=is_train)\n",
    "\n",
    "def print_size_of_model(model, label=\"\"):\n",
    "    torch.save(model.state_dict(), \"temp.p\")\n",
    "    size=os.path.getsize(\"temp.p\")\n",
    "    print(\"model: \",label,' \\t','Size (MB):', size/1e6)\n",
    "    os.remove('temp.p')\n",
    "    return size\n",
    "\n",
    "def count_parameters(model):\n",
    "    total_params = 0\n",
    "    for name, buffer in model.named_buffers():\n",
    "        if 'weight' in name or 'bias' in name:\n",
    "            total_params += buffer.numel()\n",
    "    return total_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, epochs, learning_rate, device):\n",
    "    print(f\"Training normal precision model for {epochs} epochs\")\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for data, target in val_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                outputs = model(data)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += target.size(0)\n",
    "                correct += (predicted == target).sum().item()\n",
    "        \n",
    "        print(f'Epoch {epoch+1}, Accuracy: {100 * correct / total:.2f}%')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_mixed_precision(model, train_loader, val_loader, epochs, learning_rate, device):\n",
    "    print(f\"Training model with mixed precision for {epochs} epochs\")\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Do mixed precision training with torch.autocast and GradScaler\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            with torch.cuda.amp.autocast():\n",
    "                output = model(data)\n",
    "                loss = criterion(output, target)\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for data, target in val_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                outputs = model(data)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += target.size(0)\n",
    "                correct += (predicted == target).sum().item()\n",
    "        \n",
    "        print(f'Epoch {epoch+1}, Accuracy: {100 * correct / total:.2f}%')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_inference_time(model, test_loader, batch_size, num_runs=5):\n",
    "    model.eval()\n",
    "    times = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_runs):\n",
    "            data, _ = next(iter(test_loader))\n",
    "            if batch_size == 1:\n",
    "                data = data[0:1]\n",
    "                \n",
    "            start_time = time.time()\n",
    "            _ = model(data)\n",
    "            end_time = time.time()\n",
    "            times.append(end_time - start_time)\n",
    "    \n",
    "    mean_time = np.mean(times)\n",
    "    std_time = np.std(times)\n",
    "    return mean_time, std_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, mixed_precision=False):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            if mixed_precision:\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    outputs = model(data)\n",
    "            else:\n",
    "                outputs = model(data)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "    return 100 * correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamic Quantization\n",
    "def apply_dynamic_quantization(model):\n",
    "    return torch.quantization.quantize_dynamic(\n",
    "        model,\n",
    "        {nn.Linear},\n",
    "        dtype=torch.qint8\n",
    "    )\n",
    "\n",
    "# Static Quantization\n",
    "def apply_static_quantization(model, calibration_loader):\n",
    "    model.eval()\n",
    "    \n",
    "    # Set the qconfig\n",
    "    qconfig = get_default_qconfig(\"fbgemm\")\n",
    "    qconfig_dict = {\"\": qconfig}\n",
    "    \n",
    "    example_input = torch.randn(1, 1, 20, 20)  # 1-channel (grayscale), 20x20\n",
    "    \n",
    "    # Prepare the model for static quantization\n",
    "    prepared_model = prepare_fx(model, qconfig_dict, example_input)\n",
    "    \n",
    "    # Calibrate with the training data\n",
    "    with torch.no_grad():\n",
    "        for data, _ in calibration_loader:\n",
    "            prepared_model(data.unsqueeze(1))  # Ensure correct input shape for calibration\n",
    "    \n",
    "    # Convert to quantized model\n",
    "    quantized_model = convert_fx(prepared_model)\n",
    "    \n",
    "    return quantized_model\n",
    "\n",
    "\n",
    "# Quantization Aware Training\n",
    "def apply_qat(model, train_loader, val_loader, epochs=2):\n",
    "    model.train()\n",
    "    \n",
    "    # Set the qconfig\n",
    "    qconfig = get_default_qconfig(\"fbgemm\")\n",
    "    qconfig_dict = {\"\": qconfig}\n",
    "\n",
    "    example_input = torch.randn(1, 1, 20, 20)  # 1-channel (grayscale), 20x20\n",
    "    \n",
    "    # Prepare the model for QAT\n",
    "    prepared_model = prepare_qat_fx(model, qconfig_dict, example_input)\n",
    "    \n",
    "    # Train the model\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(prepared_model.parameters(), lr=0.001)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for data, target in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = prepared_model(data.unsqueeze(1))  # Ensure correct input shape\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    # Convert to final quantized model\n",
    "    quantized_model = convert_fx(prepared_model)\n",
    "    \n",
    "    return quantized_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Hyperparameters\n",
    "    input_size = 20 * 20  # 20x20 pixels\n",
    "    hidden_size = 1024\n",
    "    num_classes = 10\n",
    "    num_hidden_layers = 2\n",
    "    batch_size = 64\n",
    "    learning_rate = 0.001\n",
    "    epochs = 2\n",
    "    \n",
    "    # Create model\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = FFNN(input_size, hidden_size, num_classes, num_hidden_layers)\n",
    "    mixed_model = deepcopy(model)\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader = create_dataloader('data/mnist_train.csv', batch_size, True)\n",
    "    test_loader = create_dataloader('data/mnist_test.csv', batch_size, False)\n",
    "    \n",
    "    # Train base model\n",
    "    \n",
    "    model = model.to(device)\n",
    "    model = train_model(model, train_loader, test_loader, epochs, learning_rate, device)\n",
    "\n",
    "    # Mixed Precision Training\n",
    "    mixed_model = mixed_model.to(device)\n",
    "    mixed_precision_model = train_model_mixed_precision(mixed_model, train_loader, test_loader, epochs, learning_rate, device)\n",
    "    \n",
    "    # Baseline evaluation\n",
    "    base_acc = evaluate_model(model, test_loader)\n",
    "    base_size = print_size_of_model(model, \"Baseline\")\n",
    "    base_latency_b1, base_std_b1 = measure_inference_time(model, test_loader, 1)\n",
    "    base_latency_b64, base_std_b64 = measure_inference_time(model, test_loader, 64)\n",
    "    base_params = count_parameters(model)\n",
    "\n",
    "    # Mixed Precision evaluation\n",
    "    mixed_precision_acc = evaluate_model(mixed_precision_model, test_loader, mixed_precision=True)\n",
    "    mixed_precision_size = print_size_of_model(mixed_precision_model, \"Mixed Precision\")\n",
    "    mixed_precision_latency_b1, mixed_precision_std_b1 = measure_inference_time(mixed_precision_model, test_loader, 1)\n",
    "    mixed_precision_latency_b64, mixed_precision_std_b64 = measure_inference_time(mixed_precision_model, test_loader, 64)\n",
    "    mixed_precision_params = count_parameters(mixed_precision_model)\n",
    "    \n",
    "    # Dynamic Quantization\n",
    "    model = model.cpu()\n",
    "    quantized_dynamic = apply_dynamic_quantization(model)\n",
    "    dynamic_acc = evaluate_model(quantized_dynamic, test_loader)\n",
    "    dynamic_size = print_size_of_model(quantized_dynamic, \"Dynamic Quantized\")\n",
    "    dynamic_latency_b1, dynamic_std_b1 = measure_inference_time(quantized_dynamic, test_loader, 1)\n",
    "    dynamic_latency_b64, dynamic_std_b64 = measure_inference_time(quantized_dynamic, test_loader, 64)\n",
    "    dynamic_params = count_parameters(quantized_dynamic)\n",
    "    \n",
    "    # Static Quantization\n",
    "    static_quantized = apply_static_quantization(model, test_loader)\n",
    "    static_acc = evaluate_model(static_quantized, test_loader)\n",
    "    static_size = print_size_of_model(static_quantized, \"Static Quantized\")\n",
    "    static_latency_b1, static_std_b1 = measure_inference_time(static_quantized, test_loader, 1)\n",
    "    static_latency_b64, static_std_b64 = measure_inference_time(static_quantized, test_loader, 64)\n",
    "    static_params = count_parameters(static_quantized)\n",
    "    \n",
    "    # QAT\n",
    "    qat_model = apply_qat(model, train_loader, test_loader)\n",
    "    qat_acc = evaluate_model(qat_model, test_loader)\n",
    "    qat_size = print_size_of_model(qat_model, \"QAT\")\n",
    "    qat_latency_b1, qat_std_b1 = measure_inference_time(qat_model, test_loader, 1)\n",
    "    qat_latency_b64, qat_std_b64 = measure_inference_time(qat_model, test_loader, 64)\n",
    "    qat_params = count_parameters(qat_model)\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\nResults Summary:\")\n",
    "    print(f\"{'Model Type':<15} {'Accuracy':<10} {'Size (MB)':<12} {'Latency B1':<15} {'Latency B64':<15} {'Parameters':<12}\")\n",
    "    print(\"-\" * 90)\n",
    "    print(f\"{'Baseline':<15} {base_acc:.2f}% {base_size/1e6:.2f} {base_latency_b1*1000:.2f}±{base_std_b1*1000:.2f}ms {base_latency_b64*1000:.2f}±{base_std_b64*1000:.2f}ms {base_params}\")\n",
    "    print(f\"{'Mixed Precision':<15} {mixed_precision_acc:.2f}% {mixed_precision_size/1e6:.2f} {mixed_precision_latency_b1*1000:.2f}±{mixed_precision_std_b1*1000:.2f}ms {mixed_precision_latency_b64*1000:.2f}±{mixed_precision_std_b64*1000:.2f}ms {mixed_precision_params}\")\n",
    "    print(f\"{'Dynamic':<15} {dynamic_acc:.2f}% {dynamic_size/1e6:.2f} {dynamic_latency_b1*1000:.2f}±{dynamic_std_b1*1000:.2f}ms {dynamic_latency_b64*1000:.2f}±{dynamic_std_b64*1000:.2f}ms {dynamic_params}\")\n",
    "    print(f\"{'Static':<15} {static_acc:.2f}% {static_size/1e6:.2f} {static_latency_b1*1000:.2f}±{static_std_b1*1000:.2f}ms {static_latency_b64*1000:.2f}±{static_std_b64*1000:.2f}ms {static_params}\")\n",
    "    print(f\"{'QAT':<15} {qat_acc:.2f}% {qat_size/1e6:.2f} {qat_latency_b1*1000:.2f}±{qat_std_b1*1000:.2f}ms {qat_latency_b64*1000:.2f}±{qat_std_b64*1000:.2f}ms {qat_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Accuracy: 97.17%\n",
      "Epoch 2, Accuracy: 97.41%\n",
      "model:  Baseline  \t Size (MB): 5.884202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Johnny Chen\\AppData\\Local\\Temp\\ipykernel_4692\\3384503572.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n",
      "e:\\anaconda3\\envs\\p3-env\\lib\\site-packages\\torch\\amp\\grad_scaler.py:132: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Johnny Chen\\AppData\\Local\\Temp\\ipykernel_4692\\3384503572.py:14: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "e:\\anaconda3\\envs\\p3-env\\lib\\site-packages\\torch\\amp\\autocast_mode.py:265: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Accuracy: 97.42%\n",
      "Epoch 2, Accuracy: 98.06%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Johnny Chen\\AppData\\Local\\Temp\\ipykernel_4692\\3545966847.py:8: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model:  Mixed Precision  \t Size (MB): 5.884202\n",
      "model:  Dynamic Quantized  \t Size (MB): 1.480898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\anaconda3\\envs\\p3-env\\lib\\site-packages\\torch\\ao\\quantization\\quantize_fx.py:147: FutureWarning: Passing a QConfig dictionary to prepare is deprecated and will not be supported in a future version. Please pass in a QConfigMapping instead.\n",
      "  prepared = prepare(\n",
      "e:\\anaconda3\\envs\\p3-env\\lib\\site-packages\\torch\\ao\\quantization\\observer.py:221: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model:  Static Quantized  \t Size (MB): 1.515474\n",
      "model:  QAT  \t Size (MB): 1.515474\n",
      "\n",
      "Results Summary:\n",
      "Model Type      Accuracy   Size (MB)    Latency B1      Latency B64     Parameters  \n",
      "------------------------------------------------------------------------------------------\n",
      "Baseline        97.41% 5.88 0.20±0.40ms 0.86±0.45ms 0\n",
      "Mixed Precision 98.06% 5.88 0.00±0.00ms 1.10±0.20ms 0\n",
      "Dynamic         97.39% 1.48 0.20±0.40ms 0.80±0.40ms 0\n",
      "Static          97.28% 1.52 0.40±0.49ms 0.40±0.49ms 0\n",
      "QAT             97.42% 1.52 0.30±0.60ms 0.60±0.49ms 0\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p3-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
