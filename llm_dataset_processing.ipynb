{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I ran this on Databricks PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset_builder\n",
    "from psutil._common import bytes2human\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_dataset_size_if_provided(*args, **kwargs):\n",
    "  dataset_builder = load_dataset_builder(*args, **kwargs)\n",
    "\n",
    "  if dataset_builder.info.download_size and dataset_builder.info.dataset_size:\n",
    "    print(f'download_size={bytes2human(dataset_builder.info.download_size)}, dataset_size={bytes2human(dataset_builder.info.dataset_size)}')\n",
    "  else:\n",
    "    print('Dataset size is not provided by uploader')\n",
    "\n",
    "print_dataset_size_if_provided(\"community-datasets/generics_kb\", \"generics_kb_best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"community-datasets/generics_kb\", \"generics_kb_best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ds[\"train\"].to_pandas()\n",
    "\n",
    "# Print the first 5 rows\n",
    "print(df.columns)\n",
    "print(df.iloc[0:5][[\"term\", \"generic_sentence\"]])\n",
    "print(len(df))\n",
    "df = df[[\"term\", \"generic_sentence\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercase the terms\n",
    "df[\"term\"] = df[\"term\"].str.lower()\n",
    "# Remove punctuation\n",
    "df[\"generic_sentence\"] = df[\"generic_sentence\"].str.replace(r'[^\\w\\s]', '')\n",
    "\n",
    "# Deduplicate\n",
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out rows with terms and sentences that are too short or too long\n",
    "df = df[(df[\"term\"].apply(len) >= 3) & (df[\"term\"].apply(len) <= 20)]\n",
    "df = df[(df[\"generic_sentence\"].apply(len) >= 20) & (df[\"generic_sentence\"].apply(len) <= 40)]\n",
    "print(df.iloc[0:20][[\"term\", \"generic_sentence\"]])\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each term has multiple generic sentences. We will only keep the first few\n",
    "df = df.groupby(\"term\").head(3)\n",
    "print(df.iloc[0:20][[\"term\", \"generic_sentence\"]])\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataset\n",
    "display(df)\n",
    "# Saved as data/generic_sentences.csv\n",
    "# Turns out the dataset is full of \"Noun\" is blablabla type of sentences. Not very useful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This I ran locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"iastate/onestop_english\")\n",
    "\n",
    "df = ds[\"train\"].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "567\n",
      "27412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Johnny Chen\\AppData\\Local\\Temp\\ipykernel_15524\\1255900053.py:9: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df[\"text\"] = df[\"text\"].str.replace(r'[^a-zA-Z0-9\\s]', '').str.strip()\n"
     ]
    }
   ],
   "source": [
    "df = df[[\"text\"]]\n",
    "print(len(df))\n",
    "\n",
    "# Split it by sentence and period\n",
    "df = df[\"text\"].str.split(\"\\n\", expand=True).stack().reset_index(level=1, drop=True).reset_index().rename(columns={0: \"text\"})\n",
    "df = df[\"text\"].str.split(\".\", expand=True).stack().reset_index(level=1, drop=True).reset_index().rename(columns={0: \"text\"})\n",
    "\n",
    "# Only keep alphanumeric characters\n",
    "df[\"text\"] = df[\"text\"].str.replace(r'[^a-zA-Z0-9\\s]', '').str.strip()\n",
    "\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    27412.000000\n",
      "mean        78.844119\n",
      "std         69.536195\n",
      "min          0.000000\n",
      "25%          0.000000\n",
      "50%         73.000000\n",
      "75%        123.000000\n",
      "max        811.000000\n",
      "Name: length, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Calculate the length of each sentence\n",
    "df[\"length\"] = df[\"text\"].apply(len)\n",
    "print(df[\"length\"].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10107\n"
     ]
    }
   ],
   "source": [
    "df = df[(df[\"length\"] <= 100) & (df[\"length\"] >= 20)]\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textnoisr import noise\n",
    "\n",
    "def add_noise(text):\n",
    "    augmenter = noise.CharNoiseAugmenter(noise_level=0.1)\n",
    "    text = augmenter.add_noise(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"noisy_text\"] = df[\"text\"].apply(add_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  \\\n",
      "1  These are the questions in a debate about the ...   \n",
      "\n",
      "                                          noisy_text  \n",
      "1  These are the questions in adebate aout the in...  \n"
     ]
    }
   ],
   "source": [
    "print(df.iloc[0:1][[\"text\", \"noisy_text\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[[\"text\", \"noisy_text\"]].reset_index(drop=True)\n",
    "df.to_csv(\"data/onestop_english_sentences.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p3-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
