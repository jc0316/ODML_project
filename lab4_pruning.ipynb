{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.utils.prune as prune\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform=None):\n",
    "        data = pd.read_csv(csv_file)\n",
    "        self.labels = data.iloc[:, 0].values\n",
    "        self.pixels = data.iloc[:, 1:].values.astype('float32')\n",
    "        self.pixels = self.pixels.reshape(-1, 28, 28)  # Reshape to 28x28 images\n",
    "\n",
    "        # Normalize the pixel values\n",
    "        self.pixels_mean = self.pixels.mean()\n",
    "        self.pixels_std = self.pixels.std()\n",
    "        self.pixels = (self.pixels - self.pixels_mean) / self.pixels_std\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.pixels[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(torch.tensor(image).unsqueeze(0))\n",
    "\n",
    "        return image.squeeze(0), torch.tensor(label)\n",
    "    \n",
    "\n",
    "class FFNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes, num_hidden_layers):\n",
    "        super(FFNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_classes = num_classes\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(nn.Linear(input_size, hidden_size))\n",
    "        for _ in range(num_hidden_layers - 1):\n",
    "            self.layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "        self.layers.append(nn.Linear(hidden_size, num_classes))\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten the input\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = self.relu(layer(x))\n",
    "        return self.layers[-1](x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader(dataset_path, batch_size, is_train=True):\n",
    "    # Create center crop transform\n",
    "    transform = transforms.Compose([\n",
    "        transforms.CenterCrop(20)  # Crop to 20x20 as specified\n",
    "    ])\n",
    "    \n",
    "    # Create dataset and dataloader\n",
    "    dataset = MNISTDataset(dataset_path, transform=transform)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=is_train)\n",
    "\n",
    "def print_size_of_model(model, label=\"\"):\n",
    "    sd = model.state_dict()\n",
    "    for item in sd:\n",
    "        sd[item] = model.state_dict()[item].to_sparse()\n",
    "    \n",
    "    torch.save(sd, \"temp.pt\")\n",
    "    size=os.path.getsize(\"temp.pt\")\n",
    "    #print(\"model: \",label,' \\t','Size (MB):', size/1e6)\n",
    "    os.remove('temp.pt')\n",
    "    return size\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, epochs, learning_rate, device):\n",
    "    print(f\"Training normal precision model for {epochs} epochs\")\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for data, target in val_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                outputs = model(data)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += target.size(0)\n",
    "                correct += (predicted == target).sum().item()\n",
    "        \n",
    "        print(f'Epoch {epoch+1}, Accuracy: {100 * correct / total:.2f}%')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_inference_time(model, test_loader, batch_size=-1, num_runs=5):\n",
    "    model.eval()\n",
    "    times = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_runs):\n",
    "            data, _ = next(iter(test_loader))\n",
    "            if batch_size == 1:\n",
    "                data = data[0:1]\n",
    "                \n",
    "            start_time = time.time()\n",
    "            _ = model(data)\n",
    "            end_time = time.time()\n",
    "            times.append(end_time - start_time)\n",
    "    \n",
    "    mean_time = np.mean(times)\n",
    "    std_time = np.std(times)\n",
    "    return mean_time, std_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            outputs = model(data)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "\n",
    "    size = print_size_of_model(model, \"sparse\")\n",
    "    inference_time, inference_std = measure_inference_time(model, test_loader)\n",
    "    \n",
    "    return accuracy, size, inference_time, inference_std\n",
    "\n",
    "def sparse_evaluate(model, dataloader, device):\n",
    "    model.to(device)\n",
    "    model_copy = FFNN(\n",
    "        model.input_size,\n",
    "        model.hidden_size,\n",
    "        model.num_classes,\n",
    "        model.num_hidden_layers\n",
    "    )\n",
    "    prune_params = [(m[1], \"weight\") for m in model_copy.named_modules() if len(list(m[1].children()))==0 and not isinstance(m[1], nn.ReLU)]\n",
    "    for p in prune_params:\n",
    "        prune.identity(p[0], \"weight\")\n",
    "    # Copy the parameters\n",
    "    model_copy.load_state_dict(model.state_dict())\n",
    "    \n",
    "    copy_params = [(m[1], \"weight\") for m in model_copy.named_modules() if len(list(m[1].children()))==0 and not isinstance(m[1], nn.ReLU)]\n",
    "    # (we assume the same model architecture as the MNIST or SST-2 architecture we specify above)\n",
    "    for p in copy_params:\n",
    "        prune.remove(*p)\n",
    "    \n",
    "    return evaluate_model(model_copy, dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sparsity(model, print_results=False):\n",
    "    \"\"\"\n",
    "    Calculate the sparsity level (using the percent of elements that are 0) for:\n",
    "    - each parameter,\n",
    "    - all pruned parameters overall, and\n",
    "    - the model overall.\n",
    "    \n",
    "    Report each of these values: \n",
    "    - the sparsity level of each parameter, \n",
    "    - across all pruned parameters, and \n",
    "    - for the model overall. \n",
    "    \"\"\"\n",
    "    sparsity_per_parameter = {}\n",
    "    total_zero_count_pruned = 0\n",
    "    total_element_count_pruned = 0\n",
    "    total_zero_count_model = 0\n",
    "    total_element_count_model = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "    # Iterate over all buffers in the model\n",
    "    for name, buffer in model.named_buffers():\n",
    "        # Calculate the number of zero elements and total elements in the buffer\n",
    "        zero_count = (buffer == 0).sum().item()\n",
    "        total_elements = buffer.numel()\n",
    "        \n",
    "        # Calculate the sparsity level for this parameter\n",
    "        sparsity_per_parameter[name] = zero_count / total_elements * 100\n",
    "\n",
    "        # Check if this is a pruned parameter by looking for \"weight_mask\" or \"bias_mask\" in the name\n",
    "        if \"weight_mask\" in name or \"bias_mask\" in name:\n",
    "            total_zero_count_pruned += zero_count\n",
    "            total_element_count_pruned += total_elements\n",
    "\n",
    "        # Accumulate for overall model sparsity\n",
    "        total_zero_count_model += zero_count\n",
    "\n",
    "    # Calculate overall sparsity for pruned parameters and the entire model\n",
    "    sparsity_pruned_parameters = (total_zero_count_pruned / total_element_count_pruned * 100\n",
    "                                  if total_element_count_pruned > 0 else 0)\n",
    "    sparsity_model = total_zero_count_model / total_element_count_model * 100\n",
    "\n",
    "    # Print or return the results\n",
    "    if print_results:\n",
    "        print(\"Sparsity per parameter:\")\n",
    "        for name, sparsity in sparsity_per_parameter.items():\n",
    "            print(f\"  {name}: {sparsity:.2f}%\")\n",
    "        \n",
    "        print(f\"Sparsity across all pruned parameters: {sparsity_pruned_parameters:.2f}%\")\n",
    "        print(f\"Sparsity for the model overall: {sparsity_model:.2f}%\")\n",
    "\n",
    "    # Optionally, return the values for further use\n",
    "    return {\n",
    "        \"sparsity_per_parameter\": sparsity_per_parameter,\n",
    "        \"sparsity_pruned_parameters\": sparsity_pruned_parameters,\n",
    "        \"sparsity_model\": sparsity_model\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def main():\n",
    "    # Hyperparameters\n",
    "    input_size = 20 * 20  # 20x20 pixels\n",
    "    hidden_size = 1024\n",
    "    num_classes = 10\n",
    "    num_hidden_layers = 2\n",
    "    batch_size = 64\n",
    "    learning_rate = 0.001\n",
    "    epochs = 2\n",
    "    \n",
    "    # Create empty DataFrame to store results\n",
    "    results_df = pd.DataFrame(columns=[\n",
    "        'iteration', \n",
    "        'accuracy', \n",
    "        'size_mb', \n",
    "        'inference_time_avg', \n",
    "        'inference_time_std',\n",
    "        'sparsity_pruned_parameters',\n",
    "        'sparsity_model'\n",
    "    ])\n",
    "    \n",
    "    # Create model\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = FFNN(input_size, hidden_size, num_classes, num_hidden_layers)\n",
    "    prune_params = [(m[1], \"weight\") for m in model.named_modules() if len(list(m[1].children()))==0 and not isinstance(m[1], nn.ReLU)]\n",
    "\n",
    "    torch.save(model.state_dict(), \"data/lab4/initial_weights.pth\")\n",
    "    \n",
    "    train_loader = create_dataloader('data/mnist_train.csv', batch_size, True)\n",
    "    test_loader = create_dataloader('data/mnist_test.csv', batch_size, False)\n",
    "    \n",
    "    model = model.to(device)\n",
    "    model = train_model(model, train_loader, test_loader, epochs, learning_rate, device)\n",
    "\n",
    "    # Store initial results (iteration 0)\n",
    "    accuracy, size, inference_time, inference_std = evaluate_model(model, test_loader)\n",
    "    sparsity_results = calculate_sparsity(model)\n",
    "    \n",
    "    results_df.loc[0] = {\n",
    "        'iteration': 0,\n",
    "        'accuracy': accuracy,\n",
    "        'size_mb': size/1e6,\n",
    "        'inference_time_avg': inference_time,\n",
    "        'inference_time_std': inference_std,\n",
    "        'sparsity_pruned_parameters': sparsity_results['sparsity_pruned_parameters'],\n",
    "        'sparsity_model': sparsity_results['sparsity_model']\n",
    "    }\n",
    "\n",
    "    print(f\"Iteration 0 - Accuracy: {accuracy:.2f}%, Model Size: {size/1e6:.2f} MB, \"\n",
    "          f\"Mean Inference Time: {inference_time:.4f}s ± {inference_std:.4f}s\")\n",
    "\n",
    "    for i in range(1, 11):\n",
    "        print(f\"Pruning iteration {i}\")\n",
    "        prune.global_unstructured(prune_params, pruning_method=prune.L1Unstructured, amount=0.33)\n",
    "        \n",
    "        accuracy, size, inference_time, inference_std = sparse_evaluate(model, test_loader, device)\n",
    "        sparsity_results = calculate_sparsity(model)\n",
    "        \n",
    "        # Store results in DataFrame\n",
    "        results_df.loc[i] = {\n",
    "            'iteration': i,\n",
    "            'accuracy': accuracy,\n",
    "            'size_mb': size/1e6,\n",
    "            'inference_time_avg': inference_time,\n",
    "            'inference_time_std': inference_std,\n",
    "            'sparsity_pruned_parameters': sparsity_results['sparsity_pruned_parameters'],\n",
    "            'sparsity_model': sparsity_results['sparsity_model']\n",
    "        }\n",
    "        \n",
    "        print(f\"Accuracy: {accuracy:.2f}%, Model Size: {size/1e6:.2f} MB, \"\n",
    "              f\"Mean Inference Time: {inference_time:.4f}s ± {inference_std:.4f}s\")\n",
    "        print(\"Sparsity per parameter:\")\n",
    "        for name, sparsity in sparsity_results[\"sparsity_per_parameter\"].items():\n",
    "            print(f\"  {name}: {sparsity:.2f}%\")\n",
    "        \n",
    "        print(f\"Sparsity across all pruned parameters: {sparsity_results['sparsity_pruned_parameters']:.2f}%\")\n",
    "        print(f\"Sparsity for the model overall: {sparsity_results['sparsity_model']:.2f}%\")\n",
    "    \n",
    "    # Save results to CSV\n",
    "    results_df.to_csv('pruning_results.csv', index=False)\n",
    "    \n",
    "    # Display the DataFrame\n",
    "    print(\"\\nFinal Results DataFrame:\")\n",
    "    print(results_df)\n",
    "    \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training normal precision model for 2 epochs\n",
      "Epoch 1, Accuracy: 96.95%\n",
      "Epoch 2, Accuracy: 97.42%\n",
      "Iteration 0 - Accuracy: 97.42%, Model Size: 29.40 MB, Mean Inference Time: 0.0013s ± 0.0006s\n",
      "Pruning iteration 1\n",
      "Accuracy: 97.40%, Model Size: 19.71 MB, Mean Inference Time: 0.0015s ± 0.0005s\n",
      "Sparsity per parameter:\n",
      "  layers.0.weight_mask: 26.41%\n",
      "  layers.1.weight_mask: 35.57%\n",
      "  layers.2.weight_mask: 33.42%\n",
      "Sparsity across all pruned parameters: 33.00%\n",
      "Sparsity for the model overall: 32.95%\n",
      "Pruning iteration 2\n",
      "Accuracy: 97.18%, Model Size: 13.21 MB, Mean Inference Time: 0.0008s ± 0.0004s\n",
      "Sparsity per parameter:\n",
      "  layers.0.weight_mask: 45.33%\n",
      "  layers.1.weight_mask: 58.89%\n",
      "  layers.2.weight_mask: 59.51%\n",
      "Sparsity across all pruned parameters: 55.11%\n",
      "Sparsity for the model overall: 55.03%\n",
      "Pruning iteration 3\n",
      "Accuracy: 96.19%, Model Size: 8.86 MB, Mean Inference Time: 0.0009s ± 0.0005s\n",
      "Sparsity per parameter:\n",
      "  layers.0.weight_mask: 59.63%\n",
      "  layers.1.weight_mask: 73.87%\n",
      "  layers.2.weight_mask: 77.53%\n",
      "Sparsity across all pruned parameters: 69.92%\n",
      "Sparsity for the model overall: 69.83%\n",
      "Pruning iteration 4\n",
      "Accuracy: 96.00%, Model Size: 5.95 MB, Mean Inference Time: 0.0011s ± 0.0001s\n",
      "Sparsity per parameter:\n",
      "  layers.0.weight_mask: 70.68%\n",
      "  layers.1.weight_mask: 83.36%\n",
      "  layers.2.weight_mask: 86.74%\n",
      "Sparsity across all pruned parameters: 79.85%\n",
      "Sparsity for the model overall: 79.74%\n",
      "Pruning iteration 5\n",
      "Accuracy: 94.82%, Model Size: 3.99 MB, Mean Inference Time: 0.0009s ± 0.0005s\n",
      "Sparsity per parameter:\n",
      "  layers.0.weight_mask: 78.98%\n",
      "  layers.1.weight_mask: 89.38%\n",
      "  layers.2.weight_mask: 91.89%\n",
      "Sparsity across all pruned parameters: 86.50%\n",
      "Sparsity for the model overall: 86.38%\n",
      "Pruning iteration 6\n",
      "Accuracy: 89.81%, Model Size: 2.69 MB, Mean Inference Time: 0.0008s ± 0.0004s\n",
      "Sparsity per parameter:\n",
      "  layers.0.weight_mask: 85.15%\n",
      "  layers.1.weight_mask: 93.18%\n",
      "  layers.2.weight_mask: 94.97%\n",
      "Sparsity across all pruned parameters: 90.95%\n",
      "Sparsity for the model overall: 90.83%\n",
      "Pruning iteration 7\n",
      "Accuracy: 69.82%, Model Size: 1.81 MB, Mean Inference Time: 0.0004s ± 0.0005s\n",
      "Sparsity per parameter:\n",
      "  layers.0.weight_mask: 89.58%\n",
      "  layers.1.weight_mask: 95.62%\n",
      "  layers.2.weight_mask: 96.77%\n",
      "Sparsity across all pruned parameters: 93.94%\n",
      "Sparsity for the model overall: 93.81%\n",
      "Pruning iteration 8\n",
      "Accuracy: 43.67%, Model Size: 1.22 MB, Mean Inference Time: 0.0007s ± 0.0006s\n",
      "Sparsity per parameter:\n",
      "  layers.0.weight_mask: 92.68%\n",
      "  layers.1.weight_mask: 97.19%\n",
      "  layers.2.weight_mask: 97.88%\n",
      "Sparsity across all pruned parameters: 95.94%\n",
      "Sparsity for the model overall: 95.81%\n",
      "Pruning iteration 9\n",
      "Accuracy: 27.96%, Model Size: 0.83 MB, Mean Inference Time: 0.0010s ± 0.0000s\n",
      "Sparsity per parameter:\n",
      "  layers.0.weight_mask: 94.90%\n",
      "  layers.1.weight_mask: 98.19%\n",
      "  layers.2.weight_mask: 98.61%\n",
      "Sparsity across all pruned parameters: 97.28%\n",
      "Sparsity for the model overall: 97.14%\n",
      "Pruning iteration 10\n",
      "Accuracy: 17.63%, Model Size: 0.56 MB, Mean Inference Time: 0.0006s ± 0.0005s\n",
      "Sparsity per parameter:\n",
      "  layers.0.weight_mask: 96.43%\n",
      "  layers.1.weight_mask: 98.85%\n",
      "  layers.2.weight_mask: 99.13%\n",
      "Sparsity across all pruned parameters: 98.18%\n",
      "Sparsity for the model overall: 98.04%\n",
      "\n",
      "Final Results DataFrame:\n",
      "   iteration   accuracy    size_mb  inference_time_avg  inference_time_std  \\\n",
      "0          0  97.419742  29.396738            0.001303            0.000604   \n",
      "1          1  97.399740  19.705410            0.001507            0.000450   \n",
      "2          2  97.179718  13.212034            0.000802            0.000401   \n",
      "3          3  96.189619   8.861506            0.000901            0.000491   \n",
      "4          4  95.999600   5.946626            0.001054            0.000106   \n",
      "5          5  94.819482   3.993538            0.000901            0.000491   \n",
      "6          6  89.808981   2.685122            0.000801            0.000400   \n",
      "7          7  69.816982   1.808450            0.000400            0.000490   \n",
      "8          8  43.674367   1.221122            0.000700            0.000601   \n",
      "9          9  27.962796   0.827522            0.001001            0.000002   \n",
      "10        10  17.631763   0.563970            0.000600            0.000490   \n",
      "\n",
      "   sparsity_pruned_parameters  sparsity_model  \n",
      "0                           0        0.000000  \n",
      "1                   32.999981       32.953796  \n",
      "2                   55.109996       55.032867  \n",
      "3                   69.923714       69.825852  \n",
      "4                   79.848898       79.737146  \n",
      "5                   86.498785       86.377726  \n",
      "6                   90.954198       90.826903  \n",
      "7                    93.93932       93.807847  \n",
      "8                   95.939366       95.805094  \n",
      "9                   97.279381       97.143234  \n",
      "10                   98.17722       98.039816  \n"
     ]
    }
   ],
   "source": [
    "df = main()\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('pruning_results.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p3-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
