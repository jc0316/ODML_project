{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.utils.prune as prune\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform=None):\n",
    "        data = pd.read_csv(csv_file)\n",
    "        self.labels = data.iloc[:, 0].values\n",
    "        self.pixels = data.iloc[:, 1:].values.astype('float32')\n",
    "        self.pixels = self.pixels.reshape(-1, 28, 28)  # Reshape to 28x28 images\n",
    "\n",
    "        # Normalize the pixel values\n",
    "        self.pixels_mean = self.pixels.mean()\n",
    "        self.pixels_std = self.pixels.std()\n",
    "        self.pixels = (self.pixels - self.pixels_mean) / self.pixels_std\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.pixels[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(torch.tensor(image).unsqueeze(0))\n",
    "\n",
    "        return image.squeeze(0), torch.tensor(label)\n",
    "    \n",
    "\n",
    "class FFNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes, num_hidden_layers):\n",
    "        super(FFNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_classes = num_classes\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(nn.Linear(input_size, hidden_size))\n",
    "        for _ in range(num_hidden_layers - 1):\n",
    "            self.layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "        self.layers.append(nn.Linear(hidden_size, num_classes))\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten the input\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = self.relu(layer(x))\n",
    "        return self.layers[-1](x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader(dataset_path, batch_size, is_train=True):\n",
    "    # Create center crop transform\n",
    "    transform = transforms.Compose([\n",
    "        transforms.CenterCrop(20)  # Crop to 20x20 as specified\n",
    "    ])\n",
    "    \n",
    "    # Create dataset and dataloader\n",
    "    dataset = MNISTDataset(dataset_path, transform=transform)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=is_train)\n",
    "\n",
    "def print_size_of_model(model, label=\"\"):\n",
    "    sd = model.state_dict()\n",
    "    for item in sd:\n",
    "        sd[item] = model.state_dict()[item].to_sparse()\n",
    "    \n",
    "    torch.save(sd, \"temp.pt\")\n",
    "    size=os.path.getsize(\"temp.pt\")\n",
    "    #print(\"model: \",label,' \\t','Size (MB):', size/1e6)\n",
    "    os.remove('temp.pt')\n",
    "    return size\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, epochs, learning_rate, device):\n",
    "    print(f\"Training normal precision model for {epochs} epochs\")\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for data, target in val_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                outputs = model(data)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += target.size(0)\n",
    "                correct += (predicted == target).sum().item()\n",
    "        \n",
    "        print(f'Epoch {epoch+1}, Accuracy: {100 * correct / total:.2f}%')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_inference_time(model, test_loader, batch_size=-1, num_runs=5):\n",
    "    model.eval()\n",
    "    times = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_runs):\n",
    "            data, _ = next(iter(test_loader))\n",
    "            if batch_size == 1:\n",
    "                data = data[0:1]\n",
    "                \n",
    "            start_time = time.time()\n",
    "            _ = model(data)\n",
    "            end_time = time.time()\n",
    "            times.append(end_time - start_time)\n",
    "    \n",
    "    mean_time = np.mean(times)\n",
    "    std_time = np.std(times)\n",
    "    return mean_time, std_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            outputs = model(data)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "\n",
    "    size = print_size_of_model(model, \"sparse\")\n",
    "    inference_time, inference_std = measure_inference_time(model, test_loader)\n",
    "    \n",
    "    return accuracy, size, inference_time, inference_std\n",
    "\n",
    "def sparse_evaluate(model, dataloader, device):\n",
    "    model.to(device)\n",
    "    model_copy = FFNN(\n",
    "        model.input_size,\n",
    "        model.hidden_size,\n",
    "        model.num_classes,\n",
    "        model.num_hidden_layers\n",
    "    )\n",
    "    prune_params = [(m[1], \"weight\") for m in model_copy.named_modules() if len(list(m[1].children()))==0 and not isinstance(m[1], nn.ReLU)]\n",
    "    for p in prune_params:\n",
    "        prune.identity(p[0], \"weight\")\n",
    "    # Copy the parameters\n",
    "    model_copy.load_state_dict(model.state_dict())\n",
    "    \n",
    "    copy_params = [(m[1], \"weight\") for m in model_copy.named_modules() if len(list(m[1].children()))==0 and not isinstance(m[1], nn.ReLU)]\n",
    "    # (we assume the same model architecture as the MNIST or SST-2 architecture we specify above)\n",
    "    for p in copy_params:\n",
    "        prune.remove(*p)\n",
    "    \n",
    "    return evaluate_model(model_copy, dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sparsity(model, print_results=False):\n",
    "    \"\"\"\n",
    "    Calculate the sparsity level (using the percent of elements that are 0) for:\n",
    "    - each parameter,\n",
    "    - all pruned parameters overall, and\n",
    "    - the model overall.\n",
    "    \n",
    "    Report each of these values: \n",
    "    - the sparsity level of each parameter, \n",
    "    - across all pruned parameters, and \n",
    "    - for the model overall. \n",
    "    \"\"\"\n",
    "    sparsity_per_parameter = {}\n",
    "    total_zero_count_pruned = 0\n",
    "    total_element_count_pruned = 0\n",
    "    total_zero_count_model = 0\n",
    "    total_element_count_model = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "    # Iterate over all buffers in the model\n",
    "    for name, buffer in model.named_buffers():\n",
    "        # Calculate the number of zero elements and total elements in the buffer\n",
    "        zero_count = (buffer == 0).sum().item()\n",
    "        total_elements = buffer.numel()\n",
    "        \n",
    "        # Calculate the sparsity level for this parameter\n",
    "        sparsity_per_parameter[name] = zero_count / total_elements * 100\n",
    "\n",
    "        # Check if this is a pruned parameter by looking for \"weight_mask\" or \"bias_mask\" in the name\n",
    "        if \"weight_mask\" in name or \"bias_mask\" in name:\n",
    "            total_zero_count_pruned += zero_count\n",
    "            total_element_count_pruned += total_elements\n",
    "\n",
    "        # Accumulate for overall model sparsity\n",
    "        total_zero_count_model += zero_count\n",
    "\n",
    "    # Calculate overall sparsity for pruned parameters and the entire model\n",
    "    sparsity_pruned_parameters = (total_zero_count_pruned / total_element_count_pruned * 100\n",
    "                                  if total_element_count_pruned > 0 else 0)\n",
    "    sparsity_model = total_zero_count_model / total_element_count_model * 100\n",
    "\n",
    "    # Print or return the results\n",
    "    if print_results:\n",
    "        print(\"Sparsity per parameter:\")\n",
    "        for name, sparsity in sparsity_per_parameter.items():\n",
    "            print(f\"  {name}: {sparsity:.2f}%\")\n",
    "        \n",
    "        print(f\"Sparsity across all pruned parameters: {sparsity_pruned_parameters:.2f}%\")\n",
    "        print(f\"Sparsity for the model overall: {sparsity_model:.2f}%\")\n",
    "\n",
    "    # Optionally, return the values for further use\n",
    "    return {\n",
    "        \"sparsity_per_parameter\": sparsity_per_parameter,\n",
    "        \"sparsity_pruned_parameters\": sparsity_pruned_parameters,\n",
    "        \"sparsity_model\": sparsity_model\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeated_pruning():\n",
    "    # Hyperparameters\n",
    "    input_size = 20 * 20  # 20x20 pixels\n",
    "    hidden_size = 1024\n",
    "    num_classes = 10\n",
    "    num_hidden_layers = 2\n",
    "    batch_size = 64\n",
    "    learning_rate = 0.001\n",
    "    epochs = 2\n",
    "    \n",
    "    # Create empty DataFrame to store results\n",
    "    results_df = pd.DataFrame(columns=[\n",
    "        'iteration', \n",
    "        'accuracy', \n",
    "        'size_mb', \n",
    "        'inference_time_avg', \n",
    "        'inference_time_std',\n",
    "        'sparsity_pruned_parameters',\n",
    "        'sparsity_model'\n",
    "    ])\n",
    "    \n",
    "    # Create model\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = FFNN(input_size, hidden_size, num_classes, num_hidden_layers)\n",
    "    prune_params = [(m[1], \"weight\") for m in model.named_modules() if len(list(m[1].children()))==0 and not isinstance(m[1], nn.ReLU)]\n",
    "\n",
    "    torch.save(model.state_dict(), \"data/lab4/initial_weights.pth\")\n",
    "    \n",
    "    train_loader = create_dataloader('data/mnist_train.csv', batch_size, True)\n",
    "    test_loader = create_dataloader('data/mnist_test.csv', batch_size, False)\n",
    "    \n",
    "    model = model.to(device)\n",
    "    model = train_model(model, train_loader, test_loader, epochs, learning_rate, device)\n",
    "\n",
    "    # Store initial results (iteration 0)\n",
    "    accuracy, size, inference_time, inference_std = evaluate_model(model, test_loader)\n",
    "    sparsity_results = calculate_sparsity(model)\n",
    "    \n",
    "    results_df.loc[0] = {\n",
    "        'iteration': 0,\n",
    "        'accuracy': accuracy,\n",
    "        'size_mb': size/1e6,\n",
    "        'inference_time_avg': inference_time,\n",
    "        'inference_time_std': inference_std,\n",
    "        'sparsity_pruned_parameters': sparsity_results['sparsity_pruned_parameters'],\n",
    "        'sparsity_model': sparsity_results['sparsity_model']\n",
    "    }\n",
    "\n",
    "    print(f\"Iteration 0 - Accuracy: {accuracy:.2f}%, Model Size: {size/1e6:.2f} MB, \"\n",
    "          f\"Mean Inference Time: {inference_time:.4f}s ± {inference_std:.4f}s\")\n",
    "\n",
    "    for i in range(1, 11):\n",
    "        print(f\"Pruning iteration {i}\")\n",
    "        prune.global_unstructured(prune_params, pruning_method=prune.L1Unstructured, amount=0.33)\n",
    "        \n",
    "        accuracy, size, inference_time, inference_std = sparse_evaluate(model, test_loader, device)\n",
    "        sparsity_results = calculate_sparsity(model)\n",
    "        \n",
    "        # Store results in DataFrame\n",
    "        results_df.loc[i] = {\n",
    "            'iteration': i,\n",
    "            'accuracy': accuracy,\n",
    "            'size_mb': size/1e6,\n",
    "            'inference_time_avg': inference_time,\n",
    "            'inference_time_std': inference_std,\n",
    "            'sparsity_pruned_parameters': sparsity_results['sparsity_pruned_parameters'],\n",
    "            'sparsity_model': sparsity_results['sparsity_model']\n",
    "        }\n",
    "        \n",
    "        print(f\"Accuracy: {accuracy:.2f}%, Model Size: {size/1e6:.2f} MB, \"\n",
    "              f\"Mean Inference Time: {inference_time:.4f}s ± {inference_std:.4f}s\")\n",
    "        print(\"Sparsity per parameter:\")\n",
    "        for name, sparsity in sparsity_results[\"sparsity_per_parameter\"].items():\n",
    "            print(f\"  {name}: {sparsity:.2f}%\")\n",
    "        \n",
    "        print(f\"Sparsity across all pruned parameters: {sparsity_results['sparsity_pruned_parameters']:.2f}%\")\n",
    "        print(f\"Sparsity for the model overall: {sparsity_results['sparsity_model']:.2f}%\")\n",
    "    \n",
    "    \n",
    "    # Display the DataFrame\n",
    "    print(\"\\nFinal Results DataFrame:\")\n",
    "    print(results_df)\n",
    "    \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterative_pruning():\n",
    "    # Hyperparameters\n",
    "    input_size = 20 * 20  # 20x20 pixels\n",
    "    hidden_size = 1024\n",
    "    num_classes = 10\n",
    "    num_hidden_layers = 2\n",
    "    batch_size = 64\n",
    "    learning_rate = 0.001\n",
    "    epochs = 2\n",
    "    \n",
    "    # Create empty DataFrame to store results\n",
    "    results_df = pd.DataFrame(columns=[\n",
    "        'iteration', \n",
    "        'accuracy', \n",
    "        'size_mb', \n",
    "        'inference_time_avg', \n",
    "        'inference_time_std',\n",
    "        'sparsity_pruned_parameters',\n",
    "        'sparsity_model'\n",
    "    ])\n",
    "    \n",
    "    # Create model\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = FFNN(input_size, hidden_size, num_classes, num_hidden_layers)\n",
    "    prune_params = [(m[1], \"weight\") for m in model.named_modules() if len(list(m[1].children()))==0 and not isinstance(m[1], nn.ReLU)]\n",
    "    prune_param_list = [m[0] for m in model.named_parameters() if \"weight\" in m[0]]\n",
    "\n",
    "    initial_model_weight_path = \"data/lab4/initial_weights.pth\"\n",
    "    torch.save(model.state_dict(), initial_model_weight_path)\n",
    "    init_weights = torch.load(initial_model_weight_path)\n",
    "    \n",
    "    train_loader = create_dataloader('data/mnist_train.csv', batch_size, True)\n",
    "    test_loader = create_dataloader('data/mnist_test.csv', batch_size, False)\n",
    "    \n",
    "    model = model.to(device)\n",
    "    model = train_model(model, train_loader, test_loader, epochs, learning_rate, device)\n",
    "\n",
    "    # Store initial results (iteration 0)\n",
    "    accuracy, size, inference_time, inference_std = evaluate_model(model, test_loader)\n",
    "    sparsity_results = calculate_sparsity(model)\n",
    "    \n",
    "    results_df.loc[0] = {\n",
    "        'iteration': 0,\n",
    "        'accuracy': accuracy,\n",
    "        'size_mb': size/1e6,\n",
    "        'inference_time_avg': inference_time,\n",
    "        'inference_time_std': inference_std,\n",
    "        'sparsity_pruned_parameters': sparsity_results['sparsity_pruned_parameters'],\n",
    "        'sparsity_model': sparsity_results['sparsity_model']\n",
    "    }\n",
    "\n",
    "    print(f\"Iteration 0 - Accuracy: {accuracy:.2f}%, Model Size: {size/1e6:.2f} MB, \"\n",
    "          f\"Mean Inference Time: {inference_time:.4f}s ± {inference_std:.4f}s\")\n",
    "\n",
    "    for i in range(1, 11):\n",
    "        print(f\"Pruning iteration {i}\")\n",
    "        prune.global_unstructured(prune_params, pruning_method=prune.L1Unstructured, amount=0.33)\n",
    "        init_updated = {k + (\"_orig\" if k in prune_param_list else \"\"):v for k,v in init_weights.items()}\n",
    "        ffn_mnist_copy = copy.deepcopy(model.state_dict())\n",
    "        ffn_mnist_copy.update(init_updated)\n",
    "        model.load_state_dict(ffn_mnist_copy)\n",
    "        model = model.to(device)\n",
    "        model = train_model(model, train_loader, test_loader, epochs, learning_rate, device)\n",
    "        \n",
    "        accuracy, size, inference_time, inference_std = sparse_evaluate(model, test_loader, device)\n",
    "        sparsity_results = calculate_sparsity(model)\n",
    "        \n",
    "        # Store results in DataFrame\n",
    "        results_df.loc[i] = {\n",
    "            'iteration': i,\n",
    "            'accuracy': accuracy,\n",
    "            'size_mb': size/1e6,\n",
    "            'inference_time_avg': inference_time,\n",
    "            'inference_time_std': inference_std,\n",
    "            'sparsity_pruned_parameters': sparsity_results['sparsity_pruned_parameters'],\n",
    "            'sparsity_model': sparsity_results['sparsity_model']\n",
    "        }\n",
    "        \n",
    "        print(f\"Accuracy: {accuracy:.2f}%, Model Size: {size/1e6:.2f} MB, \"\n",
    "              f\"Mean Inference Time: {inference_time:.4f}s ± {inference_std:.4f}s\")\n",
    "        print(\"Sparsity per parameter:\")\n",
    "        for name, sparsity in sparsity_results[\"sparsity_per_parameter\"].items():\n",
    "            print(f\"  {name}: {sparsity:.2f}%\")\n",
    "        \n",
    "        print(f\"Sparsity across all pruned parameters: {sparsity_results['sparsity_pruned_parameters']:.2f}%\")\n",
    "        print(f\"Sparsity for the model overall: {sparsity_results['sparsity_model']:.2f}%\")\n",
    "    \n",
    "    \n",
    "    # Display the DataFrame\n",
    "    print(\"\\nFinal Results DataFrame:\")\n",
    "    print(results_df)\n",
    "    \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training normal precision model for 2 epochs\n",
      "Epoch 1, Accuracy: 96.53%\n",
      "Epoch 2, Accuracy: 96.96%\n",
      "Iteration 0 - Accuracy: 96.96%, Model Size: 29.40 MB, Mean Inference Time: 0.0007s ± 0.0006s\n",
      "Pruning iteration 1\n",
      "Accuracy: 97.01%, Model Size: 19.71 MB, Mean Inference Time: 0.0007s ± 0.0004s\n",
      "Sparsity per parameter:\n",
      "  layers.0.weight_mask: 26.33%\n",
      "  layers.1.weight_mask: 35.59%\n",
      "  layers.2.weight_mask: 34.81%\n",
      "Sparsity across all pruned parameters: 33.00%\n",
      "Sparsity for the model overall: 32.95%\n",
      "Pruning iteration 2\n",
      "Accuracy: 96.71%, Model Size: 13.21 MB, Mean Inference Time: 0.0006s ± 0.0005s\n",
      "Sparsity per parameter:\n",
      "  layers.0.weight_mask: 45.12%\n",
      "  layers.1.weight_mask: 58.96%\n",
      "  layers.2.weight_mask: 60.22%\n",
      "Sparsity across all pruned parameters: 55.11%\n",
      "Sparsity for the model overall: 55.03%\n",
      "Pruning iteration 3\n",
      "Accuracy: 95.60%, Model Size: 8.86 MB, Mean Inference Time: 0.0010s ± 0.0000s\n",
      "Sparsity per parameter:\n",
      "  layers.0.weight_mask: 59.33%\n",
      "  layers.1.weight_mask: 73.99%\n",
      "  layers.2.weight_mask: 77.32%\n",
      "Sparsity across all pruned parameters: 69.92%\n",
      "Sparsity for the model overall: 69.83%\n",
      "Pruning iteration 4\n",
      "Accuracy: 94.87%, Model Size: 5.95 MB, Mean Inference Time: 0.0009s ± 0.0005s\n",
      "Sparsity per parameter:\n",
      "  layers.0.weight_mask: 70.42%\n",
      "  layers.1.weight_mask: 83.46%\n",
      "  layers.2.weight_mask: 87.03%\n",
      "Sparsity across all pruned parameters: 79.85%\n",
      "Sparsity for the model overall: 79.74%\n",
      "Pruning iteration 5\n",
      "Accuracy: 91.72%, Model Size: 3.99 MB, Mean Inference Time: 0.0008s ± 0.0004s\n",
      "Sparsity per parameter:\n",
      "  layers.0.weight_mask: 78.79%\n",
      "  layers.1.weight_mask: 89.45%\n",
      "  layers.2.weight_mask: 92.18%\n",
      "Sparsity across all pruned parameters: 86.50%\n",
      "Sparsity for the model overall: 86.38%\n",
      "Pruning iteration 6\n",
      "Accuracy: 84.38%, Model Size: 2.69 MB, Mean Inference Time: 0.0004s ± 0.0005s\n",
      "Sparsity per parameter:\n",
      "  layers.0.weight_mask: 84.99%\n",
      "  layers.1.weight_mask: 93.24%\n",
      "  layers.2.weight_mask: 95.09%\n",
      "Sparsity across all pruned parameters: 90.95%\n",
      "Sparsity for the model overall: 90.83%\n",
      "Pruning iteration 7\n",
      "Accuracy: 61.49%, Model Size: 1.81 MB, Mean Inference Time: 0.0008s ± 0.0004s\n",
      "Sparsity per parameter:\n",
      "  layers.0.weight_mask: 89.49%\n",
      "  layers.1.weight_mask: 95.65%\n",
      "  layers.2.weight_mask: 96.94%\n",
      "Sparsity across all pruned parameters: 93.94%\n",
      "Sparsity for the model overall: 93.81%\n",
      "Pruning iteration 8\n",
      "Accuracy: 29.38%, Model Size: 1.22 MB, Mean Inference Time: 0.0004s ± 0.0005s\n",
      "Sparsity per parameter:\n",
      "  layers.0.weight_mask: 92.65%\n",
      "  layers.1.weight_mask: 97.20%\n",
      "  layers.2.weight_mask: 98.15%\n",
      "Sparsity across all pruned parameters: 95.94%\n",
      "Sparsity for the model overall: 95.81%\n",
      "Pruning iteration 9\n",
      "Accuracy: 17.14%, Model Size: 0.83 MB, Mean Inference Time: 0.0006s ± 0.0005s\n",
      "Sparsity per parameter:\n",
      "  layers.0.weight_mask: 94.89%\n",
      "  layers.1.weight_mask: 98.20%\n",
      "  layers.2.weight_mask: 98.78%\n",
      "Sparsity across all pruned parameters: 97.28%\n",
      "Sparsity for the model overall: 97.14%\n",
      "Pruning iteration 10\n",
      "Accuracy: 12.35%, Model Size: 0.56 MB, Mean Inference Time: 0.0006s ± 0.0005s\n",
      "Sparsity per parameter:\n",
      "  layers.0.weight_mask: 96.42%\n",
      "  layers.1.weight_mask: 98.85%\n",
      "  layers.2.weight_mask: 99.26%\n",
      "Sparsity across all pruned parameters: 98.18%\n",
      "Sparsity for the model overall: 98.04%\n",
      "\n",
      "Final Results DataFrame:\n",
      "   iteration   accuracy    size_mb  inference_time_avg  inference_time_std  \\\n",
      "0          0  96.959696  29.396738            0.000702            0.000602   \n",
      "1          1  97.009701  19.705282            0.000671            0.000419   \n",
      "2          2  96.709671  13.212098            0.000600            0.000490   \n",
      "3          3  95.599560   8.861506            0.001002            0.000002   \n",
      "4          4  94.869487   5.946562            0.000902            0.000491   \n",
      "5          5  91.719172   3.993730            0.000815            0.000409   \n",
      "6          6  84.378438   2.685122            0.000400            0.000490   \n",
      "7          7  61.486149   1.808514            0.000801            0.000401   \n",
      "8          8  29.382938   1.221122            0.000400            0.000490   \n",
      "9          9  17.141714   0.827586            0.000601            0.000490   \n",
      "10        10  12.351235   0.563842            0.000600            0.000490   \n",
      "\n",
      "   sparsity_pruned_parameters  sparsity_model  \n",
      "0                           0        0.000000  \n",
      "1                   32.999981       32.953796  \n",
      "2                   55.109996       55.032867  \n",
      "3                   69.923714       69.825852  \n",
      "4                   79.848898       79.737146  \n",
      "5                   86.498785       86.377726  \n",
      "6                   90.954198       90.826903  \n",
      "7                    93.93932       93.807847  \n",
      "8                   95.939366       95.805094  \n",
      "9                   97.279381       97.143234  \n",
      "10                   98.17722       98.039816  \n"
     ]
    }
   ],
   "source": [
    "df = repeated_pruning()\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('pruning_results_repeated.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Johnny Chen\\AppData\\Local\\Temp\\ipykernel_15788\\3265794120.py:30: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  init_weights = torch.load(initial_model_weight_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training normal precision model for 2 epochs\n",
      "Epoch 1, Accuracy: 96.84%\n",
      "Epoch 2, Accuracy: 97.68%\n",
      "Iteration 0 - Accuracy: 97.68%, Model Size: 29.40 MB, Mean Inference Time: 0.0006s ± 0.0005s\n",
      "Pruning iteration 1\n",
      "init_updated:  {'layers.0.weight_orig': tensor([[ 0.0457, -0.0249,  0.0128,  ...,  0.0147,  0.0492,  0.0053],\n",
      "        [-0.0101,  0.0405,  0.0396,  ...,  0.0192,  0.0305, -0.0367],\n",
      "        [ 0.0100, -0.0402, -0.0475,  ..., -0.0210, -0.0397, -0.0177],\n",
      "        ...,\n",
      "        [-0.0221,  0.0138, -0.0103,  ...,  0.0069, -0.0313,  0.0489],\n",
      "        [ 0.0125, -0.0151, -0.0174,  ..., -0.0367, -0.0056,  0.0487],\n",
      "        [ 0.0420, -0.0300,  0.0429,  ..., -0.0440, -0.0477,  0.0327]]), 'layers.0.bias': tensor([ 0.0207,  0.0246, -0.0463,  ...,  0.0493, -0.0022,  0.0134]), 'layers.1.weight_orig': tensor([[ 0.0166, -0.0082,  0.0294,  ...,  0.0002,  0.0290,  0.0217],\n",
      "        [ 0.0156, -0.0085, -0.0027,  ..., -0.0134, -0.0092,  0.0150],\n",
      "        [ 0.0155, -0.0105,  0.0253,  ...,  0.0154, -0.0124, -0.0047],\n",
      "        ...,\n",
      "        [-0.0007,  0.0020, -0.0059,  ...,  0.0310, -0.0267, -0.0075],\n",
      "        [-0.0166, -0.0059, -0.0279,  ..., -0.0235,  0.0150, -0.0084],\n",
      "        [ 0.0121, -0.0234,  0.0307,  ...,  0.0134, -0.0199, -0.0189]]), 'layers.1.bias': tensor([-0.0220,  0.0133,  0.0240,  ...,  0.0289, -0.0138,  0.0121]), 'layers.2.weight_orig': tensor([[ 0.0135,  0.0259, -0.0073,  ...,  0.0033, -0.0057, -0.0195],\n",
      "        [ 0.0272, -0.0092,  0.0116,  ...,  0.0005, -0.0218, -0.0232],\n",
      "        [-0.0092, -0.0215, -0.0213,  ..., -0.0295,  0.0259,  0.0296],\n",
      "        ...,\n",
      "        [-0.0014,  0.0176, -0.0108,  ...,  0.0204, -0.0239,  0.0016],\n",
      "        [-0.0155, -0.0185, -0.0183,  ...,  0.0303,  0.0045,  0.0269],\n",
      "        [ 0.0291, -0.0093,  0.0207,  ..., -0.0213,  0.0127, -0.0123]]), 'layers.2.bias': tensor([-0.0204, -0.0054, -0.0001, -0.0088,  0.0177,  0.0100,  0.0190,  0.0261,\n",
      "         0.0121, -0.0078])}\n",
      "ffn_mnist_copy:  OrderedDict([('layers.0.bias', tensor([-0.0128, -0.0261, -0.0834,  ...,  0.0277, -0.0474, -0.0328])), ('layers.0.weight_orig', tensor([[ 0.0796,  0.0146,  0.0521,  ...,  0.0340,  0.0712,  0.0380],\n",
      "        [ 0.0375,  0.0774,  0.0741,  ...,  0.0221,  0.0477, -0.0069],\n",
      "        [ 0.0477, -0.0041, -0.0156,  ..., -0.0074, -0.0056,  0.0301],\n",
      "        ...,\n",
      "        [ 0.0002,  0.0387,  0.0087,  ..., -0.0026, -0.0424,  0.0428],\n",
      "        [ 0.0588,  0.0337,  0.0355,  ..., -0.0233,  0.0274,  0.0629],\n",
      "        [ 0.0872,  0.0052,  0.0647,  ..., -0.0237, -0.0106,  0.0950]])), ('layers.0.weight_mask', tensor([[1., 0., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 1.],\n",
      "        ...,\n",
      "        [0., 1., 0.,  ..., 0., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 0., 1.,  ..., 1., 0., 1.]])), ('layers.1.bias', tensor([-0.0305, -0.0532,  0.0207,  ...,  0.0582, -0.0183,  0.0737])), ('layers.1.weight_orig', tensor([[ 0.0115,  0.0085,  0.0351,  ..., -0.0048,  0.0668,  0.0310],\n",
      "        [ 0.0043, -0.0215, -0.0251,  ..., -0.0508, -0.0441,  0.0269],\n",
      "        [-0.0131,  0.0015,  0.0494,  ...,  0.0545,  0.0396, -0.0049],\n",
      "        ...,\n",
      "        [ 0.0469,  0.0093,  0.0103,  ...,  0.0259, -0.0209, -0.0497],\n",
      "        [-0.0118, -0.0081, -0.0487,  ..., -0.0347, -0.0021, -0.0249],\n",
      "        [-0.0293, -0.0116, -0.0081,  ...,  0.0178, -0.0222, -0.0849]])), ('layers.1.weight_mask', tensor([[0., 0., 1.,  ..., 0., 1., 1.],\n",
      "        [0., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [0., 0., 1.,  ..., 1., 1., 0.],\n",
      "        ...,\n",
      "        [1., 0., 0.,  ..., 1., 1., 1.],\n",
      "        [0., 0., 1.,  ..., 1., 0., 1.],\n",
      "        [1., 0., 0.,  ..., 1., 1., 1.]])), ('layers.2.bias', tensor([-0.0226, -0.0324, -0.0178, -0.0199,  0.0067, -0.0105,  0.0103, -0.0117,\n",
      "         0.1102,  0.0108])), ('layers.2.weight_orig', tensor([[ 0.0417,  0.0336, -0.0145,  ..., -0.0301, -0.0169, -0.0214],\n",
      "        [ 0.0478, -0.0057,  0.0090,  ...,  0.0356, -0.0147, -0.0246],\n",
      "        [-0.0530, -0.0124, -0.0747,  ..., -0.0446,  0.0160,  0.0395],\n",
      "        ...,\n",
      "        [-0.0076,  0.0180, -0.0358,  ...,  0.0211, -0.0503, -0.0072],\n",
      "        [-0.0129, -0.0410, -0.0415,  ...,  0.0426,  0.0148,  0.0366],\n",
      "        [ 0.0451, -0.0416,  0.0369,  ..., -0.0201,  0.0016, -0.0637]])), ('layers.2.weight_mask', tensor([[1., 1., 0.,  ..., 1., 1., 1.],\n",
      "        [1., 0., 0.,  ..., 1., 0., 1.],\n",
      "        [1., 0., 1.,  ..., 1., 1., 1.],\n",
      "        ...,\n",
      "        [0., 1., 1.,  ..., 1., 1., 0.],\n",
      "        [0., 1., 1.,  ..., 1., 0., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 0., 1.]]))])\n",
      "ffn_mnist_copy_updated:  OrderedDict([('layers.0.bias', tensor([ 0.0207,  0.0246, -0.0463,  ...,  0.0493, -0.0022,  0.0134])), ('layers.0.weight_orig', tensor([[ 0.0457, -0.0249,  0.0128,  ...,  0.0147,  0.0492,  0.0053],\n",
      "        [-0.0101,  0.0405,  0.0396,  ...,  0.0192,  0.0305, -0.0367],\n",
      "        [ 0.0100, -0.0402, -0.0475,  ..., -0.0210, -0.0397, -0.0177],\n",
      "        ...,\n",
      "        [-0.0221,  0.0138, -0.0103,  ...,  0.0069, -0.0313,  0.0489],\n",
      "        [ 0.0125, -0.0151, -0.0174,  ..., -0.0367, -0.0056,  0.0487],\n",
      "        [ 0.0420, -0.0300,  0.0429,  ..., -0.0440, -0.0477,  0.0327]])), ('layers.0.weight_mask', tensor([[1., 0., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 1.],\n",
      "        ...,\n",
      "        [0., 1., 0.,  ..., 0., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 0., 1.,  ..., 1., 0., 1.]])), ('layers.1.bias', tensor([-0.0220,  0.0133,  0.0240,  ...,  0.0289, -0.0138,  0.0121])), ('layers.1.weight_orig', tensor([[ 0.0166, -0.0082,  0.0294,  ...,  0.0002,  0.0290,  0.0217],\n",
      "        [ 0.0156, -0.0085, -0.0027,  ..., -0.0134, -0.0092,  0.0150],\n",
      "        [ 0.0155, -0.0105,  0.0253,  ...,  0.0154, -0.0124, -0.0047],\n",
      "        ...,\n",
      "        [-0.0007,  0.0020, -0.0059,  ...,  0.0310, -0.0267, -0.0075],\n",
      "        [-0.0166, -0.0059, -0.0279,  ..., -0.0235,  0.0150, -0.0084],\n",
      "        [ 0.0121, -0.0234,  0.0307,  ...,  0.0134, -0.0199, -0.0189]])), ('layers.1.weight_mask', tensor([[0., 0., 1.,  ..., 0., 1., 1.],\n",
      "        [0., 1., 1.,  ..., 1., 1., 1.],\n",
      "        [0., 0., 1.,  ..., 1., 1., 0.],\n",
      "        ...,\n",
      "        [1., 0., 0.,  ..., 1., 1., 1.],\n",
      "        [0., 0., 1.,  ..., 1., 0., 1.],\n",
      "        [1., 0., 0.,  ..., 1., 1., 1.]])), ('layers.2.bias', tensor([-0.0204, -0.0054, -0.0001, -0.0088,  0.0177,  0.0100,  0.0190,  0.0261,\n",
      "         0.0121, -0.0078])), ('layers.2.weight_orig', tensor([[ 0.0135,  0.0259, -0.0073,  ...,  0.0033, -0.0057, -0.0195],\n",
      "        [ 0.0272, -0.0092,  0.0116,  ...,  0.0005, -0.0218, -0.0232],\n",
      "        [-0.0092, -0.0215, -0.0213,  ..., -0.0295,  0.0259,  0.0296],\n",
      "        ...,\n",
      "        [-0.0014,  0.0176, -0.0108,  ...,  0.0204, -0.0239,  0.0016],\n",
      "        [-0.0155, -0.0185, -0.0183,  ...,  0.0303,  0.0045,  0.0269],\n",
      "        [ 0.0291, -0.0093,  0.0207,  ..., -0.0213,  0.0127, -0.0123]])), ('layers.2.weight_mask', tensor([[1., 1., 0.,  ..., 1., 1., 1.],\n",
      "        [1., 0., 0.,  ..., 1., 0., 1.],\n",
      "        [1., 0., 1.,  ..., 1., 1., 1.],\n",
      "        ...,\n",
      "        [0., 1., 1.,  ..., 1., 1., 0.],\n",
      "        [0., 1., 1.,  ..., 1., 0., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 0., 1.]]))])\n",
      "Training normal precision model for 2 epochs\n",
      "Epoch 1, Accuracy: 97.29%\n",
      "Epoch 2, Accuracy: 97.62%\n",
      "Accuracy: 97.62%, Model Size: 19.71 MB, Mean Inference Time: 0.0014s ± 0.0004s\n",
      "Sparsity per parameter:\n",
      "  layers.0.weight_mask: 26.33%\n",
      "  layers.1.weight_mask: 35.60%\n",
      "  layers.2.weight_mask: 33.75%\n",
      "Sparsity across all pruned parameters: 33.00%\n",
      "Sparsity for the model overall: 32.95%\n",
      "Pruning iteration 2\n",
      "init_updated:  {'layers.0.weight_orig': tensor([[ 0.0457, -0.0249,  0.0128,  ...,  0.0147,  0.0492,  0.0053],\n",
      "        [-0.0101,  0.0405,  0.0396,  ...,  0.0192,  0.0305, -0.0367],\n",
      "        [ 0.0100, -0.0402, -0.0475,  ..., -0.0210, -0.0397, -0.0177],\n",
      "        ...,\n",
      "        [-0.0221,  0.0138, -0.0103,  ...,  0.0069, -0.0313,  0.0489],\n",
      "        [ 0.0125, -0.0151, -0.0174,  ..., -0.0367, -0.0056,  0.0487],\n",
      "        [ 0.0420, -0.0300,  0.0429,  ..., -0.0440, -0.0477,  0.0327]]), 'layers.0.bias': tensor([ 0.0207,  0.0246, -0.0463,  ...,  0.0493, -0.0022,  0.0134]), 'layers.1.weight_orig': tensor([[ 0.0166, -0.0082,  0.0294,  ...,  0.0002,  0.0290,  0.0217],\n",
      "        [ 0.0156, -0.0085, -0.0027,  ..., -0.0134, -0.0092,  0.0150],\n",
      "        [ 0.0155, -0.0105,  0.0253,  ...,  0.0154, -0.0124, -0.0047],\n",
      "        ...,\n",
      "        [-0.0007,  0.0020, -0.0059,  ...,  0.0310, -0.0267, -0.0075],\n",
      "        [-0.0166, -0.0059, -0.0279,  ..., -0.0235,  0.0150, -0.0084],\n",
      "        [ 0.0121, -0.0234,  0.0307,  ...,  0.0134, -0.0199, -0.0189]]), 'layers.1.bias': tensor([-0.0220,  0.0133,  0.0240,  ...,  0.0289, -0.0138,  0.0121]), 'layers.2.weight_orig': tensor([[ 0.0135,  0.0259, -0.0073,  ...,  0.0033, -0.0057, -0.0195],\n",
      "        [ 0.0272, -0.0092,  0.0116,  ...,  0.0005, -0.0218, -0.0232],\n",
      "        [-0.0092, -0.0215, -0.0213,  ..., -0.0295,  0.0259,  0.0296],\n",
      "        ...,\n",
      "        [-0.0014,  0.0176, -0.0108,  ...,  0.0204, -0.0239,  0.0016],\n",
      "        [-0.0155, -0.0185, -0.0183,  ...,  0.0303,  0.0045,  0.0269],\n",
      "        [ 0.0291, -0.0093,  0.0207,  ..., -0.0213,  0.0127, -0.0123]]), 'layers.2.bias': tensor([-0.0204, -0.0054, -0.0001, -0.0088,  0.0177,  0.0100,  0.0190,  0.0261,\n",
      "         0.0121, -0.0078])}\n",
      "ffn_mnist_copy:  OrderedDict([('layers.0.bias', tensor([-0.0026, -0.0117, -0.0658,  ...,  0.0011, -0.0267, -0.0120])), ('layers.0.weight_orig', tensor([[ 0.0718, -0.0249,  0.0297,  ...,  0.0265,  0.0681,  0.0286],\n",
      "        [ 0.0259,  0.0666,  0.0602,  ...,  0.0422,  0.0443, -0.0367],\n",
      "        [ 0.0288, -0.0402, -0.0475,  ..., -0.0210, -0.0397, -0.0373],\n",
      "        ...,\n",
      "        [-0.0221,  0.0327, -0.0103,  ...,  0.0069, -0.0264,  0.0555],\n",
      "        [ 0.0361,  0.0085,  0.0026,  ..., -0.0525, -0.0115,  0.0324],\n",
      "        [ 0.0679, -0.0300,  0.0722,  ..., -0.0547, -0.0477,  0.0416]])), ('layers.0.weight_mask', tensor([[1., 0., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 1.],\n",
      "        ...,\n",
      "        [0., 1., 0.,  ..., 0., 1., 1.],\n",
      "        [1., 0., 0.,  ..., 1., 0., 1.],\n",
      "        [1., 0., 1.,  ..., 1., 0., 1.]])), ('layers.1.bias', tensor([-0.0245, -0.0240,  0.0092,  ...,  0.0917, -0.0077,  0.0477])), ('layers.1.weight_orig', tensor([[ 0.0166, -0.0082,  0.0416,  ...,  0.0002,  0.0489,  0.0278],\n",
      "        [ 0.0156,  0.0071, -0.0558,  ..., -0.0233,  0.0046,  0.0603],\n",
      "        [ 0.0155, -0.0105,  0.0299,  ..., -0.0033,  0.0467, -0.0047],\n",
      "        ...,\n",
      "        [ 0.0067,  0.0020, -0.0059,  ...,  0.0483, -0.0913, -0.0425],\n",
      "        [-0.0166, -0.0059, -0.0156,  ..., -0.0190,  0.0150, -0.0619],\n",
      "        [-0.0056, -0.0234,  0.0307,  ...,  0.0097, -0.1253, -0.0411]])), ('layers.1.weight_mask', tensor([[0., 0., 1.,  ..., 0., 1., 1.],\n",
      "        [0., 0., 1.,  ..., 1., 0., 1.],\n",
      "        [0., 0., 1.,  ..., 0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 1., 1., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 1.]])), ('layers.2.bias', tensor([-0.0147, -0.0341, -0.0163, -0.0167,  0.0168, -0.0160,  0.0096,  0.0025,\n",
      "         0.0900, -0.0047])), ('layers.2.weight_orig', tensor([[ 0.0697, -0.0333, -0.0073,  ...,  0.0119, -0.0120, -0.0681],\n",
      "        [ 0.0536, -0.0092,  0.0116,  ...,  0.0204, -0.0218, -0.0298],\n",
      "        [-0.0694, -0.0215, -0.0499,  ..., -0.0216,  0.0203,  0.0454],\n",
      "        ...,\n",
      "        [-0.0014,  0.0186, -0.0382,  ...,  0.0139, -0.0552,  0.0016],\n",
      "        [-0.0155, -0.0247, -0.0294,  ...,  0.0355,  0.0045,  0.0435],\n",
      "        [ 0.0431, -0.0184,  0.0396,  ..., -0.0449,  0.0127, -0.0811]])), ('layers.2.weight_mask', tensor([[1., 1., 0.,  ..., 0., 0., 1.],\n",
      "        [1., 0., 0.,  ..., 1., 0., 1.],\n",
      "        [1., 0., 1.,  ..., 1., 1., 1.],\n",
      "        ...,\n",
      "        [0., 0., 1.,  ..., 0., 1., 0.],\n",
      "        [0., 1., 1.,  ..., 1., 0., 1.],\n",
      "        [1., 0., 1.,  ..., 1., 0., 1.]]))])\n",
      "ffn_mnist_copy_updated:  OrderedDict([('layers.0.bias', tensor([ 0.0207,  0.0246, -0.0463,  ...,  0.0493, -0.0022,  0.0134])), ('layers.0.weight_orig', tensor([[ 0.0457, -0.0249,  0.0128,  ...,  0.0147,  0.0492,  0.0053],\n",
      "        [-0.0101,  0.0405,  0.0396,  ...,  0.0192,  0.0305, -0.0367],\n",
      "        [ 0.0100, -0.0402, -0.0475,  ..., -0.0210, -0.0397, -0.0177],\n",
      "        ...,\n",
      "        [-0.0221,  0.0138, -0.0103,  ...,  0.0069, -0.0313,  0.0489],\n",
      "        [ 0.0125, -0.0151, -0.0174,  ..., -0.0367, -0.0056,  0.0487],\n",
      "        [ 0.0420, -0.0300,  0.0429,  ..., -0.0440, -0.0477,  0.0327]])), ('layers.0.weight_mask', tensor([[1., 0., 1.,  ..., 1., 1., 1.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 1.],\n",
      "        ...,\n",
      "        [0., 1., 0.,  ..., 0., 1., 1.],\n",
      "        [1., 0., 0.,  ..., 1., 0., 1.],\n",
      "        [1., 0., 1.,  ..., 1., 0., 1.]])), ('layers.1.bias', tensor([-0.0220,  0.0133,  0.0240,  ...,  0.0289, -0.0138,  0.0121])), ('layers.1.weight_orig', tensor([[ 0.0166, -0.0082,  0.0294,  ...,  0.0002,  0.0290,  0.0217],\n",
      "        [ 0.0156, -0.0085, -0.0027,  ..., -0.0134, -0.0092,  0.0150],\n",
      "        [ 0.0155, -0.0105,  0.0253,  ...,  0.0154, -0.0124, -0.0047],\n",
      "        ...,\n",
      "        [-0.0007,  0.0020, -0.0059,  ...,  0.0310, -0.0267, -0.0075],\n",
      "        [-0.0166, -0.0059, -0.0279,  ..., -0.0235,  0.0150, -0.0084],\n",
      "        [ 0.0121, -0.0234,  0.0307,  ...,  0.0134, -0.0199, -0.0189]])), ('layers.1.weight_mask', tensor([[0., 0., 1.,  ..., 0., 1., 1.],\n",
      "        [0., 0., 1.,  ..., 1., 0., 1.],\n",
      "        [0., 0., 1.,  ..., 0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 1., 1., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 1.]])), ('layers.2.bias', tensor([-0.0204, -0.0054, -0.0001, -0.0088,  0.0177,  0.0100,  0.0190,  0.0261,\n",
      "         0.0121, -0.0078])), ('layers.2.weight_orig', tensor([[ 0.0135,  0.0259, -0.0073,  ...,  0.0033, -0.0057, -0.0195],\n",
      "        [ 0.0272, -0.0092,  0.0116,  ...,  0.0005, -0.0218, -0.0232],\n",
      "        [-0.0092, -0.0215, -0.0213,  ..., -0.0295,  0.0259,  0.0296],\n",
      "        ...,\n",
      "        [-0.0014,  0.0176, -0.0108,  ...,  0.0204, -0.0239,  0.0016],\n",
      "        [-0.0155, -0.0185, -0.0183,  ...,  0.0303,  0.0045,  0.0269],\n",
      "        [ 0.0291, -0.0093,  0.0207,  ..., -0.0213,  0.0127, -0.0123]])), ('layers.2.weight_mask', tensor([[1., 1., 0.,  ..., 0., 0., 1.],\n",
      "        [1., 0., 0.,  ..., 1., 0., 1.],\n",
      "        [1., 0., 1.,  ..., 1., 1., 1.],\n",
      "        ...,\n",
      "        [0., 0., 1.,  ..., 0., 1., 0.],\n",
      "        [0., 1., 1.,  ..., 1., 0., 1.],\n",
      "        [1., 0., 1.,  ..., 1., 0., 1.]]))])\n",
      "Training normal precision model for 2 epochs\n",
      "Epoch 1, Accuracy: 97.65%\n",
      "Epoch 2, Accuracy: 97.57%\n",
      "Accuracy: 97.57%, Model Size: 13.21 MB, Mean Inference Time: 0.0009s ± 0.0005s\n",
      "Sparsity per parameter:\n",
      "  layers.0.weight_mask: 46.02%\n",
      "  layers.1.weight_mask: 58.71%\n",
      "  layers.2.weight_mask: 49.60%\n",
      "Sparsity across all pruned parameters: 55.11%\n",
      "Sparsity for the model overall: 55.03%\n",
      "Pruning iteration 3\n",
      "init_updated:  {'layers.0.weight_orig': tensor([[ 0.0457, -0.0249,  0.0128,  ...,  0.0147,  0.0492,  0.0053],\n",
      "        [-0.0101,  0.0405,  0.0396,  ...,  0.0192,  0.0305, -0.0367],\n",
      "        [ 0.0100, -0.0402, -0.0475,  ..., -0.0210, -0.0397, -0.0177],\n",
      "        ...,\n",
      "        [-0.0221,  0.0138, -0.0103,  ...,  0.0069, -0.0313,  0.0489],\n",
      "        [ 0.0125, -0.0151, -0.0174,  ..., -0.0367, -0.0056,  0.0487],\n",
      "        [ 0.0420, -0.0300,  0.0429,  ..., -0.0440, -0.0477,  0.0327]]), 'layers.0.bias': tensor([ 0.0207,  0.0246, -0.0463,  ...,  0.0493, -0.0022,  0.0134]), 'layers.1.weight_orig': tensor([[ 0.0166, -0.0082,  0.0294,  ...,  0.0002,  0.0290,  0.0217],\n",
      "        [ 0.0156, -0.0085, -0.0027,  ..., -0.0134, -0.0092,  0.0150],\n",
      "        [ 0.0155, -0.0105,  0.0253,  ...,  0.0154, -0.0124, -0.0047],\n",
      "        ...,\n",
      "        [-0.0007,  0.0020, -0.0059,  ...,  0.0310, -0.0267, -0.0075],\n",
      "        [-0.0166, -0.0059, -0.0279,  ..., -0.0235,  0.0150, -0.0084],\n",
      "        [ 0.0121, -0.0234,  0.0307,  ...,  0.0134, -0.0199, -0.0189]]), 'layers.1.bias': tensor([-0.0220,  0.0133,  0.0240,  ...,  0.0289, -0.0138,  0.0121]), 'layers.2.weight_orig': tensor([[ 0.0135,  0.0259, -0.0073,  ...,  0.0033, -0.0057, -0.0195],\n",
      "        [ 0.0272, -0.0092,  0.0116,  ...,  0.0005, -0.0218, -0.0232],\n",
      "        [-0.0092, -0.0215, -0.0213,  ..., -0.0295,  0.0259,  0.0296],\n",
      "        ...,\n",
      "        [-0.0014,  0.0176, -0.0108,  ...,  0.0204, -0.0239,  0.0016],\n",
      "        [-0.0155, -0.0185, -0.0183,  ...,  0.0303,  0.0045,  0.0269],\n",
      "        [ 0.0291, -0.0093,  0.0207,  ..., -0.0213,  0.0127, -0.0123]]), 'layers.2.bias': tensor([-0.0204, -0.0054, -0.0001, -0.0088,  0.0177,  0.0100,  0.0190,  0.0261,\n",
      "         0.0121, -0.0078])}\n",
      "ffn_mnist_copy:  OrderedDict([('layers.0.bias', tensor([ 0.0030,  0.0004, -0.0710,  ...,  0.0156, -0.0303, -0.0165])), ('layers.0.weight_orig', tensor([[ 0.0668, -0.0249,  0.0400,  ...,  0.0321,  0.0670,  0.0259],\n",
      "        [ 0.0216,  0.0712,  0.0543,  ...,  0.0446,  0.0191, -0.0367],\n",
      "        [ 0.0328, -0.0402, -0.0475,  ..., -0.0210, -0.0397, -0.0155],\n",
      "        ...,\n",
      "        [-0.0221,  0.0471, -0.0103,  ...,  0.0069, -0.0368,  0.0668],\n",
      "        [ 0.0398, -0.0151, -0.0174,  ..., -0.0141, -0.0056,  0.0812],\n",
      "        [ 0.0776, -0.0300,  0.0615,  ..., -0.0692, -0.0477,  0.0523]])), ('layers.0.weight_mask', tensor([[1., 0., 1.,  ..., 1., 1., 1.],\n",
      "        [0., 1., 1.,  ..., 1., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.,  ..., 0., 1., 1.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [1., 0., 1.,  ..., 1., 0., 1.]])), ('layers.1.bias', tensor([-0.0110, -0.0349,  0.0270,  ...,  0.0818, -0.0148,  0.0602])), ('layers.1.weight_orig', tensor([[ 0.0166, -0.0082,  0.0532,  ...,  0.0002,  0.0258,  0.0775],\n",
      "        [ 0.0156, -0.0085, -0.0324,  ..., -0.0600, -0.0092, -0.0071],\n",
      "        [ 0.0155, -0.0105,  0.0558,  ...,  0.0154,  0.0634, -0.0047],\n",
      "        ...,\n",
      "        [-0.0007,  0.0020, -0.0059,  ...,  0.0462, -0.0957, -0.0229],\n",
      "        [-0.0166, -0.0059, -0.0279,  ..., -0.0235,  0.0150, -0.0098],\n",
      "        [ 0.0121, -0.0234,  0.0307,  ...,  0.0134, -0.0620, -0.0192]])), ('layers.1.weight_mask', tensor([[0., 0., 1.,  ..., 0., 1., 1.],\n",
      "        [0., 0., 1.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 1., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.]])), ('layers.2.bias', tensor([-0.0120, -0.0227, -0.0107, -0.0140,  0.0150, -0.0027,  0.0053,  0.0096,\n",
      "         0.0643, -0.0033])), ('layers.2.weight_orig', tensor([[ 0.0174,  0.0124, -0.0073,  ...,  0.0033, -0.0057, -0.0294],\n",
      "        [ 0.0730, -0.0092,  0.0116,  ...,  0.0068, -0.0218, -0.0358],\n",
      "        [-0.0935, -0.0215, -0.0589,  ..., -0.0345,  0.0149,  0.0593],\n",
      "        ...,\n",
      "        [-0.0014,  0.0176, -0.0609,  ...,  0.0204, -0.0880,  0.0016],\n",
      "        [-0.0155, -0.0203, -0.0836,  ...,  0.0587,  0.0045,  0.0447],\n",
      "        [ 0.0735, -0.0093,  0.0415,  ..., -0.0590,  0.0127, -0.0906]])), ('layers.2.weight_mask', tensor([[0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [1., 0., 1.,  ..., 1., 0., 1.],\n",
      "        ...,\n",
      "        [0., 0., 1.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 1.,  ..., 1., 0., 1.],\n",
      "        [1., 0., 1.,  ..., 1., 0., 1.]]))])\n",
      "ffn_mnist_copy_updated:  OrderedDict([('layers.0.bias', tensor([ 0.0207,  0.0246, -0.0463,  ...,  0.0493, -0.0022,  0.0134])), ('layers.0.weight_orig', tensor([[ 0.0457, -0.0249,  0.0128,  ...,  0.0147,  0.0492,  0.0053],\n",
      "        [-0.0101,  0.0405,  0.0396,  ...,  0.0192,  0.0305, -0.0367],\n",
      "        [ 0.0100, -0.0402, -0.0475,  ..., -0.0210, -0.0397, -0.0177],\n",
      "        ...,\n",
      "        [-0.0221,  0.0138, -0.0103,  ...,  0.0069, -0.0313,  0.0489],\n",
      "        [ 0.0125, -0.0151, -0.0174,  ..., -0.0367, -0.0056,  0.0487],\n",
      "        [ 0.0420, -0.0300,  0.0429,  ..., -0.0440, -0.0477,  0.0327]])), ('layers.0.weight_mask', tensor([[1., 0., 1.,  ..., 1., 1., 1.],\n",
      "        [0., 1., 1.,  ..., 1., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.,  ..., 0., 1., 1.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [1., 0., 1.,  ..., 1., 0., 1.]])), ('layers.1.bias', tensor([-0.0220,  0.0133,  0.0240,  ...,  0.0289, -0.0138,  0.0121])), ('layers.1.weight_orig', tensor([[ 0.0166, -0.0082,  0.0294,  ...,  0.0002,  0.0290,  0.0217],\n",
      "        [ 0.0156, -0.0085, -0.0027,  ..., -0.0134, -0.0092,  0.0150],\n",
      "        [ 0.0155, -0.0105,  0.0253,  ...,  0.0154, -0.0124, -0.0047],\n",
      "        ...,\n",
      "        [-0.0007,  0.0020, -0.0059,  ...,  0.0310, -0.0267, -0.0075],\n",
      "        [-0.0166, -0.0059, -0.0279,  ..., -0.0235,  0.0150, -0.0084],\n",
      "        [ 0.0121, -0.0234,  0.0307,  ...,  0.0134, -0.0199, -0.0189]])), ('layers.1.weight_mask', tensor([[0., 0., 1.,  ..., 0., 1., 1.],\n",
      "        [0., 0., 1.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 1., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.]])), ('layers.2.bias', tensor([-0.0204, -0.0054, -0.0001, -0.0088,  0.0177,  0.0100,  0.0190,  0.0261,\n",
      "         0.0121, -0.0078])), ('layers.2.weight_orig', tensor([[ 0.0135,  0.0259, -0.0073,  ...,  0.0033, -0.0057, -0.0195],\n",
      "        [ 0.0272, -0.0092,  0.0116,  ...,  0.0005, -0.0218, -0.0232],\n",
      "        [-0.0092, -0.0215, -0.0213,  ..., -0.0295,  0.0259,  0.0296],\n",
      "        ...,\n",
      "        [-0.0014,  0.0176, -0.0108,  ...,  0.0204, -0.0239,  0.0016],\n",
      "        [-0.0155, -0.0185, -0.0183,  ...,  0.0303,  0.0045,  0.0269],\n",
      "        [ 0.0291, -0.0093,  0.0207,  ..., -0.0213,  0.0127, -0.0123]])), ('layers.2.weight_mask', tensor([[0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [1., 0., 1.,  ..., 1., 0., 1.],\n",
      "        ...,\n",
      "        [0., 0., 1.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 1.,  ..., 1., 0., 1.],\n",
      "        [1., 0., 1.,  ..., 1., 0., 1.]]))])\n",
      "Training normal precision model for 2 epochs\n",
      "Epoch 1, Accuracy: 98.11%\n",
      "Epoch 2, Accuracy: 98.10%\n",
      "Accuracy: 98.10%, Model Size: 8.86 MB, Mean Inference Time: 0.0009s ± 0.0006s\n",
      "Sparsity per parameter:\n",
      "  layers.0.weight_mask: 60.32%\n",
      "  layers.1.weight_mask: 73.81%\n",
      "  layers.2.weight_mask: 56.31%\n",
      "Sparsity across all pruned parameters: 69.92%\n",
      "Sparsity for the model overall: 69.83%\n",
      "Pruning iteration 4\n",
      "init_updated:  {'layers.0.weight_orig': tensor([[ 0.0457, -0.0249,  0.0128,  ...,  0.0147,  0.0492,  0.0053],\n",
      "        [-0.0101,  0.0405,  0.0396,  ...,  0.0192,  0.0305, -0.0367],\n",
      "        [ 0.0100, -0.0402, -0.0475,  ..., -0.0210, -0.0397, -0.0177],\n",
      "        ...,\n",
      "        [-0.0221,  0.0138, -0.0103,  ...,  0.0069, -0.0313,  0.0489],\n",
      "        [ 0.0125, -0.0151, -0.0174,  ..., -0.0367, -0.0056,  0.0487],\n",
      "        [ 0.0420, -0.0300,  0.0429,  ..., -0.0440, -0.0477,  0.0327]]), 'layers.0.bias': tensor([ 0.0207,  0.0246, -0.0463,  ...,  0.0493, -0.0022,  0.0134]), 'layers.1.weight_orig': tensor([[ 0.0166, -0.0082,  0.0294,  ...,  0.0002,  0.0290,  0.0217],\n",
      "        [ 0.0156, -0.0085, -0.0027,  ..., -0.0134, -0.0092,  0.0150],\n",
      "        [ 0.0155, -0.0105,  0.0253,  ...,  0.0154, -0.0124, -0.0047],\n",
      "        ...,\n",
      "        [-0.0007,  0.0020, -0.0059,  ...,  0.0310, -0.0267, -0.0075],\n",
      "        [-0.0166, -0.0059, -0.0279,  ..., -0.0235,  0.0150, -0.0084],\n",
      "        [ 0.0121, -0.0234,  0.0307,  ...,  0.0134, -0.0199, -0.0189]]), 'layers.1.bias': tensor([-0.0220,  0.0133,  0.0240,  ...,  0.0289, -0.0138,  0.0121]), 'layers.2.weight_orig': tensor([[ 0.0135,  0.0259, -0.0073,  ...,  0.0033, -0.0057, -0.0195],\n",
      "        [ 0.0272, -0.0092,  0.0116,  ...,  0.0005, -0.0218, -0.0232],\n",
      "        [-0.0092, -0.0215, -0.0213,  ..., -0.0295,  0.0259,  0.0296],\n",
      "        ...,\n",
      "        [-0.0014,  0.0176, -0.0108,  ...,  0.0204, -0.0239,  0.0016],\n",
      "        [-0.0155, -0.0185, -0.0183,  ...,  0.0303,  0.0045,  0.0269],\n",
      "        [ 0.0291, -0.0093,  0.0207,  ..., -0.0213,  0.0127, -0.0123]]), 'layers.2.bias': tensor([-0.0204, -0.0054, -0.0001, -0.0088,  0.0177,  0.0100,  0.0190,  0.0261,\n",
      "         0.0121, -0.0078])}\n",
      "ffn_mnist_copy:  OrderedDict([('layers.0.bias', tensor([ 0.0128, -0.0074, -0.0578,  ...,  0.0279, -0.0238, -0.0200])), ('layers.0.weight_orig', tensor([[ 0.0600, -0.0249,  0.0513,  ...,  0.0278,  0.0589,  0.0219],\n",
      "        [-0.0101,  0.0673,  0.0317,  ...,  0.0377,  0.0305, -0.0367],\n",
      "        [ 0.0172, -0.0402, -0.0475,  ..., -0.0210, -0.0397, -0.0177],\n",
      "        ...,\n",
      "        [-0.0221,  0.0268, -0.0103,  ...,  0.0069, -0.0537,  0.0318],\n",
      "        [ 0.0339, -0.0151, -0.0174,  ..., -0.0367, -0.0056,  0.0481],\n",
      "        [ 0.0777, -0.0300,  0.0454,  ..., -0.0670, -0.0477,  0.0641]])), ('layers.0.weight_mask', tensor([[1., 0., 1.,  ..., 0., 1., 0.],\n",
      "        [0., 1., 1.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 1., 1.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [1., 0., 1.,  ..., 1., 0., 1.]])), ('layers.1.bias', tensor([-0.0089,  0.0133,  0.0397,  ...,  0.0789,  0.0140,  0.0528])), ('layers.1.weight_orig', tensor([[ 0.0166, -0.0082,  0.0597,  ...,  0.0002,  0.0757,  0.0021],\n",
      "        [ 0.0156, -0.0085, -0.0027,  ..., -0.0134, -0.0092,  0.0150],\n",
      "        [ 0.0155, -0.0105,  0.0338,  ...,  0.0154,  0.0390, -0.0047],\n",
      "        ...,\n",
      "        [-0.0007,  0.0020, -0.0059,  ...,  0.0321, -0.0946, -0.0075],\n",
      "        [-0.0166, -0.0059, -0.0279,  ..., -0.0235,  0.0150, -0.0084],\n",
      "        [ 0.0121, -0.0234,  0.0307,  ...,  0.0134, -0.0730, -0.0189]])), ('layers.1.weight_mask', tensor([[0., 0., 1.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 1., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.]])), ('layers.2.bias', tensor([-0.0205, -0.0219, -0.0148, -0.0122,  0.0167, -0.0088,  0.0142,  0.0215,\n",
      "         0.0566, -0.0039])), ('layers.2.weight_orig', tensor([[ 0.0135,  0.0259, -0.0073,  ...,  0.0033, -0.0057, -0.0271],\n",
      "        [ 0.0759, -0.0092,  0.0116,  ...,  0.0005, -0.0218, -0.0658],\n",
      "        [-0.0843, -0.0215, -0.0572,  ..., -0.0620,  0.0259,  0.0593],\n",
      "        ...,\n",
      "        [-0.0014,  0.0176, -0.0530,  ...,  0.0204, -0.1234,  0.0016],\n",
      "        [-0.0155, -0.0185, -0.0853,  ...,  0.0654,  0.0045,  0.0530],\n",
      "        [ 0.0718, -0.0093,  0.0524,  ..., -0.0558,  0.0127, -0.0636]])), ('layers.2.weight_mask', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [1., 0., 1.,  ..., 1., 0., 1.],\n",
      "        ...,\n",
      "        [0., 0., 1.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 1.,  ..., 1., 0., 1.],\n",
      "        [1., 0., 1.,  ..., 1., 0., 1.]]))])\n",
      "ffn_mnist_copy_updated:  OrderedDict([('layers.0.bias', tensor([ 0.0207,  0.0246, -0.0463,  ...,  0.0493, -0.0022,  0.0134])), ('layers.0.weight_orig', tensor([[ 0.0457, -0.0249,  0.0128,  ...,  0.0147,  0.0492,  0.0053],\n",
      "        [-0.0101,  0.0405,  0.0396,  ...,  0.0192,  0.0305, -0.0367],\n",
      "        [ 0.0100, -0.0402, -0.0475,  ..., -0.0210, -0.0397, -0.0177],\n",
      "        ...,\n",
      "        [-0.0221,  0.0138, -0.0103,  ...,  0.0069, -0.0313,  0.0489],\n",
      "        [ 0.0125, -0.0151, -0.0174,  ..., -0.0367, -0.0056,  0.0487],\n",
      "        [ 0.0420, -0.0300,  0.0429,  ..., -0.0440, -0.0477,  0.0327]])), ('layers.0.weight_mask', tensor([[1., 0., 1.,  ..., 0., 1., 0.],\n",
      "        [0., 1., 1.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 1., 1.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [1., 0., 1.,  ..., 1., 0., 1.]])), ('layers.1.bias', tensor([-0.0220,  0.0133,  0.0240,  ...,  0.0289, -0.0138,  0.0121])), ('layers.1.weight_orig', tensor([[ 0.0166, -0.0082,  0.0294,  ...,  0.0002,  0.0290,  0.0217],\n",
      "        [ 0.0156, -0.0085, -0.0027,  ..., -0.0134, -0.0092,  0.0150],\n",
      "        [ 0.0155, -0.0105,  0.0253,  ...,  0.0154, -0.0124, -0.0047],\n",
      "        ...,\n",
      "        [-0.0007,  0.0020, -0.0059,  ...,  0.0310, -0.0267, -0.0075],\n",
      "        [-0.0166, -0.0059, -0.0279,  ..., -0.0235,  0.0150, -0.0084],\n",
      "        [ 0.0121, -0.0234,  0.0307,  ...,  0.0134, -0.0199, -0.0189]])), ('layers.1.weight_mask', tensor([[0., 0., 1.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 1., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.]])), ('layers.2.bias', tensor([-0.0204, -0.0054, -0.0001, -0.0088,  0.0177,  0.0100,  0.0190,  0.0261,\n",
      "         0.0121, -0.0078])), ('layers.2.weight_orig', tensor([[ 0.0135,  0.0259, -0.0073,  ...,  0.0033, -0.0057, -0.0195],\n",
      "        [ 0.0272, -0.0092,  0.0116,  ...,  0.0005, -0.0218, -0.0232],\n",
      "        [-0.0092, -0.0215, -0.0213,  ..., -0.0295,  0.0259,  0.0296],\n",
      "        ...,\n",
      "        [-0.0014,  0.0176, -0.0108,  ...,  0.0204, -0.0239,  0.0016],\n",
      "        [-0.0155, -0.0185, -0.0183,  ...,  0.0303,  0.0045,  0.0269],\n",
      "        [ 0.0291, -0.0093,  0.0207,  ..., -0.0213,  0.0127, -0.0123]])), ('layers.2.weight_mask', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [1., 0., 1.,  ..., 1., 0., 1.],\n",
      "        ...,\n",
      "        [0., 0., 1.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 1.,  ..., 1., 0., 1.],\n",
      "        [1., 0., 1.,  ..., 1., 0., 1.]]))])\n",
      "Training normal precision model for 2 epochs\n",
      "Epoch 1, Accuracy: 98.27%\n",
      "Epoch 2, Accuracy: 98.28%\n",
      "Accuracy: 98.28%, Model Size: 5.95 MB, Mean Inference Time: 0.0010s ± 0.0005s\n",
      "Sparsity per parameter:\n",
      "  layers.0.weight_mask: 71.06%\n",
      "  layers.1.weight_mask: 83.48%\n",
      "  layers.2.weight_mask: 59.41%\n",
      "Sparsity across all pruned parameters: 79.85%\n",
      "Sparsity for the model overall: 79.74%\n",
      "Pruning iteration 5\n",
      "init_updated:  {'layers.0.weight_orig': tensor([[ 0.0457, -0.0249,  0.0128,  ...,  0.0147,  0.0492,  0.0053],\n",
      "        [-0.0101,  0.0405,  0.0396,  ...,  0.0192,  0.0305, -0.0367],\n",
      "        [ 0.0100, -0.0402, -0.0475,  ..., -0.0210, -0.0397, -0.0177],\n",
      "        ...,\n",
      "        [-0.0221,  0.0138, -0.0103,  ...,  0.0069, -0.0313,  0.0489],\n",
      "        [ 0.0125, -0.0151, -0.0174,  ..., -0.0367, -0.0056,  0.0487],\n",
      "        [ 0.0420, -0.0300,  0.0429,  ..., -0.0440, -0.0477,  0.0327]]), 'layers.0.bias': tensor([ 0.0207,  0.0246, -0.0463,  ...,  0.0493, -0.0022,  0.0134]), 'layers.1.weight_orig': tensor([[ 0.0166, -0.0082,  0.0294,  ...,  0.0002,  0.0290,  0.0217],\n",
      "        [ 0.0156, -0.0085, -0.0027,  ..., -0.0134, -0.0092,  0.0150],\n",
      "        [ 0.0155, -0.0105,  0.0253,  ...,  0.0154, -0.0124, -0.0047],\n",
      "        ...,\n",
      "        [-0.0007,  0.0020, -0.0059,  ...,  0.0310, -0.0267, -0.0075],\n",
      "        [-0.0166, -0.0059, -0.0279,  ..., -0.0235,  0.0150, -0.0084],\n",
      "        [ 0.0121, -0.0234,  0.0307,  ...,  0.0134, -0.0199, -0.0189]]), 'layers.1.bias': tensor([-0.0220,  0.0133,  0.0240,  ...,  0.0289, -0.0138,  0.0121]), 'layers.2.weight_orig': tensor([[ 0.0135,  0.0259, -0.0073,  ...,  0.0033, -0.0057, -0.0195],\n",
      "        [ 0.0272, -0.0092,  0.0116,  ...,  0.0005, -0.0218, -0.0232],\n",
      "        [-0.0092, -0.0215, -0.0213,  ..., -0.0295,  0.0259,  0.0296],\n",
      "        ...,\n",
      "        [-0.0014,  0.0176, -0.0108,  ...,  0.0204, -0.0239,  0.0016],\n",
      "        [-0.0155, -0.0185, -0.0183,  ...,  0.0303,  0.0045,  0.0269],\n",
      "        [ 0.0291, -0.0093,  0.0207,  ..., -0.0213,  0.0127, -0.0123]]), 'layers.2.bias': tensor([-0.0204, -0.0054, -0.0001, -0.0088,  0.0177,  0.0100,  0.0190,  0.0261,\n",
      "         0.0121, -0.0078])}\n",
      "ffn_mnist_copy:  OrderedDict([('layers.0.bias', tensor([-0.0110,  0.0345, -0.0648,  ...,  0.0246, -0.0158,  0.0111])), ('layers.0.weight_orig', tensor([[ 0.0798, -0.0249,  0.0364,  ...,  0.0147,  0.0714,  0.0053],\n",
      "        [-0.0101,  0.0389,  0.0411,  ...,  0.0029,  0.0305, -0.0367],\n",
      "        [ 0.0100, -0.0402, -0.0475,  ..., -0.0210, -0.0397, -0.0177],\n",
      "        ...,\n",
      "        [-0.0221,  0.0138, -0.0103,  ...,  0.0069, -0.0192,  0.0545],\n",
      "        [ 0.0260, -0.0151, -0.0174,  ..., -0.0367, -0.0056,  0.0466],\n",
      "        [ 0.0456, -0.0300,  0.0471,  ..., -0.0662, -0.0477,  0.0321]])), ('layers.0.weight_mask', tensor([[1., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [1., 0., 1.,  ..., 1., 0., 0.]])), ('layers.1.bias', tensor([-0.0169,  0.0133,  0.0265,  ...,  0.0685,  0.0019,  0.0377])), ('layers.1.weight_orig', tensor([[ 0.0166, -0.0082,  0.0715,  ...,  0.0002,  0.0354,  0.0217],\n",
      "        [ 0.0156, -0.0085, -0.0027,  ..., -0.0134, -0.0092,  0.0150],\n",
      "        [ 0.0155, -0.0105,  0.0345,  ...,  0.0154,  0.0588, -0.0047],\n",
      "        ...,\n",
      "        [-0.0007,  0.0020, -0.0059,  ...,  0.0463, -0.1265, -0.0075],\n",
      "        [-0.0166, -0.0059, -0.0279,  ..., -0.0235,  0.0150, -0.0084],\n",
      "        [ 0.0121, -0.0234,  0.0307,  ...,  0.0134, -0.0665, -0.0189]])), ('layers.1.weight_mask', tensor([[0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 1., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.]])), ('layers.2.bias', tensor([-2.7376e-02, -1.1464e-02, -7.1901e-03, -1.9525e-02,  1.6942e-02,\n",
      "         3.1373e-05,  1.5053e-02,  2.0019e-02,  4.3555e-02,  6.0426e-04])), ('layers.2.weight_orig', tensor([[ 0.0135,  0.0259, -0.0073,  ...,  0.0033, -0.0057, -0.0195],\n",
      "        [ 0.0700, -0.0092,  0.0116,  ...,  0.0005, -0.0218, -0.0200],\n",
      "        [-0.1182, -0.0215, -0.0622,  ..., -0.0871,  0.0259,  0.0624],\n",
      "        ...,\n",
      "        [-0.0014,  0.0176, -0.0627,  ...,  0.0204, -0.1290,  0.0016],\n",
      "        [-0.0155, -0.0185, -0.0971,  ...,  0.0828,  0.0045,  0.0562],\n",
      "        [ 0.0873, -0.0093,  0.0639,  ..., -0.0720,  0.0127, -0.1039]])), ('layers.2.weight_mask', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 1.,  ..., 1., 0., 1.],\n",
      "        ...,\n",
      "        [0., 0., 1.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 1.,  ..., 1., 0., 1.],\n",
      "        [1., 0., 1.,  ..., 1., 0., 1.]]))])\n",
      "ffn_mnist_copy_updated:  OrderedDict([('layers.0.bias', tensor([ 0.0207,  0.0246, -0.0463,  ...,  0.0493, -0.0022,  0.0134])), ('layers.0.weight_orig', tensor([[ 0.0457, -0.0249,  0.0128,  ...,  0.0147,  0.0492,  0.0053],\n",
      "        [-0.0101,  0.0405,  0.0396,  ...,  0.0192,  0.0305, -0.0367],\n",
      "        [ 0.0100, -0.0402, -0.0475,  ..., -0.0210, -0.0397, -0.0177],\n",
      "        ...,\n",
      "        [-0.0221,  0.0138, -0.0103,  ...,  0.0069, -0.0313,  0.0489],\n",
      "        [ 0.0125, -0.0151, -0.0174,  ..., -0.0367, -0.0056,  0.0487],\n",
      "        [ 0.0420, -0.0300,  0.0429,  ..., -0.0440, -0.0477,  0.0327]])), ('layers.0.weight_mask', tensor([[1., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [1., 0., 1.,  ..., 1., 0., 0.]])), ('layers.1.bias', tensor([-0.0220,  0.0133,  0.0240,  ...,  0.0289, -0.0138,  0.0121])), ('layers.1.weight_orig', tensor([[ 0.0166, -0.0082,  0.0294,  ...,  0.0002,  0.0290,  0.0217],\n",
      "        [ 0.0156, -0.0085, -0.0027,  ..., -0.0134, -0.0092,  0.0150],\n",
      "        [ 0.0155, -0.0105,  0.0253,  ...,  0.0154, -0.0124, -0.0047],\n",
      "        ...,\n",
      "        [-0.0007,  0.0020, -0.0059,  ...,  0.0310, -0.0267, -0.0075],\n",
      "        [-0.0166, -0.0059, -0.0279,  ..., -0.0235,  0.0150, -0.0084],\n",
      "        [ 0.0121, -0.0234,  0.0307,  ...,  0.0134, -0.0199, -0.0189]])), ('layers.1.weight_mask', tensor([[0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 1., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.]])), ('layers.2.bias', tensor([-0.0204, -0.0054, -0.0001, -0.0088,  0.0177,  0.0100,  0.0190,  0.0261,\n",
      "         0.0121, -0.0078])), ('layers.2.weight_orig', tensor([[ 0.0135,  0.0259, -0.0073,  ...,  0.0033, -0.0057, -0.0195],\n",
      "        [ 0.0272, -0.0092,  0.0116,  ...,  0.0005, -0.0218, -0.0232],\n",
      "        [-0.0092, -0.0215, -0.0213,  ..., -0.0295,  0.0259,  0.0296],\n",
      "        ...,\n",
      "        [-0.0014,  0.0176, -0.0108,  ...,  0.0204, -0.0239,  0.0016],\n",
      "        [-0.0155, -0.0185, -0.0183,  ...,  0.0303,  0.0045,  0.0269],\n",
      "        [ 0.0291, -0.0093,  0.0207,  ..., -0.0213,  0.0127, -0.0123]])), ('layers.2.weight_mask', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 1.,  ..., 1., 0., 1.],\n",
      "        ...,\n",
      "        [0., 0., 1.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 1.,  ..., 1., 0., 1.],\n",
      "        [1., 0., 1.,  ..., 1., 0., 1.]]))])\n",
      "Training normal precision model for 2 epochs\n",
      "Epoch 1, Accuracy: 98.16%\n",
      "Epoch 2, Accuracy: 98.43%\n",
      "Accuracy: 98.43%, Model Size: 3.99 MB, Mean Inference Time: 0.0012s ± 0.0002s\n",
      "Sparsity per parameter:\n",
      "  layers.0.weight_mask: 79.91%\n",
      "  layers.1.weight_mask: 89.31%\n",
      "  layers.2.weight_mask: 61.71%\n",
      "Sparsity across all pruned parameters: 86.50%\n",
      "Sparsity for the model overall: 86.38%\n",
      "Pruning iteration 6\n",
      "init_updated:  {'layers.0.weight_orig': tensor([[ 0.0457, -0.0249,  0.0128,  ...,  0.0147,  0.0492,  0.0053],\n",
      "        [-0.0101,  0.0405,  0.0396,  ...,  0.0192,  0.0305, -0.0367],\n",
      "        [ 0.0100, -0.0402, -0.0475,  ..., -0.0210, -0.0397, -0.0177],\n",
      "        ...,\n",
      "        [-0.0221,  0.0138, -0.0103,  ...,  0.0069, -0.0313,  0.0489],\n",
      "        [ 0.0125, -0.0151, -0.0174,  ..., -0.0367, -0.0056,  0.0487],\n",
      "        [ 0.0420, -0.0300,  0.0429,  ..., -0.0440, -0.0477,  0.0327]]), 'layers.0.bias': tensor([ 0.0207,  0.0246, -0.0463,  ...,  0.0493, -0.0022,  0.0134]), 'layers.1.weight_orig': tensor([[ 0.0166, -0.0082,  0.0294,  ...,  0.0002,  0.0290,  0.0217],\n",
      "        [ 0.0156, -0.0085, -0.0027,  ..., -0.0134, -0.0092,  0.0150],\n",
      "        [ 0.0155, -0.0105,  0.0253,  ...,  0.0154, -0.0124, -0.0047],\n",
      "        ...,\n",
      "        [-0.0007,  0.0020, -0.0059,  ...,  0.0310, -0.0267, -0.0075],\n",
      "        [-0.0166, -0.0059, -0.0279,  ..., -0.0235,  0.0150, -0.0084],\n",
      "        [ 0.0121, -0.0234,  0.0307,  ...,  0.0134, -0.0199, -0.0189]]), 'layers.1.bias': tensor([-0.0220,  0.0133,  0.0240,  ...,  0.0289, -0.0138,  0.0121]), 'layers.2.weight_orig': tensor([[ 0.0135,  0.0259, -0.0073,  ...,  0.0033, -0.0057, -0.0195],\n",
      "        [ 0.0272, -0.0092,  0.0116,  ...,  0.0005, -0.0218, -0.0232],\n",
      "        [-0.0092, -0.0215, -0.0213,  ..., -0.0295,  0.0259,  0.0296],\n",
      "        ...,\n",
      "        [-0.0014,  0.0176, -0.0108,  ...,  0.0204, -0.0239,  0.0016],\n",
      "        [-0.0155, -0.0185, -0.0183,  ...,  0.0303,  0.0045,  0.0269],\n",
      "        [ 0.0291, -0.0093,  0.0207,  ..., -0.0213,  0.0127, -0.0123]]), 'layers.2.bias': tensor([-0.0204, -0.0054, -0.0001, -0.0088,  0.0177,  0.0100,  0.0190,  0.0261,\n",
      "         0.0121, -0.0078])}\n",
      "ffn_mnist_copy:  OrderedDict([('layers.0.bias', tensor([-0.0033,  0.0037, -0.0578,  ...,  0.0111, -0.0020, -0.0208])), ('layers.0.weight_orig', tensor([[ 0.0723, -0.0249,  0.0128,  ...,  0.0147,  0.0386,  0.0053],\n",
      "        [-0.0101,  0.0748,  0.0760,  ...,  0.0192,  0.0305, -0.0367],\n",
      "        [ 0.0100, -0.0402, -0.0475,  ..., -0.0210, -0.0397, -0.0177],\n",
      "        ...,\n",
      "        [-0.0221,  0.0138, -0.0103,  ...,  0.0069, -0.0313,  0.0498],\n",
      "        [ 0.0125, -0.0151, -0.0174,  ..., -0.0367, -0.0056,  0.0145],\n",
      "        [ 0.0749, -0.0300,  0.0780,  ..., -0.0281, -0.0477,  0.0327]])), ('layers.0.weight_mask', tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 1.,  ..., 0., 0., 0.]])), ('layers.1.bias', tensor([-0.0053,  0.0133,  0.0485,  ...,  0.0718,  0.0348,  0.0614])), ('layers.1.weight_orig', tensor([[ 0.0166, -0.0082,  0.0977,  ...,  0.0002,  0.0290,  0.0217],\n",
      "        [ 0.0156, -0.0085, -0.0027,  ..., -0.0134, -0.0092,  0.0150],\n",
      "        [ 0.0155, -0.0105,  0.0253,  ...,  0.0154,  0.0476, -0.0047],\n",
      "        ...,\n",
      "        [-0.0007,  0.0020, -0.0059,  ...,  0.0461, -0.0998, -0.0075],\n",
      "        [-0.0166, -0.0059, -0.0279,  ..., -0.0235,  0.0150, -0.0084],\n",
      "        [ 0.0121, -0.0234,  0.0307,  ...,  0.0134, -0.1054, -0.0189]])), ('layers.1.weight_mask', tensor([[0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.]])), ('layers.2.bias', tensor([-0.0174, -0.0024, -0.0177, -0.0133,  0.0245,  0.0032,  0.0018,  0.0198,\n",
      "         0.0402, -0.0085])), ('layers.2.weight_orig', tensor([[ 0.0135,  0.0259, -0.0073,  ...,  0.0033, -0.0057, -0.0195],\n",
      "        [ 0.0777, -0.0092,  0.0116,  ...,  0.0005, -0.0218, -0.0232],\n",
      "        [-0.0849, -0.0215, -0.0999,  ..., -0.0934,  0.0259,  0.1050],\n",
      "        ...,\n",
      "        [-0.0014,  0.0176, -0.0692,  ...,  0.0204, -0.1455,  0.0016],\n",
      "        [-0.0155, -0.0185, -0.1453,  ...,  0.0932,  0.0045,  0.0863],\n",
      "        [ 0.0869, -0.0093,  0.0710,  ..., -0.0851,  0.0127, -0.1213]])), ('layers.2.weight_mask', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 1.,  ..., 1., 0., 1.],\n",
      "        ...,\n",
      "        [0., 0., 1.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 1.,  ..., 1., 0., 1.],\n",
      "        [1., 0., 1.,  ..., 1., 0., 1.]]))])\n",
      "ffn_mnist_copy_updated:  OrderedDict([('layers.0.bias', tensor([ 0.0207,  0.0246, -0.0463,  ...,  0.0493, -0.0022,  0.0134])), ('layers.0.weight_orig', tensor([[ 0.0457, -0.0249,  0.0128,  ...,  0.0147,  0.0492,  0.0053],\n",
      "        [-0.0101,  0.0405,  0.0396,  ...,  0.0192,  0.0305, -0.0367],\n",
      "        [ 0.0100, -0.0402, -0.0475,  ..., -0.0210, -0.0397, -0.0177],\n",
      "        ...,\n",
      "        [-0.0221,  0.0138, -0.0103,  ...,  0.0069, -0.0313,  0.0489],\n",
      "        [ 0.0125, -0.0151, -0.0174,  ..., -0.0367, -0.0056,  0.0487],\n",
      "        [ 0.0420, -0.0300,  0.0429,  ..., -0.0440, -0.0477,  0.0327]])), ('layers.0.weight_mask', tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 1.,  ..., 0., 0., 0.]])), ('layers.1.bias', tensor([-0.0220,  0.0133,  0.0240,  ...,  0.0289, -0.0138,  0.0121])), ('layers.1.weight_orig', tensor([[ 0.0166, -0.0082,  0.0294,  ...,  0.0002,  0.0290,  0.0217],\n",
      "        [ 0.0156, -0.0085, -0.0027,  ..., -0.0134, -0.0092,  0.0150],\n",
      "        [ 0.0155, -0.0105,  0.0253,  ...,  0.0154, -0.0124, -0.0047],\n",
      "        ...,\n",
      "        [-0.0007,  0.0020, -0.0059,  ...,  0.0310, -0.0267, -0.0075],\n",
      "        [-0.0166, -0.0059, -0.0279,  ..., -0.0235,  0.0150, -0.0084],\n",
      "        [ 0.0121, -0.0234,  0.0307,  ...,  0.0134, -0.0199, -0.0189]])), ('layers.1.weight_mask', tensor([[0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.]])), ('layers.2.bias', tensor([-0.0204, -0.0054, -0.0001, -0.0088,  0.0177,  0.0100,  0.0190,  0.0261,\n",
      "         0.0121, -0.0078])), ('layers.2.weight_orig', tensor([[ 0.0135,  0.0259, -0.0073,  ...,  0.0033, -0.0057, -0.0195],\n",
      "        [ 0.0272, -0.0092,  0.0116,  ...,  0.0005, -0.0218, -0.0232],\n",
      "        [-0.0092, -0.0215, -0.0213,  ..., -0.0295,  0.0259,  0.0296],\n",
      "        ...,\n",
      "        [-0.0014,  0.0176, -0.0108,  ...,  0.0204, -0.0239,  0.0016],\n",
      "        [-0.0155, -0.0185, -0.0183,  ...,  0.0303,  0.0045,  0.0269],\n",
      "        [ 0.0291, -0.0093,  0.0207,  ..., -0.0213,  0.0127, -0.0123]])), ('layers.2.weight_mask', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 1.,  ..., 1., 0., 1.],\n",
      "        ...,\n",
      "        [0., 0., 1.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 1.,  ..., 1., 0., 1.],\n",
      "        [1., 0., 1.,  ..., 1., 0., 1.]]))])\n",
      "Training normal precision model for 2 epochs\n",
      "Epoch 1, Accuracy: 98.24%\n",
      "Epoch 2, Accuracy: 98.38%\n",
      "Accuracy: 98.38%, Model Size: 2.69 MB, Mean Inference Time: 0.0005s ± 0.0004s\n",
      "Sparsity per parameter:\n",
      "  layers.0.weight_mask: 86.93%\n",
      "  layers.1.weight_mask: 92.80%\n",
      "  layers.2.weight_mask: 63.38%\n",
      "Sparsity across all pruned parameters: 90.95%\n",
      "Sparsity for the model overall: 90.83%\n",
      "Pruning iteration 7\n",
      "init_updated:  {'layers.0.weight_orig': tensor([[ 0.0457, -0.0249,  0.0128,  ...,  0.0147,  0.0492,  0.0053],\n",
      "        [-0.0101,  0.0405,  0.0396,  ...,  0.0192,  0.0305, -0.0367],\n",
      "        [ 0.0100, -0.0402, -0.0475,  ..., -0.0210, -0.0397, -0.0177],\n",
      "        ...,\n",
      "        [-0.0221,  0.0138, -0.0103,  ...,  0.0069, -0.0313,  0.0489],\n",
      "        [ 0.0125, -0.0151, -0.0174,  ..., -0.0367, -0.0056,  0.0487],\n",
      "        [ 0.0420, -0.0300,  0.0429,  ..., -0.0440, -0.0477,  0.0327]]), 'layers.0.bias': tensor([ 0.0207,  0.0246, -0.0463,  ...,  0.0493, -0.0022,  0.0134]), 'layers.1.weight_orig': tensor([[ 0.0166, -0.0082,  0.0294,  ...,  0.0002,  0.0290,  0.0217],\n",
      "        [ 0.0156, -0.0085, -0.0027,  ..., -0.0134, -0.0092,  0.0150],\n",
      "        [ 0.0155, -0.0105,  0.0253,  ...,  0.0154, -0.0124, -0.0047],\n",
      "        ...,\n",
      "        [-0.0007,  0.0020, -0.0059,  ...,  0.0310, -0.0267, -0.0075],\n",
      "        [-0.0166, -0.0059, -0.0279,  ..., -0.0235,  0.0150, -0.0084],\n",
      "        [ 0.0121, -0.0234,  0.0307,  ...,  0.0134, -0.0199, -0.0189]]), 'layers.1.bias': tensor([-0.0220,  0.0133,  0.0240,  ...,  0.0289, -0.0138,  0.0121]), 'layers.2.weight_orig': tensor([[ 0.0135,  0.0259, -0.0073,  ...,  0.0033, -0.0057, -0.0195],\n",
      "        [ 0.0272, -0.0092,  0.0116,  ...,  0.0005, -0.0218, -0.0232],\n",
      "        [-0.0092, -0.0215, -0.0213,  ..., -0.0295,  0.0259,  0.0296],\n",
      "        ...,\n",
      "        [-0.0014,  0.0176, -0.0108,  ...,  0.0204, -0.0239,  0.0016],\n",
      "        [-0.0155, -0.0185, -0.0183,  ...,  0.0303,  0.0045,  0.0269],\n",
      "        [ 0.0291, -0.0093,  0.0207,  ..., -0.0213,  0.0127, -0.0123]]), 'layers.2.bias': tensor([-0.0204, -0.0054, -0.0001, -0.0088,  0.0177,  0.0100,  0.0190,  0.0261,\n",
      "         0.0121, -0.0078])}\n",
      "ffn_mnist_copy:  OrderedDict([('layers.0.bias', tensor([-0.0037,  0.0221, -0.0669,  ...,  0.0422, -0.0220,  0.0140])), ('layers.0.weight_orig', tensor([[ 0.0722, -0.0249,  0.0128,  ...,  0.0147,  0.0492,  0.0053],\n",
      "        [-0.0101,  0.0489,  0.0496,  ...,  0.0192,  0.0305, -0.0367],\n",
      "        [ 0.0100, -0.0402, -0.0475,  ..., -0.0210, -0.0397, -0.0177],\n",
      "        ...,\n",
      "        [-0.0221,  0.0138, -0.0103,  ...,  0.0069, -0.0313,  0.0133],\n",
      "        [ 0.0125, -0.0151, -0.0174,  ..., -0.0367, -0.0056,  0.0487],\n",
      "        [ 0.0451, -0.0300,  0.0373,  ..., -0.0440, -0.0477,  0.0327]])), ('layers.0.weight_mask', tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])), ('layers.1.bias', tensor([-0.0060,  0.0133,  0.0363,  ...,  0.0642,  0.0091,  0.0609])), ('layers.1.weight_orig', tensor([[ 0.0166, -0.0082,  0.0685,  ...,  0.0002,  0.0290,  0.0217],\n",
      "        [ 0.0156, -0.0085, -0.0027,  ..., -0.0134, -0.0092,  0.0150],\n",
      "        [ 0.0155, -0.0105,  0.0253,  ...,  0.0154,  0.0371, -0.0047],\n",
      "        ...,\n",
      "        [-0.0007,  0.0020, -0.0059,  ...,  0.0310, -0.1003, -0.0075],\n",
      "        [-0.0166, -0.0059, -0.0279,  ..., -0.0235,  0.0150, -0.0084],\n",
      "        [ 0.0121, -0.0234,  0.0307,  ...,  0.0134, -0.0907, -0.0189]])), ('layers.1.weight_mask', tensor([[0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.]])), ('layers.2.bias', tensor([-0.0238, -0.0087, -0.0007, -0.0160,  0.0222, -0.0044,  0.0194,  0.0288,\n",
      "         0.0307, -0.0143])), ('layers.2.weight_orig', tensor([[ 0.0135,  0.0259, -0.0073,  ...,  0.0033, -0.0057, -0.0195],\n",
      "        [ 0.0616, -0.0092,  0.0116,  ...,  0.0005, -0.0218, -0.0232],\n",
      "        [-0.1065, -0.0215, -0.0490,  ..., -0.1055,  0.0259,  0.0994],\n",
      "        ...,\n",
      "        [-0.0014,  0.0176, -0.0654,  ...,  0.0204, -0.1339,  0.0016],\n",
      "        [-0.0155, -0.0185, -0.1221,  ...,  0.1065,  0.0045,  0.0958],\n",
      "        [ 0.0949, -0.0093,  0.0943,  ..., -0.1010,  0.0127, -0.1174]])), ('layers.2.weight_mask', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 1., 0., 1.],\n",
      "        ...,\n",
      "        [0., 0., 1.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 1.,  ..., 1., 0., 1.],\n",
      "        [1., 0., 1.,  ..., 1., 0., 1.]]))])\n",
      "ffn_mnist_copy_updated:  OrderedDict([('layers.0.bias', tensor([ 0.0207,  0.0246, -0.0463,  ...,  0.0493, -0.0022,  0.0134])), ('layers.0.weight_orig', tensor([[ 0.0457, -0.0249,  0.0128,  ...,  0.0147,  0.0492,  0.0053],\n",
      "        [-0.0101,  0.0405,  0.0396,  ...,  0.0192,  0.0305, -0.0367],\n",
      "        [ 0.0100, -0.0402, -0.0475,  ..., -0.0210, -0.0397, -0.0177],\n",
      "        ...,\n",
      "        [-0.0221,  0.0138, -0.0103,  ...,  0.0069, -0.0313,  0.0489],\n",
      "        [ 0.0125, -0.0151, -0.0174,  ..., -0.0367, -0.0056,  0.0487],\n",
      "        [ 0.0420, -0.0300,  0.0429,  ..., -0.0440, -0.0477,  0.0327]])), ('layers.0.weight_mask', tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])), ('layers.1.bias', tensor([-0.0220,  0.0133,  0.0240,  ...,  0.0289, -0.0138,  0.0121])), ('layers.1.weight_orig', tensor([[ 0.0166, -0.0082,  0.0294,  ...,  0.0002,  0.0290,  0.0217],\n",
      "        [ 0.0156, -0.0085, -0.0027,  ..., -0.0134, -0.0092,  0.0150],\n",
      "        [ 0.0155, -0.0105,  0.0253,  ...,  0.0154, -0.0124, -0.0047],\n",
      "        ...,\n",
      "        [-0.0007,  0.0020, -0.0059,  ...,  0.0310, -0.0267, -0.0075],\n",
      "        [-0.0166, -0.0059, -0.0279,  ..., -0.0235,  0.0150, -0.0084],\n",
      "        [ 0.0121, -0.0234,  0.0307,  ...,  0.0134, -0.0199, -0.0189]])), ('layers.1.weight_mask', tensor([[0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.]])), ('layers.2.bias', tensor([-0.0204, -0.0054, -0.0001, -0.0088,  0.0177,  0.0100,  0.0190,  0.0261,\n",
      "         0.0121, -0.0078])), ('layers.2.weight_orig', tensor([[ 0.0135,  0.0259, -0.0073,  ...,  0.0033, -0.0057, -0.0195],\n",
      "        [ 0.0272, -0.0092,  0.0116,  ...,  0.0005, -0.0218, -0.0232],\n",
      "        [-0.0092, -0.0215, -0.0213,  ..., -0.0295,  0.0259,  0.0296],\n",
      "        ...,\n",
      "        [-0.0014,  0.0176, -0.0108,  ...,  0.0204, -0.0239,  0.0016],\n",
      "        [-0.0155, -0.0185, -0.0183,  ...,  0.0303,  0.0045,  0.0269],\n",
      "        [ 0.0291, -0.0093,  0.0207,  ..., -0.0213,  0.0127, -0.0123]])), ('layers.2.weight_mask', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 1., 0., 1.],\n",
      "        ...,\n",
      "        [0., 0., 1.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 1.,  ..., 1., 0., 1.],\n",
      "        [1., 0., 1.,  ..., 1., 0., 1.]]))])\n",
      "Training normal precision model for 2 epochs\n",
      "Epoch 1, Accuracy: 98.20%\n",
      "Epoch 2, Accuracy: 98.40%\n",
      "Accuracy: 98.40%, Model Size: 1.81 MB, Mean Inference Time: 0.0008s ± 0.0004s\n",
      "Sparsity per parameter:\n",
      "  layers.0.weight_mask: 92.02%\n",
      "  layers.1.weight_mask: 94.98%\n",
      "  layers.2.weight_mask: 64.66%\n",
      "Sparsity across all pruned parameters: 93.94%\n",
      "Sparsity for the model overall: 93.81%\n",
      "Pruning iteration 8\n",
      "init_updated:  {'layers.0.weight_orig': tensor([[ 0.0457, -0.0249,  0.0128,  ...,  0.0147,  0.0492,  0.0053],\n",
      "        [-0.0101,  0.0405,  0.0396,  ...,  0.0192,  0.0305, -0.0367],\n",
      "        [ 0.0100, -0.0402, -0.0475,  ..., -0.0210, -0.0397, -0.0177],\n",
      "        ...,\n",
      "        [-0.0221,  0.0138, -0.0103,  ...,  0.0069, -0.0313,  0.0489],\n",
      "        [ 0.0125, -0.0151, -0.0174,  ..., -0.0367, -0.0056,  0.0487],\n",
      "        [ 0.0420, -0.0300,  0.0429,  ..., -0.0440, -0.0477,  0.0327]]), 'layers.0.bias': tensor([ 0.0207,  0.0246, -0.0463,  ...,  0.0493, -0.0022,  0.0134]), 'layers.1.weight_orig': tensor([[ 0.0166, -0.0082,  0.0294,  ...,  0.0002,  0.0290,  0.0217],\n",
      "        [ 0.0156, -0.0085, -0.0027,  ..., -0.0134, -0.0092,  0.0150],\n",
      "        [ 0.0155, -0.0105,  0.0253,  ...,  0.0154, -0.0124, -0.0047],\n",
      "        ...,\n",
      "        [-0.0007,  0.0020, -0.0059,  ...,  0.0310, -0.0267, -0.0075],\n",
      "        [-0.0166, -0.0059, -0.0279,  ..., -0.0235,  0.0150, -0.0084],\n",
      "        [ 0.0121, -0.0234,  0.0307,  ...,  0.0134, -0.0199, -0.0189]]), 'layers.1.bias': tensor([-0.0220,  0.0133,  0.0240,  ...,  0.0289, -0.0138,  0.0121]), 'layers.2.weight_orig': tensor([[ 0.0135,  0.0259, -0.0073,  ...,  0.0033, -0.0057, -0.0195],\n",
      "        [ 0.0272, -0.0092,  0.0116,  ...,  0.0005, -0.0218, -0.0232],\n",
      "        [-0.0092, -0.0215, -0.0213,  ..., -0.0295,  0.0259,  0.0296],\n",
      "        ...,\n",
      "        [-0.0014,  0.0176, -0.0108,  ...,  0.0204, -0.0239,  0.0016],\n",
      "        [-0.0155, -0.0185, -0.0183,  ...,  0.0303,  0.0045,  0.0269],\n",
      "        [ 0.0291, -0.0093,  0.0207,  ..., -0.0213,  0.0127, -0.0123]]), 'layers.2.bias': tensor([-0.0204, -0.0054, -0.0001, -0.0088,  0.0177,  0.0100,  0.0190,  0.0261,\n",
      "         0.0121, -0.0078])}\n",
      "ffn_mnist_copy:  OrderedDict([('layers.0.bias', tensor([ 0.0097,  0.0233, -0.0321,  ...,  0.0568, -0.0017, -0.0215])), ('layers.0.weight_orig', tensor([[ 0.0594, -0.0249,  0.0128,  ...,  0.0147,  0.0492,  0.0053],\n",
      "        [-0.0101,  0.0405,  0.0396,  ...,  0.0192,  0.0305, -0.0367],\n",
      "        [ 0.0100, -0.0402, -0.0475,  ..., -0.0210, -0.0397, -0.0177],\n",
      "        ...,\n",
      "        [-0.0221,  0.0138, -0.0103,  ...,  0.0069, -0.0313,  0.0489],\n",
      "        [ 0.0125, -0.0151, -0.0174,  ..., -0.0367, -0.0056,  0.0487],\n",
      "        [ 0.0420, -0.0300,  0.0429,  ..., -0.0440, -0.0477,  0.0327]])), ('layers.0.weight_mask', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])), ('layers.1.bias', tensor([0.0084, 0.0133, 0.0798,  ..., 0.0821, 0.0108, 0.0661])), ('layers.1.weight_orig', tensor([[ 0.0166, -0.0082,  0.0856,  ...,  0.0002,  0.0290,  0.0217],\n",
      "        [ 0.0156, -0.0085, -0.0027,  ..., -0.0134, -0.0092,  0.0150],\n",
      "        [ 0.0155, -0.0105,  0.0253,  ...,  0.0154, -0.0124, -0.0047],\n",
      "        ...,\n",
      "        [-0.0007,  0.0020, -0.0059,  ...,  0.0310, -0.0955, -0.0075],\n",
      "        [-0.0166, -0.0059, -0.0279,  ..., -0.0235,  0.0150, -0.0084],\n",
      "        [ 0.0121, -0.0234,  0.0307,  ...,  0.0134, -0.0849, -0.0189]])), ('layers.1.weight_mask', tensor([[0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.]])), ('layers.2.bias', tensor([-0.0148, -0.0004, -0.0129, -0.0186,  0.0194, -0.0073,  0.0273,  0.0145,\n",
      "         0.0363, -0.0048])), ('layers.2.weight_orig', tensor([[ 0.0135,  0.0259, -0.0073,  ...,  0.0033, -0.0057, -0.0195],\n",
      "        [ 0.0867, -0.0092,  0.0116,  ...,  0.0005, -0.0218, -0.0232],\n",
      "        [-0.0964, -0.0215, -0.0213,  ..., -0.1363,  0.0259,  0.1078],\n",
      "        ...,\n",
      "        [-0.0014,  0.0176, -0.1182,  ...,  0.0204, -0.1700,  0.0016],\n",
      "        [-0.0155, -0.0185, -0.1748,  ...,  0.1237,  0.0045,  0.1134],\n",
      "        [ 0.1032, -0.0093,  0.1213,  ..., -0.0696,  0.0127, -0.1170]])), ('layers.2.weight_mask', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 1., 0., 1.],\n",
      "        ...,\n",
      "        [0., 0., 1.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 1.,  ..., 1., 0., 1.],\n",
      "        [1., 0., 1.,  ..., 0., 0., 1.]]))])\n",
      "ffn_mnist_copy_updated:  OrderedDict([('layers.0.bias', tensor([ 0.0207,  0.0246, -0.0463,  ...,  0.0493, -0.0022,  0.0134])), ('layers.0.weight_orig', tensor([[ 0.0457, -0.0249,  0.0128,  ...,  0.0147,  0.0492,  0.0053],\n",
      "        [-0.0101,  0.0405,  0.0396,  ...,  0.0192,  0.0305, -0.0367],\n",
      "        [ 0.0100, -0.0402, -0.0475,  ..., -0.0210, -0.0397, -0.0177],\n",
      "        ...,\n",
      "        [-0.0221,  0.0138, -0.0103,  ...,  0.0069, -0.0313,  0.0489],\n",
      "        [ 0.0125, -0.0151, -0.0174,  ..., -0.0367, -0.0056,  0.0487],\n",
      "        [ 0.0420, -0.0300,  0.0429,  ..., -0.0440, -0.0477,  0.0327]])), ('layers.0.weight_mask', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])), ('layers.1.bias', tensor([-0.0220,  0.0133,  0.0240,  ...,  0.0289, -0.0138,  0.0121])), ('layers.1.weight_orig', tensor([[ 0.0166, -0.0082,  0.0294,  ...,  0.0002,  0.0290,  0.0217],\n",
      "        [ 0.0156, -0.0085, -0.0027,  ..., -0.0134, -0.0092,  0.0150],\n",
      "        [ 0.0155, -0.0105,  0.0253,  ...,  0.0154, -0.0124, -0.0047],\n",
      "        ...,\n",
      "        [-0.0007,  0.0020, -0.0059,  ...,  0.0310, -0.0267, -0.0075],\n",
      "        [-0.0166, -0.0059, -0.0279,  ..., -0.0235,  0.0150, -0.0084],\n",
      "        [ 0.0121, -0.0234,  0.0307,  ...,  0.0134, -0.0199, -0.0189]])), ('layers.1.weight_mask', tensor([[0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.]])), ('layers.2.bias', tensor([-0.0204, -0.0054, -0.0001, -0.0088,  0.0177,  0.0100,  0.0190,  0.0261,\n",
      "         0.0121, -0.0078])), ('layers.2.weight_orig', tensor([[ 0.0135,  0.0259, -0.0073,  ...,  0.0033, -0.0057, -0.0195],\n",
      "        [ 0.0272, -0.0092,  0.0116,  ...,  0.0005, -0.0218, -0.0232],\n",
      "        [-0.0092, -0.0215, -0.0213,  ..., -0.0295,  0.0259,  0.0296],\n",
      "        ...,\n",
      "        [-0.0014,  0.0176, -0.0108,  ...,  0.0204, -0.0239,  0.0016],\n",
      "        [-0.0155, -0.0185, -0.0183,  ...,  0.0303,  0.0045,  0.0269],\n",
      "        [ 0.0291, -0.0093,  0.0207,  ..., -0.0213,  0.0127, -0.0123]])), ('layers.2.weight_mask', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 1., 0., 1.],\n",
      "        ...,\n",
      "        [0., 0., 1.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 1.,  ..., 1., 0., 1.],\n",
      "        [1., 0., 1.,  ..., 0., 0., 1.]]))])\n",
      "Training normal precision model for 2 epochs\n",
      "Epoch 1, Accuracy: 98.52%\n",
      "Epoch 2, Accuracy: 98.38%\n",
      "Accuracy: 98.38%, Model Size: 1.22 MB, Mean Inference Time: 0.0009s ± 0.0005s\n",
      "Sparsity per parameter:\n",
      "  layers.0.weight_mask: 95.27%\n",
      "  layers.1.weight_mask: 96.49%\n",
      "  layers.2.weight_mask: 66.25%\n",
      "Sparsity across all pruned parameters: 95.94%\n",
      "Sparsity for the model overall: 95.81%\n",
      "Pruning iteration 9\n",
      "init_updated:  {'layers.0.weight_orig': tensor([[ 0.0457, -0.0249,  0.0128,  ...,  0.0147,  0.0492,  0.0053],\n",
      "        [-0.0101,  0.0405,  0.0396,  ...,  0.0192,  0.0305, -0.0367],\n",
      "        [ 0.0100, -0.0402, -0.0475,  ..., -0.0210, -0.0397, -0.0177],\n",
      "        ...,\n",
      "        [-0.0221,  0.0138, -0.0103,  ...,  0.0069, -0.0313,  0.0489],\n",
      "        [ 0.0125, -0.0151, -0.0174,  ..., -0.0367, -0.0056,  0.0487],\n",
      "        [ 0.0420, -0.0300,  0.0429,  ..., -0.0440, -0.0477,  0.0327]]), 'layers.0.bias': tensor([ 0.0207,  0.0246, -0.0463,  ...,  0.0493, -0.0022,  0.0134]), 'layers.1.weight_orig': tensor([[ 0.0166, -0.0082,  0.0294,  ...,  0.0002,  0.0290,  0.0217],\n",
      "        [ 0.0156, -0.0085, -0.0027,  ..., -0.0134, -0.0092,  0.0150],\n",
      "        [ 0.0155, -0.0105,  0.0253,  ...,  0.0154, -0.0124, -0.0047],\n",
      "        ...,\n",
      "        [-0.0007,  0.0020, -0.0059,  ...,  0.0310, -0.0267, -0.0075],\n",
      "        [-0.0166, -0.0059, -0.0279,  ..., -0.0235,  0.0150, -0.0084],\n",
      "        [ 0.0121, -0.0234,  0.0307,  ...,  0.0134, -0.0199, -0.0189]]), 'layers.1.bias': tensor([-0.0220,  0.0133,  0.0240,  ...,  0.0289, -0.0138,  0.0121]), 'layers.2.weight_orig': tensor([[ 0.0135,  0.0259, -0.0073,  ...,  0.0033, -0.0057, -0.0195],\n",
      "        [ 0.0272, -0.0092,  0.0116,  ...,  0.0005, -0.0218, -0.0232],\n",
      "        [-0.0092, -0.0215, -0.0213,  ..., -0.0295,  0.0259,  0.0296],\n",
      "        ...,\n",
      "        [-0.0014,  0.0176, -0.0108,  ...,  0.0204, -0.0239,  0.0016],\n",
      "        [-0.0155, -0.0185, -0.0183,  ...,  0.0303,  0.0045,  0.0269],\n",
      "        [ 0.0291, -0.0093,  0.0207,  ..., -0.0213,  0.0127, -0.0123]]), 'layers.2.bias': tensor([-0.0204, -0.0054, -0.0001, -0.0088,  0.0177,  0.0100,  0.0190,  0.0261,\n",
      "         0.0121, -0.0078])}\n",
      "ffn_mnist_copy:  OrderedDict([('layers.0.bias', tensor([-0.0015,  0.0188,  0.0016,  ...,  0.0726, -0.0039,  0.0338])), ('layers.0.weight_orig', tensor([[ 0.0457, -0.0249,  0.0128,  ...,  0.0147,  0.0492,  0.0053],\n",
      "        [-0.0101,  0.0405,  0.0396,  ...,  0.0192,  0.0305, -0.0367],\n",
      "        [ 0.0100, -0.0402, -0.0475,  ..., -0.0210, -0.0397, -0.0177],\n",
      "        ...,\n",
      "        [-0.0221,  0.0138, -0.0103,  ...,  0.0069, -0.0313,  0.0489],\n",
      "        [ 0.0125, -0.0151, -0.0174,  ..., -0.0367, -0.0056,  0.0487],\n",
      "        [ 0.0420, -0.0300,  0.0429,  ..., -0.0440, -0.0477,  0.0327]])), ('layers.0.weight_mask', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])), ('layers.1.bias', tensor([-0.0024,  0.0133,  0.0636,  ...,  0.0419,  0.0273,  0.0932])), ('layers.1.weight_orig', tensor([[ 0.0166, -0.0082,  0.0728,  ...,  0.0002,  0.0290,  0.0217],\n",
      "        [ 0.0156, -0.0085, -0.0027,  ..., -0.0134, -0.0092,  0.0150],\n",
      "        [ 0.0155, -0.0105,  0.0253,  ...,  0.0154, -0.0124, -0.0047],\n",
      "        ...,\n",
      "        [-0.0007,  0.0020, -0.0059,  ...,  0.0310, -0.0628, -0.0075],\n",
      "        [-0.0166, -0.0059, -0.0279,  ..., -0.0235,  0.0150, -0.0084],\n",
      "        [ 0.0121, -0.0234,  0.0307,  ...,  0.0134, -0.0874, -0.0189]])), ('layers.1.weight_mask', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.]])), ('layers.2.bias', tensor([-0.0194, -0.0103,  0.0073, -0.0230,  0.0216, -0.0030,  0.0249,  0.0156,\n",
      "         0.0313, -0.0091])), ('layers.2.weight_orig', tensor([[ 0.0135,  0.0259, -0.0073,  ...,  0.0033, -0.0057, -0.0195],\n",
      "        [ 0.0779, -0.0092,  0.0116,  ...,  0.0005, -0.0218, -0.0232],\n",
      "        [-0.1279, -0.0215, -0.0213,  ..., -0.1504,  0.0259,  0.1550],\n",
      "        ...,\n",
      "        [-0.0014,  0.0176, -0.0920,  ...,  0.0204, -0.1627,  0.0016],\n",
      "        [-0.0155, -0.0185, -0.2187,  ...,  0.1145,  0.0045,  0.1369],\n",
      "        [ 0.1124, -0.0093,  0.1311,  ..., -0.0213,  0.0127, -0.1193]])), ('layers.2.weight_mask', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 1., 0., 1.],\n",
      "        ...,\n",
      "        [0., 0., 1.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 1.,  ..., 1., 0., 1.],\n",
      "        [1., 0., 1.,  ..., 0., 0., 1.]]))])\n",
      "ffn_mnist_copy_updated:  OrderedDict([('layers.0.bias', tensor([ 0.0207,  0.0246, -0.0463,  ...,  0.0493, -0.0022,  0.0134])), ('layers.0.weight_orig', tensor([[ 0.0457, -0.0249,  0.0128,  ...,  0.0147,  0.0492,  0.0053],\n",
      "        [-0.0101,  0.0405,  0.0396,  ...,  0.0192,  0.0305, -0.0367],\n",
      "        [ 0.0100, -0.0402, -0.0475,  ..., -0.0210, -0.0397, -0.0177],\n",
      "        ...,\n",
      "        [-0.0221,  0.0138, -0.0103,  ...,  0.0069, -0.0313,  0.0489],\n",
      "        [ 0.0125, -0.0151, -0.0174,  ..., -0.0367, -0.0056,  0.0487],\n",
      "        [ 0.0420, -0.0300,  0.0429,  ..., -0.0440, -0.0477,  0.0327]])), ('layers.0.weight_mask', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])), ('layers.1.bias', tensor([-0.0220,  0.0133,  0.0240,  ...,  0.0289, -0.0138,  0.0121])), ('layers.1.weight_orig', tensor([[ 0.0166, -0.0082,  0.0294,  ...,  0.0002,  0.0290,  0.0217],\n",
      "        [ 0.0156, -0.0085, -0.0027,  ..., -0.0134, -0.0092,  0.0150],\n",
      "        [ 0.0155, -0.0105,  0.0253,  ...,  0.0154, -0.0124, -0.0047],\n",
      "        ...,\n",
      "        [-0.0007,  0.0020, -0.0059,  ...,  0.0310, -0.0267, -0.0075],\n",
      "        [-0.0166, -0.0059, -0.0279,  ..., -0.0235,  0.0150, -0.0084],\n",
      "        [ 0.0121, -0.0234,  0.0307,  ...,  0.0134, -0.0199, -0.0189]])), ('layers.1.weight_mask', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.]])), ('layers.2.bias', tensor([-0.0204, -0.0054, -0.0001, -0.0088,  0.0177,  0.0100,  0.0190,  0.0261,\n",
      "         0.0121, -0.0078])), ('layers.2.weight_orig', tensor([[ 0.0135,  0.0259, -0.0073,  ...,  0.0033, -0.0057, -0.0195],\n",
      "        [ 0.0272, -0.0092,  0.0116,  ...,  0.0005, -0.0218, -0.0232],\n",
      "        [-0.0092, -0.0215, -0.0213,  ..., -0.0295,  0.0259,  0.0296],\n",
      "        ...,\n",
      "        [-0.0014,  0.0176, -0.0108,  ...,  0.0204, -0.0239,  0.0016],\n",
      "        [-0.0155, -0.0185, -0.0183,  ...,  0.0303,  0.0045,  0.0269],\n",
      "        [ 0.0291, -0.0093,  0.0207,  ..., -0.0213,  0.0127, -0.0123]])), ('layers.2.weight_mask', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 1., 0., 1.],\n",
      "        ...,\n",
      "        [0., 0., 1.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 1.,  ..., 1., 0., 1.],\n",
      "        [1., 0., 1.,  ..., 0., 0., 1.]]))])\n",
      "Training normal precision model for 2 epochs\n",
      "Epoch 1, Accuracy: 98.15%\n",
      "Epoch 2, Accuracy: 98.31%\n",
      "Accuracy: 98.31%, Model Size: 0.83 MB, Mean Inference Time: 0.0008s ± 0.0004s\n",
      "Sparsity per parameter:\n",
      "  layers.0.weight_mask: 97.20%\n",
      "  layers.1.weight_mask: 97.59%\n",
      "  layers.2.weight_mask: 69.00%\n",
      "Sparsity across all pruned parameters: 97.28%\n",
      "Sparsity for the model overall: 97.14%\n",
      "Pruning iteration 10\n",
      "init_updated:  {'layers.0.weight_orig': tensor([[ 0.0457, -0.0249,  0.0128,  ...,  0.0147,  0.0492,  0.0053],\n",
      "        [-0.0101,  0.0405,  0.0396,  ...,  0.0192,  0.0305, -0.0367],\n",
      "        [ 0.0100, -0.0402, -0.0475,  ..., -0.0210, -0.0397, -0.0177],\n",
      "        ...,\n",
      "        [-0.0221,  0.0138, -0.0103,  ...,  0.0069, -0.0313,  0.0489],\n",
      "        [ 0.0125, -0.0151, -0.0174,  ..., -0.0367, -0.0056,  0.0487],\n",
      "        [ 0.0420, -0.0300,  0.0429,  ..., -0.0440, -0.0477,  0.0327]]), 'layers.0.bias': tensor([ 0.0207,  0.0246, -0.0463,  ...,  0.0493, -0.0022,  0.0134]), 'layers.1.weight_orig': tensor([[ 0.0166, -0.0082,  0.0294,  ...,  0.0002,  0.0290,  0.0217],\n",
      "        [ 0.0156, -0.0085, -0.0027,  ..., -0.0134, -0.0092,  0.0150],\n",
      "        [ 0.0155, -0.0105,  0.0253,  ...,  0.0154, -0.0124, -0.0047],\n",
      "        ...,\n",
      "        [-0.0007,  0.0020, -0.0059,  ...,  0.0310, -0.0267, -0.0075],\n",
      "        [-0.0166, -0.0059, -0.0279,  ..., -0.0235,  0.0150, -0.0084],\n",
      "        [ 0.0121, -0.0234,  0.0307,  ...,  0.0134, -0.0199, -0.0189]]), 'layers.1.bias': tensor([-0.0220,  0.0133,  0.0240,  ...,  0.0289, -0.0138,  0.0121]), 'layers.2.weight_orig': tensor([[ 0.0135,  0.0259, -0.0073,  ...,  0.0033, -0.0057, -0.0195],\n",
      "        [ 0.0272, -0.0092,  0.0116,  ...,  0.0005, -0.0218, -0.0232],\n",
      "        [-0.0092, -0.0215, -0.0213,  ..., -0.0295,  0.0259,  0.0296],\n",
      "        ...,\n",
      "        [-0.0014,  0.0176, -0.0108,  ...,  0.0204, -0.0239,  0.0016],\n",
      "        [-0.0155, -0.0185, -0.0183,  ...,  0.0303,  0.0045,  0.0269],\n",
      "        [ 0.0291, -0.0093,  0.0207,  ..., -0.0213,  0.0127, -0.0123]]), 'layers.2.bias': tensor([-0.0204, -0.0054, -0.0001, -0.0088,  0.0177,  0.0100,  0.0190,  0.0261,\n",
      "         0.0121, -0.0078])}\n",
      "ffn_mnist_copy:  OrderedDict([('layers.0.bias', tensor([0.0733, 0.0185, 0.0139,  ..., 0.1386, 0.0432, 0.0237])), ('layers.0.weight_orig', tensor([[ 0.0457, -0.0249,  0.0128,  ...,  0.0147,  0.0492,  0.0053],\n",
      "        [-0.0101,  0.0405,  0.0396,  ...,  0.0192,  0.0305, -0.0367],\n",
      "        [ 0.0100, -0.0402, -0.0475,  ..., -0.0210, -0.0397, -0.0177],\n",
      "        ...,\n",
      "        [-0.0221,  0.0138, -0.0103,  ...,  0.0069, -0.0313,  0.0489],\n",
      "        [ 0.0125, -0.0151, -0.0174,  ..., -0.0367, -0.0056,  0.0487],\n",
      "        [ 0.0420, -0.0300,  0.0429,  ..., -0.0440, -0.0477,  0.0327]])), ('layers.0.weight_mask', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])), ('layers.1.bias', tensor([-0.0066,  0.0133,  0.0612,  ...,  0.1127,  0.0166,  0.0694])), ('layers.1.weight_orig', tensor([[ 0.0166, -0.0082,  0.0294,  ...,  0.0002,  0.0290,  0.0217],\n",
      "        [ 0.0156, -0.0085, -0.0027,  ..., -0.0134, -0.0092,  0.0150],\n",
      "        [ 0.0155, -0.0105,  0.0253,  ...,  0.0154, -0.0124, -0.0047],\n",
      "        ...,\n",
      "        [-0.0007,  0.0020, -0.0059,  ...,  0.0310, -0.0267, -0.0075],\n",
      "        [-0.0166, -0.0059, -0.0279,  ..., -0.0235,  0.0150, -0.0084],\n",
      "        [ 0.0121, -0.0234,  0.0307,  ...,  0.0134, -0.1604, -0.0189]])), ('layers.1.weight_mask', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.]])), ('layers.2.bias', tensor([ 0.0066,  0.0055, -0.0112, -0.0237,  0.0104, -0.0079,  0.0265,  0.0164,\n",
      "         0.0356, -0.0196])), ('layers.2.weight_orig', tensor([[ 0.0135,  0.0259, -0.0073,  ...,  0.0033, -0.0057, -0.0195],\n",
      "        [ 0.0272, -0.0092,  0.0116,  ...,  0.0005, -0.0218, -0.0232],\n",
      "        [-0.1554, -0.0215, -0.0213,  ..., -0.2111,  0.0259,  0.1298],\n",
      "        ...,\n",
      "        [-0.0014,  0.0176, -0.1131,  ...,  0.0204, -0.1741,  0.0016],\n",
      "        [-0.0155, -0.0185, -0.2890,  ...,  0.1148,  0.0045,  0.1287],\n",
      "        [ 0.1078, -0.0093,  0.1674,  ..., -0.0213,  0.0127, -0.1354]])), ('layers.2.weight_mask', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 1., 0., 1.],\n",
      "        ...,\n",
      "        [0., 0., 1.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 1.,  ..., 1., 0., 1.],\n",
      "        [1., 0., 1.,  ..., 0., 0., 1.]]))])\n",
      "ffn_mnist_copy_updated:  OrderedDict([('layers.0.bias', tensor([ 0.0207,  0.0246, -0.0463,  ...,  0.0493, -0.0022,  0.0134])), ('layers.0.weight_orig', tensor([[ 0.0457, -0.0249,  0.0128,  ...,  0.0147,  0.0492,  0.0053],\n",
      "        [-0.0101,  0.0405,  0.0396,  ...,  0.0192,  0.0305, -0.0367],\n",
      "        [ 0.0100, -0.0402, -0.0475,  ..., -0.0210, -0.0397, -0.0177],\n",
      "        ...,\n",
      "        [-0.0221,  0.0138, -0.0103,  ...,  0.0069, -0.0313,  0.0489],\n",
      "        [ 0.0125, -0.0151, -0.0174,  ..., -0.0367, -0.0056,  0.0487],\n",
      "        [ 0.0420, -0.0300,  0.0429,  ..., -0.0440, -0.0477,  0.0327]])), ('layers.0.weight_mask', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])), ('layers.1.bias', tensor([-0.0220,  0.0133,  0.0240,  ...,  0.0289, -0.0138,  0.0121])), ('layers.1.weight_orig', tensor([[ 0.0166, -0.0082,  0.0294,  ...,  0.0002,  0.0290,  0.0217],\n",
      "        [ 0.0156, -0.0085, -0.0027,  ..., -0.0134, -0.0092,  0.0150],\n",
      "        [ 0.0155, -0.0105,  0.0253,  ...,  0.0154, -0.0124, -0.0047],\n",
      "        ...,\n",
      "        [-0.0007,  0.0020, -0.0059,  ...,  0.0310, -0.0267, -0.0075],\n",
      "        [-0.0166, -0.0059, -0.0279,  ..., -0.0235,  0.0150, -0.0084],\n",
      "        [ 0.0121, -0.0234,  0.0307,  ...,  0.0134, -0.0199, -0.0189]])), ('layers.1.weight_mask', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.]])), ('layers.2.bias', tensor([-0.0204, -0.0054, -0.0001, -0.0088,  0.0177,  0.0100,  0.0190,  0.0261,\n",
      "         0.0121, -0.0078])), ('layers.2.weight_orig', tensor([[ 0.0135,  0.0259, -0.0073,  ...,  0.0033, -0.0057, -0.0195],\n",
      "        [ 0.0272, -0.0092,  0.0116,  ...,  0.0005, -0.0218, -0.0232],\n",
      "        [-0.0092, -0.0215, -0.0213,  ..., -0.0295,  0.0259,  0.0296],\n",
      "        ...,\n",
      "        [-0.0014,  0.0176, -0.0108,  ...,  0.0204, -0.0239,  0.0016],\n",
      "        [-0.0155, -0.0185, -0.0183,  ...,  0.0303,  0.0045,  0.0269],\n",
      "        [ 0.0291, -0.0093,  0.0207,  ..., -0.0213,  0.0127, -0.0123]])), ('layers.2.weight_mask', tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 1., 0., 1.],\n",
      "        ...,\n",
      "        [0., 0., 1.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 1.,  ..., 1., 0., 1.],\n",
      "        [1., 0., 1.,  ..., 0., 0., 1.]]))])\n",
      "Training normal precision model for 2 epochs\n",
      "Epoch 1, Accuracy: 98.21%\n",
      "Epoch 2, Accuracy: 98.41%\n",
      "Accuracy: 98.41%, Model Size: 0.56 MB, Mean Inference Time: 0.0008s ± 0.0004s\n",
      "Sparsity per parameter:\n",
      "  layers.0.weight_mask: 98.26%\n",
      "  layers.1.weight_mask: 98.40%\n",
      "  layers.2.weight_mask: 72.29%\n",
      "Sparsity across all pruned parameters: 98.18%\n",
      "Sparsity for the model overall: 98.04%\n",
      "\n",
      "Final Results DataFrame:\n",
      "   iteration   accuracy    size_mb  inference_time_avg  inference_time_std  \\\n",
      "0          0  97.679768  29.396738            0.000600            0.000490   \n",
      "1          1  97.619762  19.705346            0.001387            0.000372   \n",
      "2          2  97.569757  13.212034            0.000902            0.000492   \n",
      "3          3  98.099810   8.861442            0.000917            0.000600   \n",
      "4          4  98.279828   5.946562            0.000958            0.000515   \n",
      "5          5  98.429843   3.993730            0.001224            0.000233   \n",
      "6          6  98.379838   2.685186            0.000501            0.000447   \n",
      "7          7  98.399840   1.808514            0.000802            0.000401   \n",
      "8          8  98.379838   1.221122            0.000902            0.000491   \n",
      "9          9  98.309831   0.827586            0.000801            0.000401   \n",
      "10        10  98.409841   0.563970            0.000801            0.000401   \n",
      "\n",
      "   sparsity_pruned_parameters  sparsity_model  \n",
      "0                           0        0.000000  \n",
      "1                   32.999981       32.953796  \n",
      "2                   55.109996       55.032867  \n",
      "3                   69.923714       69.825852  \n",
      "4                   79.848898       79.737146  \n",
      "5                   86.498785       86.377726  \n",
      "6                   90.954198       90.826903  \n",
      "7                    93.93932       93.807847  \n",
      "8                   95.939366       95.805094  \n",
      "9                   97.279381       97.143234  \n",
      "10                   98.17722       98.039816  \n"
     ]
    }
   ],
   "source": [
    "df = iterative_pruning()\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('pruning_results_iterative.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_size = 20 * 20  # 20x20 pixels\n",
    "hidden_size = 1024\n",
    "num_classes = 10\n",
    "num_hidden_layers = 2\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "epochs = 2\n",
    "\n",
    "# Create empty DataFrame to store results\n",
    "results_df = pd.DataFrame(columns=[\n",
    "    'iteration', \n",
    "    'accuracy', \n",
    "    'size_mb', \n",
    "    'inference_time_avg', \n",
    "    'inference_time_std',\n",
    "    'sparsity_pruned_parameters',\n",
    "    'sparsity_model'\n",
    "])\n",
    "\n",
    "# Create model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = FFNN(input_size, hidden_size, num_classes, num_hidden_layers)\n",
    "init_weights = model.state_dict()\n",
    "prune_params = [(m[1], \"weight\") for m in model.named_modules() if len(list(m[1].children()))==0 and not isinstance(m[1], nn.ReLU)]\n",
    "prune_param_list = [m[0] for m in model.named_parameters() if \"weight\" in m[0]]\n",
    "print(\"Prune Parameters:\")\n",
    "print(prune_param_list)\n",
    "init_updated = {}\n",
    "for k, v in init_weights.items():\n",
    "    if k in prune_param_list and \"weight\" in k:\n",
    "        print(k)\n",
    "        init_updated[k + \"_orig\"] = v\n",
    "    else:\n",
    "        print(\"Cant find\", k)\n",
    "        init_updated[k] = v\n",
    "prune.global_unstructured(prune_params, pruning_method=prune.L1Unstructured, amount=0.33)\n",
    "print(\"Prune Parameters:\")\n",
    "print(prune_params)\n",
    "print(\"\\nModel State Dict:\")\n",
    "print(init_weights)\n",
    "print(\"\\nInitial Updated Weights:\")\n",
    "print(init_updated)\n",
    "print(\"\\nNamed Modules:\")\n",
    "print(list(model.named_modules()))\n",
    "print(\"\\nNamed Parameters:\")\n",
    "print(list(model.named_parameters()))\n",
    "print(\"\\nNamed Buffers:\")\n",
    "print(list(model.named_buffers()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p3-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
